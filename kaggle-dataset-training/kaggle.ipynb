{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sklearn\n",
    "from scipy.optimize import fminbound\n",
    "from sklearn import preprocessing\n",
    "# import scikit-learn\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# from textblob import TextBlob as tb\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import os\n",
    "import yfinance as yf\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "#Hyper params -- ROLLING WINDOW IN FUTUERe\n",
    "ALPHA_PLUS  = 0     # default for example: 0\n",
    "ALPHA_MINUS = 0.2   # default for example: 0.2\n",
    "KAPPA       = 1     # default for example: 1\n",
    "\n",
    "# the complete sestm function list\n",
    "def calc_returns(article):\n",
    "    returns = float(article['mrkt_info']['open']) - float(article['mrkt_info']['close'])\n",
    "    sgn_a = -1\n",
    "    if (returns > 0): # add -1 if returns are 0 or less, 1 otherwise\n",
    "        sgn_a = 1\n",
    "    return (returns, sgn_a)\n",
    "\n",
    "def html_to_bow(html):\n",
    "    readable_text = bs(html, 'lxml').get_text().lower()\n",
    "    # print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "    # substitute non alphabet chars (new lines become spaces)\n",
    "    readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "    readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "    # sub multiple spaces with one space\n",
    "    readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "    # tokenise text\n",
    "    words = nltk.wordpunct_tokenize(readable_text)\n",
    "    bow_art = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    en_words = set(nltk.corpus.words.words())\n",
    "    # lemmatised_words = []\n",
    "    if len(words) > 0:\n",
    "        # lemmatise, remove non-english, and remove stopwords\n",
    "        for w in words:\n",
    "            rootword = lemmatizer.lemmatize(w, pos=\"v\")\n",
    "            if rootword not in STOP_WORDS and rootword in en_words:\n",
    "                # lemmatised_words.append(rootword)\n",
    "                if w in bow_art:\n",
    "                    bow_art[w] += 1\n",
    "                else:\n",
    "                    bow_art[w] = 1\n",
    "        # convert to bag of words\n",
    "        # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # for l in lemmatised_words:\n",
    "        #     if l in bow_art:\n",
    "        #         bow_art[l] += 1\n",
    "        #     else:\n",
    "        #         bow_art[l] = 1\n",
    "    \n",
    "    return bow_art\n",
    "\n",
    "def calc_f(d, sgn):\n",
    "    pos_j = {}  #j occuring in positive article\n",
    "    total_j = {}#j occuring in any article\n",
    "    f = {}      #fraction of positive occurrences\n",
    "    for i in range(len(d)):\n",
    "        for w in d[i]:\n",
    "            # pos_sent = sgn[i]\n",
    "            pos_sent = 0\n",
    "            if (sgn[i] == 1): pos_sent = 1\n",
    "            if w in total_j:\n",
    "                total_j[w] += d[i][w]\n",
    "                pos_j[w] += d[i][w]*pos_sent\n",
    "            else:\n",
    "                total_j[w] = d[i][w]\n",
    "                pos_j[w] = d[i][w]*pos_sent\n",
    "            f[w] = pos_j[w]/total_j[w]\n",
    "    return (pos_j, total_j, f)\n",
    "\n",
    "def gen_sent_word_list(total_j,sgn,f):\n",
    "    pi = sum(sgn_i > 0 for sgn_i in sgn)/len(sgn)\n",
    "    print(pi)\n",
    "    sentiment_words = [] # S\n",
    "    neutral_words = []   # N\n",
    "    for i in total_j:\n",
    "        if ((f[i] >= pi + ALPHA_PLUS or f[i] <= pi - ALPHA_MINUS) and total_j[i] >= KAPPA and len(i) > 1):\n",
    "            sentiment_words.append(i)\n",
    "        else:\n",
    "            neutral_words.append(i)\n",
    "    return(sentiment_words, neutral_words)\n",
    "\n",
    "# Calculates p_i\n",
    "def calc_p(y):\n",
    "    p = [0] * len(y)\n",
    "    for i, x in enumerate(sorted(range(len(y)), key=lambda y_lam: y[y_lam])):\n",
    "        p[x] = float((i+1)/(len(y)))\n",
    "    return p\n",
    "\n",
    "# Calculates s_i\n",
    "def calc_s(sentiment_words, d):\n",
    "    s = []                                          # ith element corresponds to total count of sentiment charged words for document i\n",
    "    d_s = []                                        # ith element corresponds to list of word counts for each of the sentiment charged words for document i\n",
    "    for doc in d:\n",
    "        s.append(sum(doc.get(val,0) for val in sentiment_words))\n",
    "        d_s.append([doc.get(val,0) for val in sentiment_words])\n",
    "    return (s, d_s)\n",
    "\n",
    "# Calculates h_i\n",
    "def calc_h(sentiment_words, d, s, d_s):\n",
    "    h = np.zeros((len(d), len(sentiment_words)))    # ith element corresponds to |S|x1 vector of word frequencies divided by total sentiment words in doc i\n",
    "\n",
    "    for i in range(len(d)):\n",
    "        # subvector of sentiment words in d_i\n",
    "        if (s[i] == 0) :\n",
    "            h[i] = np.zeros(len(sentiment_words)).transpose()\n",
    "        else:\n",
    "            h[i] = np.array([(j/s[i]) for j in d_s[i]]).transpose()\n",
    "    return h\n",
    "\n",
    "# Calculates O\n",
    "def calc_o(p,h):\n",
    "    p_inv = [(1-val) for val in p]\n",
    "    W = np.column_stack((p, p_inv))\n",
    "    W = W.transpose()\n",
    "    ww = np.matmul(W,W.transpose())\n",
    "    w2 = np.matmul(W.transpose(), inv(ww))\n",
    "    O = np.matmul(h.transpose(),w2)\n",
    "    O[O < 0] = 0 # remove negative entries of O\n",
    "    O = O.transpose()\n",
    "    # Normalise O columns to have l1 norm\n",
    "    O = sklearn.preprocessing.normalize(O,norm='l1')\n",
    "    O = O.transpose()\n",
    "    return O\n",
    "\n",
    "# lam = 3 is what i normally use\n",
    "def equation_to_solve(O, p_solve, new_bow, sentiment_words, new_s, lam):\n",
    "    i = 0\n",
    "    equation = 0\n",
    "    for j in sentiment_words:\n",
    "        # a = (new_bow.get(j,0) * math.log(new_p*O[i][0] + (1-new_p)*O[i][1]))\n",
    "        d_j = new_bow.get(j,0)\n",
    "        in_log = p_solve*O[i][0] + (1-p_solve)*O[i][1]\n",
    "        if not in_log == 0:\n",
    "            equation += d_j * math.log(p_solve*O[i][0] + (1-p_solve)*O[i][1])\n",
    "\n",
    "        i += 1\n",
    "        # i += 1/new_s + lam * (new_p*(1-new_p))\n",
    "\n",
    "    # if new_s == 0:\n",
    "    #     new_s = 1\n",
    "    equation /= new_s\n",
    "    equation += lam*(p_solve*(1-p_solve))\n",
    "    equation *= -1 #flip equation for argmin\n",
    "    return equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import all of the headlines from kaggle and pull the required stock information to compile a json list of articles like we have normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1400470 lines generating 1397891 usable headlines\n"
     ]
    }
   ],
   "source": [
    "#loop through list of files\n",
    "article_list = []\n",
    "file_name = '/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/archive/analyst_ratings_processed.csv'\n",
    "with open(file_name) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and len(row) == 4:\n",
    "            new_art = {\n",
    "                'headline': row[1],\n",
    "                'date': row[2],\n",
    "                'ticker': row[3]\n",
    "            }\n",
    "            article_list.append(new_art)\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines generating {len(article_list)} usable headlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date 2009-02-14 14:02:00-05:00 and max date 2020-06-11 17:12:00-04:00\n",
      "No. articles in 2009: 14321\n",
      "No. articles in 2010: 81144\n",
      "No. articles in 2011: 132333\n",
      "No. articles in 2012: 122234\n",
      "No. articles in 2013: 121252\n",
      "No. articles in 2014: 129949\n",
      "No. articles in 2015: 132877\n",
      "No. articles in 2016: 141315\n",
      "No. articles in 2017: 120298\n",
      "No. articles in 2018: 146413\n",
      "No. articles in 2019: 150080\n",
      "No. articles in 2020: 105675\n"
     ]
    }
   ],
   "source": [
    "list_dates = [a['date'] for a in article_list]\n",
    "print(f'Min date {min(list_dates)} and max date {max(list_dates)}')\n",
    "for year in range(2009,2021):\n",
    "    year_count = len([a for a in article_list if (a['date'] < str(year+1) + '-01-01' and a['date'] > str(year) + '-01-01')])\n",
    "    print(f'No. articles in {year}: {year_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling stocks...\n",
      "[====================] 100% 6191 out of 6192 (2015)Failed stocks = ['AAN', 'AAV', 'AAVL', 'ABAC', 'ABCW', 'ABDC', 'ABGB', 'ABTL', 'ABX', 'ABY', 'ACAS', 'ACAT', 'ACCU', 'ACE', 'ACG', 'ACHN', 'ACMP', 'ACPW', 'ACSF', 'ACT', 'ACTS', 'ACXM', 'ADAT', 'ADEP', 'ADGE', 'ADHD', 'ADK', 'ADMS', 'ADNC', 'ADRA', 'ADVS', 'AEC', 'AEGN', 'AEGR', 'AEPI', 'AETI', 'AF', 'AFA', 'AFC', 'AFFX', 'AFH', 'AFOP', 'AGC', 'AGII', 'AGN', 'AGNCB', 'AGOL', 'AGU', 'AHC', 'AHP', 'AI', 'AIB', 'AIRM', 'AIXG', 'AKAO', 'AKER', 'AKG', 'AKP', 'AKRX', 'AKS', 'ALDR', 'ALDW', 'ALJ', 'ALLB', 'ALQA', 'ALSK', 'ALTV', 'ALU', 'ALXA', 'ALXN', 'AMAG', 'AMBR', 'AMCC', 'AMCO', 'AMDA', 'AMFW', 'AMIC', 'AMID', 'AMPS', 'AMRB', 'AMRE', 'AMRI', 'AMSG', 'AMTG', 'AMZG', 'ANAC', 'ANAD', 'ANCI', 'AND', 'ANH', 'ANV', 'ANW', 'AOI', 'AOL', 'APAGF', 'APC', 'APF', 'API', 'APL', 'APOL', 'APP', 'APPY', 'APRI', 'APSA', 'AQQ', 'AQXP', 'ARCI', 'ARCX', 'ARDM', 'AREX', 'ARGS', 'ARIA', 'ARIS', 'ARMH', 'ARO', 'ARPI', 'ARQL', 'ARRS', 'ARRY', 'ARTX', 'ASBI', 'ASCMA', 'ASFI', 'ASMI', 'ASNA', 'ASPX', 'AST', 'AT', 'ATE', 'ATHN', 'ATK', 'ATL', 'ATLS', 'ATML', 'ATNY', 'ATRM', 'ATTU', 'ATU', 'ATV', 'ATW', 'AUMA', 'AUMAU', 'AUQ', 'AUXL', 'AV', 'AVG', 'AVH', 'AVHI', 'AVIV', 'AVL', 'AVNR', 'AVOL', 'AVP', 'AVX', 'AXE', 'AXJS', 'AXLL', 'AXN', 'AXPW', 'AXX', 'AYR', 'AZIA', 'BAA', 'BABS', 'BABY', 'BAF', 'BAGR', 'BALT', 'BAMM', 'BAS', 'BASI', 'BBCN', 'BBF', 'BBG', 'BBK', 'BBLU', 'BBNK', 'BBRC', 'BBRY', 'BBT', 'BBX', 'BCA', 'BCOM', 'BCR', 'BDBD', 'BDCV', 'BDE', 'BDGE', 'BEAT', 'BEE', 'BEL', 'BF', 'BFR', 'BFY', 'BGCA', 'BGG', 'BHBK', 'BHI', 'BHL', 'BID', 'BIK', 'BIN', 'BIND', 'BIOA', 'BIOD', 'BIOS', 'BIRT', 'BITA', 'BKJ', 'BKK', 'BKMU', 'BKS', 'BKYF', 'BLOX', 'BLT', 'BLVD', 'BLVDU', 'BMR', 'BMTC', 'BNCL', 'BNCN', 'BOBE', 'BOCH', 'BOFI', 'BONA', 'BONE', 'BONT', 'BORN', 'BOTA', 'BOXC', 'BPFH', 'BPFHW', 'BPI', 'BPL', 'BPOPN', 'BQH', 'BRAF', 'BRAQ', 'BRAZ', 'BRCD', 'BRCM', 'BRDR', 'BREW', 'BRK', 'BRKS', 'BRLI', 'BRSS', 'BRXX', 'BSCG', 'BSD', 'BSDM', 'BSE', 'BSFT', 'BSI', 'BSTC', 'BT', 'BTE', 'BTUI', 'BUNL', 'BUNT', 'BVA', 'BVSN', 'BVX', 'BWC', 'BWINA', 'BWINB', 'BWLD', 'BWS', 'BXE', 'BXS', 'BZC', 'BZM', 'CAB', 'CACGU', 'CACQ', 'CADC', 'CADT', 'CAFE', 'CAK', 'CAM', 'CAP', 'CAPN', 'CARB', 'CARO', 'CART', 'CAS', 'CASM', 'CATM', 'CAW', 'CBAK', 'CBB', 'CBDE', 'CBF', 'CBG', 'CBIN', 'CBK', 'CBLI', 'CBM', 'CBMG', 'CBMX', 'CBNJ', 'CBPO', 'CBPX', 'CBR', 'CBRX', 'CBS', 'CBSHP', 'CBST', 'CCC', 'CCCL', 'CCCR', 'CCE', 'CCG', 'CCSC', 'CCV', 'CCX', 'CCXE', 'CDC', 'CDI', 'CECO', 'CEL', 'CELGZ', 'CEMP', 'CERE', 'CERU', 'CETV', 'CFD', 'CFN', 'CFNL', 'CFP', 'CFRXW', 'CFRXZ', 'CGG', 'CGI', 'CGIX', 'CH', 'CHA', 'CHEV', 'CHFC', 'CHK', 'CHKE', 'CHL', 'CHLN', 'CHMT', 'CHOC', 'CHOP', 'CHSP', 'CHU', 'CHXF', 'CHYR', 'CIE', 'CIFC', 'CIMT', 'CISG', 'CJES', 'CKEC', 'CKH', 'CKP', 'CKSW', 'CLAC', 'CLCT', 'CLD', 'CLDN', 'CLGX', 'CLI', 'CLMS', 'CLNT', 'CLNY', 'CLRX', 'CLTX', 'CLUB', 'CLY', 'CMCSK', 'CMD', 'CMFN', 'CMGE', 'CMLP', 'CMN', 'CMSB', 'CNBKA', 'CNCO', 'CNDA', 'CNDO', 'CNIT', 'CNNX', 'CNTF', 'CNV', 'CNW', 'CNYD', 'COB', 'COBK', 'COCO', 'CODE', 'COH', 'COOL', 'COR', 'CORE', 'COSI', 'COT', 'COVR', 'COVS', 'CPAH', 'CPGI', 'CPHD', 'CPHR', 'CPL', 'CPN', 'CPST', 'CPTA', 'CRAY', 'CRBQ', 'CRC', 'CRCM', 'CRD', 'CRDC', 'CRDS', 'CRDT', 'CRED', 'CREE', 'CRME', 'CRR', 'CRRC', 'CRRS', 'CRV', 'CRWN', 'CRZO', 'CSC', 'CSFL', 'CSG', 'CSH', 'CSJ', 'CSOD', 'CSRE', 'CSS', 'CST', 'CSUN', 'CTCT', 'CTF', 'CTL', 'CTNN', 'CTRL', 'CTRX', 'CTV', 'CTWS', 'CU', 'CUB', 'CUI', 'CUNB', 'CUO', 'CUR', 'CVA', 'CVC', 'CVD', 'CVOL', 'CVSL', 'CVTI', 'CWEI', 'CXA', 'CXO', 'CXP', 'CY', 'CYBX', 'CYN', 'CYNI', 'CYOU', 'CYT', 'CYTX', 'CZFC', 'CZZ', 'DAEG', 'DAKP', 'DANG', 'DARA', 'DATA', 'DATE', 'DBBR', 'DBMX', 'DBU', 'DBUK', 'DCA', 'DCIX', 'DCM', 'DCT', 'DDC', 'DDR', 'DEG', 'DEJ', 'DEPO', 'DEST', 'DF', 'DFRG', 'DFT', 'DGAS', 'DGI', 'DGSE', 'DHRM', 'DIVI', 'DKT', 'DLBL', 'DLPH', 'DM', 'DMD', 'DMND', 'DNB', 'DNBF', 'DNKN', 'DNO', 'DNR', 'DO', 'DOM', 'DOVR', 'DPLO', 'DPM', 'DPRX', 'DPW', 'DRAD', 'DRAM', 'DRC', 'DRII', 'DRL', 'DRNA', 'DRWI', 'DRYS', 'DSCI', 'DSCO', 'DSE', 'DSKX', 'DSKY', 'DSUM', 'DTLK', 'DTSI', 'DTV', 'DUC', 'DV', 'DVCR', 'DVD', 'DW', 'DWA', 'DWRE', 'DWTI', 'DXB', 'DXJF', 'DXJR', 'DXKW', 'DXM', 'DXPS', 'DYAX', 'DYN', 'EAC', 'EBIO', 'EBSB', 'ECA', 'ECR', 'ECT', 'ECTE', 'EDE', 'EDR', 'EDS', 'EE', 'EEHB', 'EEI', 'EEME', 'EEML', 'EFF', 'EFII', 'EFUT', 'EGAS', 'EGI', 'EGLT', 'EGOV', 'EGRW', 'EGT', 'EHIC', 'EIGI', 'EIV', 'EJ', 'ELGX', 'ELLI', 'ELNK', 'ELOS', 'ELRC', 'ELX', 'EMBB', 'EMCD', 'EMCI', 'EMCR', 'EMDI', 'EMES', 'EMEY', 'EMQ', 'EMSA', 'EMXX', 'ENBL', 'ENFC', 'ENGN', 'ENH', 'ENI', 'ENL', 'ENOC', 'ENRJ', 'ENT', 'ENVI', 'ENY', 'ENZY', 'EOC', 'EOPN', 'EOX', 'EPAX', 'EPE', 'EPIQ', 'EPRS', 'EQM', 'EQY', 'ERA', 'ERB', 'ERO', 'EROS', 'ERS', 'ESBF', 'ESCR', 'ESL', 'ESSX', 'ESV', 'ESYS', 'ETAK', 'ETE', 'ETF', 'ETFC', 'ETH', 'ETM', 'ETRM', 'EV', 'EVAR', 'EVBS', 'EVDY', 'EVEP', 'EVJ', 'EVLV', 'EVRY', 'EWCS', 'EWHS', 'EWSS', 'EXA', 'EXAM', 'EXAR', 'EXE', 'EXFO', 'EXL', 'EXLP', 'EXXI', 'FAC', 'FAV', 'FBNK', 'FBSS', 'FCAU', 'FCE', 'FCH', 'FCHI', 'FCLF', 'FCS', 'FCSC', 'FDEF', 'FDI', 'FDML', 'FDO', 'FEIC', 'FELP', 'FES', 'FEYE', 'FFG', 'FGP', 'FHCO', 'FHY', 'FI', 'FIG', 'FISH', 'FLIR', 'FLML', 'FLTX', 'FLXN', 'FLY', 'FMD', 'FMER', 'FNBC', 'FNFG', 'FNFV', 'FNJN', 'FNSR', 'FOIL', 'FOMX', 'FONE', 'FPO', 'FPRX', 'FRAN', 'FRED', 'FREE', 'FRM', 'FRP', 'FRS', 'FRSH', 'FSAM', 'FSBK', 'FSC', 'FSGI', 'FSIC', 'FSL', 'FSNN', 'FSRV', 'FSYS', 'FTD', 'FTR', 'FTT', 'FUEL', 'FULL', 'FUR', 'FWM', 'FWV', 'FXCB', 'FXCM', 'GAI', 'GAINO', 'GALE', 'GALTU', 'GARS', 'GAS', 'GBB', 'GBSN', 'GCA', 'GCAP', 'GCH', 'GCVRZ', 'GDAY', 'GDEF', 'GDF', 'GDP', 'GEUR', 'GEVA', 'GFA', 'GFIG', 'GFNCP', 'GFY', 'GG', 'GGAC', 'GGE', 'GGM', 'GGOV', 'GGP', 'GHDX', 'GHI', 'GIG', 'GIMO', 'GK', 'GKNT', 'GLDC', 'GLDX', 'GLOG', 'GLPW', 'GLUU', 'GMCR', 'GMFS', 'GMK', 'GMLP', 'GMO', 'GMT', 'GMZ', 'GNC', 'GNI', 'GNMK', 'GNVC', 'GOMO', 'GOODO', 'GOODP', 'GOV', 'GPIC', 'GPM', 'GPOR', 'GPX', 'GRAM', 'GRH', 'GRIF', 'GRN', 'GRO', 'GRT', 'GSB', 'GSH', 'GSI', 'GSOL', 'GST', 'GSVC', 'GTAA', 'GTI', 'GTIV', 'GTT', 'GTU', 'GTWN', 'GTXI', 'GUID', 'GUR', 'GURX', 'GWL', 'GWPH', 'GWR', 'GY', 'GYEN', 'GZT', 'HABT', 'HAR', 'HAWKB', 'HBHC', 'HBK', 'HBNK', 'HBOS', 'HCAC', 'HCAP', 'HCBK', 'HCHC', 'HCLP', 'HCN', 'HCOM', 'HCP', 'HCT', 'HDRA', 'HDRAU', 'HDS', 'HDY', 'HEB', 'HELI', 'HEOP', 'HF', 'HFBC', 'HFFC', 'HGG', 'HGI', 'HGR', 'HGT', 'HH', 'HIIQ', 'HILL', 'HILO', 'HK', 'HKOR', 'HKTV', 'HLS', 'HLSS', 'HME', 'HMPR', 'HMSY', 'HNH', 'HNR', 'HNSN', 'HNT', 'HOS', 'HOTRW', 'HPJ', 'HPT', 'HPTX', 'HPY', 'HRC', 'HRS', 'HRT', 'HSEA', 'HSGX', 'HSNI', 'HSOL', 'HSP', 'HTCH', 'HTF', 'HTR', 'HTS', 'HTWO', 'HTWR', 'HTZ', 'HUB', 'HVB', 'HW', 'HWAY', 'HWCC', 'HYGS', 'HYH', 'IACI', 'IBCA', 'IBKC', 'ICA', 'ICB', 'ICEL', 'ICON', 'IDHB', 'IDSY', 'IDTI', 'IDXJ', 'IEC', 'IFMI', 'IFON', 'IFT', 'IG', 'IGLD', 'IGTE', 'IID', 'IILG', 'IJNK', 'IKAN', 'IKGH', 'IKNX', 'IL', 'IM', 'IMDZ', 'IMI', 'IMMU', 'IMN', 'IMNP', 'IMPR', 'IMRS', 'IMS', 'IMUC', 'INAP', 'INB', 'INCR', 'IND', 'INF', 'INFA', 'ININ', 'INP', 'INPH', 'INS', 'INSY', 'INTL', 'INVN', 'INWK', 'INXN', 'INXX', 'INY', 'IOC', 'IOIL', 'IOT', 'IPCI', 'IPCM', 'IPD', 'IPF', 'IPHS', 'IPK', 'IPU', 'IPW', 'IQNT', 'IRC', 'IRDMB', 'IRDMZ', 'IRE', 'IRET', 'IRF', 'IRR', 'ISCA', 'ISF', 'ISH', 'ISIL', 'ISIS', 'ISLE', 'ISNS', 'ISRL', 'ISSI', 'IST', 'ITC', 'ITF', 'ITG', 'ITLT', 'ITLY', 'ITR', 'IVAN', 'IVOP', 'IXYS', 'JAH', 'JASN', 'JASO', 'JAXB', 'JCOM', 'JCP', 'JDD', 'JDSU', 'JEC', 'JFC', 'JGBD', 'JGBL', 'JGBS', 'JGBT', 'JGW', 'JIVE', 'JJA', 'JJM', 'JJN', 'JJP', 'JJT', 'JJU', 'JMEI', 'JMLP', 'JMP', 'JNS', 'JO', 'JOEZ', 'JONE', 'JOY', 'JPEP', 'JPP', 'JRN', 'JSC', 'JST', 'JTA', 'JTD', 'JTP', 'JUNR', 'JW', 'JYN', 'KATE', 'KBIO', 'KBSF', 'KBWC', 'KBWI', 'KCAP', 'KCC', 'KCG', 'KEF', 'KEG', 'KEM', 'KFH', 'KFI', 'KFX', 'KHI', 'KIN', 'KITE', 'KKD', 'KME', 'KNL', 'KNM', 'KONA', 'KONE', 'KOOL', 'KORS', 'KROO', 'KRU', 'KST', 'KSU', 'KTEC', 'KUTV', 'KWT', 'KYO', 'KYTH', 'KZ', 'LABC', 'LABL', 'LACO', 'LAS', 'LBF', 'LBIX', 'LBMH', 'LBY', 'LDL', 'LDR', 'LDRH', 'LEI', 'LEVY', 'LEVYU', 'LG', 'LGCY', 'LGF', 'LINE', 'LION', 'LIOX', 'LIQD', 'LLDM', 'LLEM', 'LLEX', 'LLSC', 'LLTC', 'LM', 'LMCA', 'LMCB', 'LMCK', 'LMIA', 'LMLP', 'LMNX', 'LMOS', 'LMRK', 'LNBB', 'LNCO', 'LNKD', 'LOCK', 'LOCM', 'LOGM', 'LOJN', 'LONG', 'LOOK', 'LORL', 'LPHI', 'LPT', 'LPTN', 'LRAD', 'LRE', 'LSC', 'LSG', 'LTM', 'LTS', 'LTXB', 'LUX', 'LVLT', 'LVNTA', 'LWC', 'LXFT', 'LXK', 'MAGS', 'MAMS', 'MBFI', 'MBLX', 'MBLY', 'MBRG', 'MBTF', 'MBVT', 'MCC', 'MCF', 'MCGC', 'MCOX', 'MCRL', 'MCUR', 'MCV', 'MCZ', 'MDAS', 'MDCA', 'MDCO', 'MDGN', 'MDLY', 'MDP', 'MDSO', 'MDSY', 'MDVN', 'MDVXU', 'MEA', 'MEET', 'MEG', 'MELA', 'MELR', 'MEN', 'MENT', 'MERU', 'MES', 'METR', 'MFI', 'MFLX', 'MFNC', 'MFRI', 'MFRM', 'MFSF', 'MFT', 'MGCD', 'MGH', 'MGLN', 'MGN', 'MGT', 'MHE', 'MHFI', 'MHGC', 'MHR', 'MIE', 'MIFI', 'MIK', 'MIL', 'MILL', 'MINI', 'MJN', 'MKTO', 'MLHR', 'MLNK', 'MLNX', 'MLPJ', 'MLPL', 'MM', 'MMAC', 'MNE', 'MNGA', 'MNI', 'MNK', 'MNRK', 'MNTA', 'MOBI', 'MOC', 'MOCO', 'MOG', 'MOKO', 'MOLG', 'MON', 'MONY', 'MORE', 'MPEL', 'MPET', 'MPO', 'MRD', 'MRH', 'MRKT', 'MRVC', 'MSBF', 'MSF', 'MSG', 'MSLI', 'MSO', 'MSON', 'MSP', 'MSTX', 'MTK', 'MTS', 'MTSC', 'MTSL', 'MTSN', 'MTT', 'MTU', 'MUH', 'MUS', 'MVC', 'MVG', 'MVNR', 'MW', 'MWE', 'MWIV', 'MWV', 'MXIM', 'MXWL', 'MY', 'MYCC', 'MYF', 'MYL', 'MYOS', 'MZF', 'NADL', 'NAME', 'NANO', 'NAO', 'NATL', 'NAV', 'NBBC', 'NBG', 'NBL', 'NBS', 'NBTF', 'NCB', 'NCFT', 'NCI', 'NCIT', 'NCQ', 'NDRO', 'NE', 'NEOT', 'NETE', 'NEWM', 'NEWS', 'NFEC', 'NGHC', 'NGHCP', 'NGLS', 'NHF', 'NHTB', 'NJ', 'NJV', 'NKA', 'NKY', 'NLNK', 'NMBL', 'NMO', 'NMRX', 'NMY', 'NNA', 'NNC', 'NOR', 'NORD', 'NPBC', 'NPD', 'NPP', 'NPSP', 'NRCIA', 'NRF', 'NRX', 'NSAM', 'NSH', 'NSPH', 'NSR', 'NTI', 'NTK', 'NTL', 'NTLS', 'NTN', 'NTRSP', 'NTT', 'NTX', 'NU', 'NUTR', 'NVDQ', 'NVGN', 'NVSL', 'NVX', 'NWHM', 'NWY', 'NXQ', 'NXR', 'NXTDW', 'NXTM', 'NYLD', 'NYMTP', 'NYNY', 'NYV', 'OAK', 'OAKS', 'OB', 'OCIR', 'OCLS', 'OCR', 'OCRX', 'OGXI', 'OHAI', 'OHGI', 'OHRP', 'OIBR', 'OILT', 'OKS', 'OKSB', 'OLO', 'OMAM', 'OME', 'OMED', 'OMG', 'OMN', 'ONEF', 'ONFC', 'ONNN', 'ONP', 'ONTY', 'ONVI', 'OPB', 'OPHT', 'OPWR', 'OPXA', 'ORB', 'ORBC', 'ORBK', 'OREX', 'ORIT', 'ORM', 'ORPN', 'OSGB', 'OSHC', 'OSIR', 'OSM', 'OSN', 'OTEL', 'OTIV', 'OUTR', 'OVTI', 'OWW', 'OXFD', 'OXLCO', 'OZM', 'OZRK', 'PACD', 'PAF', 'PAGG', 'PAH', 'PAL', 'PARN', 'PAY', 'PBCP', 'PBIB', 'PBM', 'PBMD', 'PBY', 'PCI', 'PCL', 'PCLN', 'PCMI', 'PCO', 'PCP', 'PCYC', 'PDII', 'PDLI', 'PE', 'PEGI', 'PEIX', 'PENX', 'PEOP', 'PER', 'PERF', 'PERM', 'PES', 'PETM', 'PETX', 'PFBI', 'PFK', 'PFNX', 'PFPT', 'PGI', 'PGM', 'PGN', 'PGNX', 'PHF', 'PHII', 'PHIIK', 'PHMD', 'PICO', 'PIH', 'PIP', 'PIR', 'PJC', 'PKD', 'PKO', 'PKY', 'PLCM', 'PLKI', 'PLMT', 'PLNR', 'PLPM', 'PLT', 'PLTM', 'PMBC', 'PMC', 'PMCS', 'PMFG', 'PNRA', 'PNTR', 'PNX', 'POL', 'POPE', 'POT', 'POWR', 'POZN', 'PPHM', 'PPHMP', 'PPO', 'PPP', 'PPR', 'PPS', 'PQ', 'PRAH', 'PRAN', 'PRB', 'PRCP', 'PRE', 'PRGN', 'PRGX', 'PRLS', 'PRSC', 'PRTO', 'PRXI', 'PRXL', 'PRY', 'PSAU', 'PSBH', 'PSDV', 'PSEM', 'PSG', 'PSTB', 'PSTR', 'PSUN', 'PTBI', 'PTIE', 'PTLA', 'PTM', 'PTP', 'PTRY', 'PTX', 'PULB', 'PVA', 'PVTB', 'PWE', 'PWRD', 'PWX', 'PXMC', 'PXR', 'PXSC', 'PZI', 'Q', 'QADA', 'QCAN', 'QDEM', 'QDXU', 'QEH', 'QEM', 'QEP', 'QEPM', 'QGBR', 'QIHU', 'QKOR', 'QLGC', 'QLIK', 'QLTB', 'QLTC', 'QLTI', 'QLTY', 'QSII', 'QTM', 'QTS', 'QTWN', 'QTWW', 'QUNR', 'QVCA', 'QVCB', 'QXUS', 'RAI', 'RALY', 'RAS', 'RATE', 'RAVN', 'RAX', 'RBC', 'RBL', 'RBPAA', 'RBS', 'RBY', 'RCAP', 'RCPI', 'RCPT', 'RDC', 'RDEN', 'RDS', 'RECN', 'REE', 'REMY', 'REN', 'RENT', 'RESI', 'REXI', 'REXX', 'RFT', 'RGDO', 'RGDX', 'RGSE', 'RHT', 'RIC', 'RICE', 'RIF', 'RIGP', 'RIOM', 'RIT', 'RIVR', 'RJET', 'RKT', 'RKUS', 'RLD', 'RLH', 'RLOC', 'RLOG', 'RLYP', 'RNA', 'RNDY', 'RNET', 'RNF', 'RNN', 'ROC', 'ROIAK', 'ROIQ', 'ROIQU', 'ROIQW', 'ROKA', 'RORO', 'ROSE', 'ROVI', 'ROX', 'RP', 'RPAI', 'RPRX', 'RPRXW', 'RPRXZ', 'RPTP', 'RPX', 'RRST', 'RSE', 'RSH', 'RSO', 'RST', 'RSTI', 'RT', 'RTEC', 'RTGN', 'RTI', 'RTIX', 'RTK', 'RTR', 'RTRX', 'RUBI', 'RUK', 'RVBD', 'RVLT', 'RVM', 'RWC', 'RWV', 'RWXL', 'RXDX', 'RXII', 'RYL', 'S', 'SAAS', 'SAEX', 'SAJA', 'SALT', 'SAPE', 'SARA', 'SBBX', 'SBGL', 'SBRAP', 'SBSA', 'SBY', 'SCAI', 'SCLN', 'SCMP', 'SCOK', 'SCPB', 'SCSS', 'SCTY', 'SDLP', 'SDR', 'SDRL', 'SDT', 'SEMG', 'SEMI', 'SERV', 'SEV', 'SFB', 'SFG', 'SFLA', 'SFLY', 'SFN', 'SFS', 'SFXE', 'SGAR', 'SGB', 'SGBK', 'SGG', 'SGM', 'SGNL', 'SGNT', 'SGOC', 'SGY', 'SGYP', 'SGYPU', 'SGYPW', 'SHLD', 'SHLO', 'SHOR', 'SHOS', 'SIAL', 'SIBC', 'SIFI', 'SIGM', 'SIMG', 'SINA', 'SINO', 'SIRO', 'SKBI', 'SKH', 'SKIS', 'SKUL', 'SLCT', 'SLH', 'SLI', 'SLTC', 'SLW', 'SLXP', 'SMACU', 'SMI', 'SMK', 'SMRT', 'SMTP', 'SMTX', 'SN', 'SNAK', 'SNBC', 'SNC', 'SNDK', 'SNH', 'SNOW', 'SNR', 'SNSS', 'SNTA', 'SORL', 'SPA', 'SPAN', 'SPAR', 'SPEX', 'SPF', 'SPKE', 'SPLS', 'SPNC', 'SPP', 'SPPR', 'SPPRO', 'SPRT', 'SPU', 'SPW', 'SQBG', 'SQBK', 'SQI', 'SQNM', 'SRSC', 'SSE', 'SSFN', 'SSH', 'SSI', 'SSLT', 'SSN', 'SSNI', 'SSRG', 'SSRI', 'SSS', 'SSW', 'STAY', 'STCK', 'STEM', 'STI', 'STJ', 'STML', 'STMP', 'STNR', 'STO', 'STRN', 'STRZA', 'STRZB', 'SUBK', 'SUNE', 'SUSQ', 'SUTR', 'SVLC', 'SWHC', 'SWY', 'SXCP', 'SXE', 'SYA', 'SYKE', 'SYMC', 'SYMX', 'SYNC', 'SYRG', 'SYRX', 'SYT', 'SYUT', 'SYX', 'SZYM', 'TAHO', 'TAOM', 'TAS', 'TASR', 'TAT', 'TAX', 'TAXI', 'TBAR', 'TBIO', 'TCAP', 'TCBIP', 'TCCA', 'TCHI', 'TCK', 'TCO', 'TCP', 'TCPI', 'TCRD', 'TDA', 'TDD', 'TDI', 'TE', 'TEAR', 'TECD', 'TECU', 'TEG', 'TERP', 'TESO', 'TEU', 'TFM', 'TFSCU', 'TGC', 'TGD', 'TGE', 'THOR', 'THRX', 'THTI', 'TI', 'TICC', 'TIF', 'TIK', 'TIME', 'TINY', 'TISA', 'TIVO', 'TKAI', 'TKMR', 'TLF', 'TLI', 'TLL', 'TLLP', 'TLM', 'TLMR', 'TLO', 'TLP', 'TLR', 'TMH', 'TMK', 'TNAV', 'TNDQ', 'TNGO', 'TOF', 'TOO', 'TOT', 'TOWR', 'TPI', 'TPLM', 'TPRE', 'TPUB', 'TRAK', 'TRCB', 'TRCH', 'TRCO', 'TRF', 'TRGT', 'TRIL', 'TRIV', 'TRK', 'TRLA', 'TRMR', 'TRND', 'TROV', 'TROVU', 'TROVW', 'TRR', 'TRTL', 'TRW', 'TRXC', 'TSL', 'TSLF', 'TSO', 'TSRA', 'TSRE', 'TSS', 'TST', 'TSU', 'TSYS', 'TTF', 'TTFS', 'TTHI', 'TTPH', 'TTS', 'TUBE', 'TUES', 'TVIZ', 'TWC', 'TWMC', 'TXTR', 'TYC', 'TYPE', 'UACL', 'UAM', 'UBC', 'UBIC', 'UBNK', 'UBSH', 'UCD', 'UCFC', 'UCP', 'UDF', 'UFS', 'UHN', 'UIL', 'ULTI', 'ULTR', 'UMX', 'UN', 'UNIS', 'UNT', 'UNXL', 'UPIP', 'UPL', 'URZ', 'USAG', 'USAT', 'USBI', 'USCR', 'USG', 'USMD', 'USMI', 'USTR', 'UTEK', 'UTIW', 'UWTI', 'VA', 'VAL', 'VAR', 'VCO', 'VDSI', 'VGGL', 'VIAB', 'VIAS', 'VICL', 'VIEW', 'VII', 'VIMC', 'VISI', 'VISN', 'VLCCF', 'VLTC', 'VMEM', 'VNTV', 'VOLC', 'VRD', 'VRML', 'VRNG', 'VRTB', 'VRTU', 'VSAR', 'VSCI', 'VSCP', 'VSI', 'VSLR', 'VSR', 'VTAE', 'VTG', 'VTL', 'VTSS', 'VTTI', 'VVUS', 'VYFC', 'WAC', 'WAGE', 'WAIR', 'WAVX', 'WBAI', 'WBC', 'WBIH', 'WBMD', 'WCG', 'WDR', 'WDTI', 'WEBK', 'WEET', 'WFBI', 'WFD', 'WFM', 'WFT', 'WG', 'WGA', 'WGBS', 'WGP', 'WHX', 'WHZ', 'WIBC', 'WIFI', 'WILN', 'WIN', 'WITE', 'WLB', 'WLH', 'WLRHU', 'WLT', 'WMAR', 'WMGI', 'WMLP', 'WNR', 'WNRL', 'WPCS', 'WPG', 'WPPGY', 'WPT', 'WPX', 'WR', 'WRES', 'WSH', 'WSTC', 'WTR', 'WTSL', 'WUBA', 'WWAV', 'WYN', 'XBKS', 'XCO', 'XEC', 'XGTI', 'XGTIW', 'XIV', 'XL', 'XLRN', 'XLS', 'XNY', 'XON', 'XONE', 'XOOM', 'XOVR', 'XRA', 'XRS', 'XUE', 'XXV', 'YAO', 'YDKN', 'YDLE', 'YGE', 'YHOO', 'YOD', 'YOKU', 'YRCW', 'YUMA', 'YUME', 'YZC', 'ZA', 'ZAGG', 'ZAYO', 'ZBB', 'ZFC', 'ZFGN', 'ZINC', 'ZIONW', 'ZIXI', 'ZLTQ', 'ZMH', 'ZN', 'ZPIN', 'ZQK', 'ZSPH', 'ZU', 'ZX']\n"
     ]
    }
   ],
   "source": [
    "list_tickers = [a['ticker'] for a in article_list]\n",
    "list_tickers = list(dict.fromkeys(list_tickers))\n",
    "stock_data = {}\n",
    "failed_stocks = []\n",
    "end_date = datetime.strptime(max(list_dates), '%Y-%m-%d %H:%M:%S%z') + dt.timedelta(days=5)\n",
    "start_date = datetime.strptime(min(list_dates), '%Y-%m-%d %H:%M:%S%z') - dt.timedelta(days=5)\n",
    "print('pulling stocks...')\n",
    "# data = yf.download(tickers = list_tickers, end=str(end_date.date()), start=str(start_date.date()), progress=True)\n",
    "curr_index = 0\n",
    "TOTAL_TICKERS = len(list_tickers)\n",
    "for t in list_tickers:\n",
    "    arts_ticker = [a['date'] for a in article_list if a['ticker'] == t]\n",
    "    # print(type(arts_ticker[0]))\n",
    "    end_date = datetime.strptime(max(arts_ticker), '%Y-%m-%d %H:%M:%S%z') + dt.timedelta(days=5)\n",
    "    start_date = datetime.strptime(min(arts_ticker), '%Y-%m-%d %H:%M:%S%z') - dt.timedelta(days=5)\n",
    "    try:\n",
    "        data = yf.download(tickers = t, end=str(end_date.date()), start=str(start_date.date()), progress=False, show_errors=False)\n",
    "        if len(data > 0):\n",
    "            stock_data[t] = data\n",
    "            with open('/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/processed-data/' + s + '.json', 'w') as json_file:\n",
    "                json.dump(data.to_json(), json_file)\n",
    "        else:\n",
    "            failed_stocks.append(t)\n",
    "    except:\n",
    "        failed_stocks.append(t)\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_TICKERS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d (%d)\" % ('='*int(20*j), 100*j, curr_index, TOTAL_TICKERS, len(failed_stocks)))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1\n",
    "print(\"Failed stocks = \" + str(failed_stocks))\n",
    "# for a in article_list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'failed_stocks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(failed_stocks)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39m# stock_list_data = [s.to_json() for s in stock_list_data]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000006?line=2'>3</a>\u001b[0m \u001b[39m# dict_stock = dict(zip(stock_list_tickers, stock_list_data))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000006?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m stock_data:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'failed_stocks' is not defined"
     ]
    }
   ],
   "source": [
    "print(failed_stocks)\n",
    "# stock_list_data = [s.to_json() for s in stock_list_data]\n",
    "# dict_stock = dict(zip(stock_list_tickers, stock_list_data))\n",
    "for s in stock_data:\n",
    "    with open('/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/processed-data/' + s + '.json', 'w') as json_file:\n",
    "        json.dump(stock_data[s].to_json(), json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list of articles with associated market info...\n"
     ]
    }
   ],
   "source": [
    "print('Generating list of articles with associated market info...')\n",
    "article_list_updated = [a for a in article_list if not a['ticker'] in failed_stocks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1397891\n",
      "1053226\n",
      "Done, assigning stock data to articles...\n",
      "[                    ] 4% 48147 out of 1053226"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000008?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m (article_date\u001b[39m.\u001b[39mhour \u001b[39m>\u001b[39m \u001b[39m15\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000008?line=10'>11</a>\u001b[0m     day_t \u001b[39m=\u001b[39m day_t \u001b[39m+\u001b[39m dt\u001b[39m.\u001b[39mtimedelta(days\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000008?line=11'>12</a>\u001b[0m \u001b[39mwhile\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mstr\u001b[39m(day_t) \u001b[39min\u001b[39;00m stock_data[a[\u001b[39m'\u001b[39m\u001b[39mticker\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mindex \u001b[39mand\u001b[39;00m day_t \u001b[39m<\u001b[39m \u001b[39mmax\u001b[39;49m(stock_data[a[\u001b[39m'\u001b[39;49m\u001b[39mticker\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39;49mindex)\u001b[39m.\u001b[39mdate()):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000008?line=12'>13</a>\u001b[0m     day_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mtimedelta(days\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/josh/Documents/year-4/thesis/code/kaggle-dataset-training/kaggle.ipynb#ch0000008?line=13'>14</a>\u001b[0m from_stock \u001b[39m=\u001b[39m day_t \u001b[39m-\u001b[39m dt\u001b[39m.\u001b[39mtimedelta(days\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:640\u001b[0m, in \u001b[0;36mDatetimeArray.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/josh/.local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py?line=637'>638</a>\u001b[0m start_i \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39m chunksize\n\u001b[1;32m    <a href='file:///home/josh/.local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py?line=638'>639</a>\u001b[0m end_i \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m((i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m chunksize, length)\n\u001b[0;32m--> <a href='file:///home/josh/.local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py?line=639'>640</a>\u001b[0m converted \u001b[39m=\u001b[39m ints_to_pydatetime(\n\u001b[1;32m    <a href='file:///home/josh/.local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py?line=640'>641</a>\u001b[0m     data[start_i:end_i], tz\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtz, freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfreq, box\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    <a href='file:///home/josh/.local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py?line=641'>642</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/josh/.local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py?line=642'>643</a>\u001b[0m \u001b[39myield from\u001b[39;00m converted\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "article_list_mrkt = []\n",
    "print(len(article_list))\n",
    "print(len(article_list_updated))\n",
    "curr_index = 0\n",
    "TOTAL_ARTS = len(article_list_updated)\n",
    "print('Done, assigning stock data to articles...')\n",
    "for a in article_list_updated:\n",
    "    article_date = datetime.strptime(a['date'], '%Y-%m-%d %H:%M:%S%z')\n",
    "    day_t = article_date.date()\n",
    "    if (article_date.hour > 15):\n",
    "        day_t = day_t + dt.timedelta(days=1)\n",
    "    while (not str(day_t) in stock_data[a['ticker']].index and day_t < max(stock_data[a['ticker']].index).date()):\n",
    "        day_t += dt.timedelta(days=1)\n",
    "    from_stock = day_t - dt.timedelta(days=2)\n",
    "    to_stock = day_t + dt.timedelta(days=1)\n",
    "    while (not str(from_stock) in stock_data[a['ticker']].index and from_stock >= min(stock_data[a['ticker']].index).date()):\n",
    "        from_stock -= dt.timedelta(days=1)\n",
    "    while (not str(to_stock) in stock_data[a['ticker']].index and to_stock <= max(stock_data[a['ticker']].index).date()):\n",
    "        to_stock += dt.timedelta(days=1)\n",
    "    if (to_stock < max(stock_data[a['ticker']].index).date() and from_stock > min(stock_data[a['ticker']].index).date()):\n",
    "        new_art = {\n",
    "            'headline': a['headline'],\n",
    "            'mrkt_info': {\n",
    "                'open': from_stock,\n",
    "                'close': to_stock\n",
    "            },\n",
    "            'date': a['date'],\n",
    "            'ticker': a['ticker']\n",
    "        }\n",
    "        article_list_mrkt.append(new_art)\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_ARTS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d\" % ('='*int(20*j), 100*j, curr_index, TOTAL_ARTS))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
