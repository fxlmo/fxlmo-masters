{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sklearn\n",
    "from scipy.optimize import fminbound\n",
    "from sklearn import preprocessing\n",
    "# import scikit-learn\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# from textblob import TextBlob as tb\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import datetime as dt\n",
    "import nltk\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pandas import DataFrame\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import os\n",
    "import yfinance as yf\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the big list of functions for the actual SESTM computation. We'll use this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "#Hyper params -- ROLLING WINDOW IN FUTUERe\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "en_words = set(nltk.corpus.words.words())\n",
    "\n",
    "# the complete sestm function list\n",
    "def calc_returns(article):\n",
    "    returns = float(article['mrkt_info']['open']) - float(article['mrkt_info']['close'])\n",
    "    sgn_a = -1\n",
    "    if (returns > 0): # add -1 if returns are 0 or less, 1 otherwise\n",
    "        sgn_a = 1\n",
    "    return (returns, sgn_a)\n",
    "\n",
    "def text_to_bow(text):\n",
    "    readable_text = text.lower()\n",
    "    # print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "    # substitute non alphabet chars (new lines become spaces)\n",
    "    readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "    readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "    # sub multiple spaces with one space\n",
    "    readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "    # tokenise text\n",
    "    words = nltk.wordpunct_tokenize(readable_text)\n",
    "    bow_art = {}\n",
    "    # lemmatised_words = []\n",
    "    if len(words) > 0:\n",
    "        # lemmatise, remove non-english, and remove stopwords\n",
    "        for w in words:\n",
    "            rootword = lemmatizer.lemmatize(w, pos=\"v\")\n",
    "            # rootword = w\n",
    "            if rootword not in STOP_WORDS and rootword in en_words:\n",
    "                # lemmatised_words.append(rootword)\n",
    "                if rootword in bow_art:\n",
    "                    bow_art[rootword] += 1\n",
    "                else:\n",
    "                    bow_art[rootword] = 1\n",
    "        # convert to bag of words\n",
    "        # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # for l in lemmatised_words:\n",
    "        #     if l in bow_art:\n",
    "        #         bow_art[l] += 1\n",
    "        #     else:\n",
    "        #         bow_art[l] = 1\n",
    "    \n",
    "    return bow_art\n",
    "\n",
    "def calc_f(d, sgn):\n",
    "    pos_j = {}  #j occuring in positive article\n",
    "    total_j = {}#j occuring in any article\n",
    "    f = {}      #fraction of positive occurrences\n",
    "    for i in range(len(d)):\n",
    "        for w in d[i]:\n",
    "            # pos_sent = sgn[i]\n",
    "            pos_sent = 0\n",
    "            if (sgn[i] == 1): pos_sent = 1\n",
    "            if w in total_j:\n",
    "                total_j[w] += d[i][w]\n",
    "                pos_j[w] += d[i][w]*pos_sent\n",
    "            else:\n",
    "                total_j[w] = d[i][w]\n",
    "                pos_j[w] = d[i][w]*pos_sent\n",
    "            f[w] = pos_j[w]/total_j[w]\n",
    "    return (pos_j, total_j, f)\n",
    "\n",
    "def gen_sent_word_list(total_j,sgn,f):\n",
    "    pi = sum(sgn_i > 0 for sgn_i in sgn)/len(sgn)\n",
    "    print(pi)\n",
    "    sentiment_words = [] # S\n",
    "    neutral_words = []   # N\n",
    "    for i in total_j:\n",
    "        if ((f[i] >= pi + ALPHA_PLUS or f[i] <= pi - ALPHA_MINUS) and total_j[i] >= KAPPA and len(i) > 1):\n",
    "            sentiment_words.append(i)\n",
    "        else:\n",
    "            neutral_words.append(i)\n",
    "    return(sentiment_words, neutral_words)\n",
    "\n",
    "# Calculates p_i\n",
    "def calc_p(y):\n",
    "    p = [0] * len(y)\n",
    "    for i, x in enumerate(sorted(range(len(y)), key=lambda y_lam: y[y_lam])):\n",
    "        p[x] = float((i+1)/(len(y)))\n",
    "    return p\n",
    "\n",
    "# Calculates s_i\n",
    "def calc_s(sentiment_words, d):\n",
    "    s = []                                          # ith element corresponds to total count of sentiment charged words for document i\n",
    "    d_s = []                                        # ith element corresponds to list of word counts for each of the sentiment charged words for document i\n",
    "    for doc in d:\n",
    "        s.append(sum(doc.get(val,0) for val in sentiment_words))\n",
    "        d_s.append([doc.get(val,0) for val in sentiment_words])\n",
    "    return (s, d_s)\n",
    "\n",
    "# Calculates h_i\n",
    "def calc_h(sentiment_words, d, s, d_s):\n",
    "    h = np.zeros((len(d), len(sentiment_words)))    # ith element corresponds to |S|x1 vector of word frequencies divided by total sentiment words in doc i\n",
    "\n",
    "    for i in range(len(d)):\n",
    "        # subvector of sentiment words in d_i\n",
    "        if (s[i] == 0) :\n",
    "            h[i] = np.zeros(len(sentiment_words)).transpose()\n",
    "        else:\n",
    "            h[i] = np.array([(j/s[i]) for j in d_s[i]]).transpose()\n",
    "    return h\n",
    "\n",
    "# Calculates O\n",
    "def calc_o(p,h):\n",
    "    p_inv = [(1-val) for val in p]\n",
    "    W = np.column_stack((p, p_inv))\n",
    "    W = W.transpose()\n",
    "    ww = np.matmul(W,W.transpose())\n",
    "    w2 = np.matmul(W.transpose(), inv(ww))\n",
    "    O = np.matmul(h.transpose(),w2)\n",
    "    O[O < 0] = 0 # remove negative entries of O\n",
    "    O = O.transpose()\n",
    "    # Normalise O columns to have l1 norm\n",
    "    O = sklearn.preprocessing.normalize(O,norm='l1')\n",
    "    O = O.transpose()\n",
    "    return O\n",
    "\n",
    "# lam = 3 is what i normally use\n",
    "def equation_to_solve(p_solve, O, new_bow, sentiment_words, new_s, lam):\n",
    "    i = 0\n",
    "    equation = 0\n",
    "    for j in sentiment_words:\n",
    "        # a = (new_bow.get(j,0) * math.log(new_p*O[i][0] + (1-new_p)*O[i][1]))\n",
    "        d_j = new_bow.get(j,0)\n",
    "        in_log = p_solve*O[i][0] + (1-p_solve)*O[i][1]\n",
    "        if not in_log == 0:\n",
    "            equation += d_j * math.log(p_solve*O[i][0] + (1-p_solve)*O[i][1])\n",
    "\n",
    "        i += 1\n",
    "        # i += 1/new_s + lam * (new_p*(1-new_p))\n",
    "\n",
    "    # if new_s == 0:\n",
    "    #     new_s = 1\n",
    "    equation /= new_s\n",
    "    equation += lam*(p_solve*(1-p_solve))\n",
    "    equation *= -1 #flip equation for argmin\n",
    "    return equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import all of the headlines from kaggle and pull the required stock information to compile a json list of articles like we have normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1400470 lines generating 1397891 usable headlines\n"
     ]
    }
   ],
   "source": [
    "#loop through list of files\n",
    "article_list = []\n",
    "file_name = './archive/analyst_ratings_processed.csv'\n",
    "with open(file_name) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and len(row) == 4:\n",
    "            new_art = {\n",
    "                'headline': row[1],\n",
    "                'date': row[2],\n",
    "                'ticker': row[3]\n",
    "            }\n",
    "            article_list.append(new_art)\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines generating {len(article_list)} usable headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a function to list how many articles we have per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date 2009-04-27 14:39:00-04:00 and max date 2020-06-11 15:32:00-04:00\n",
      "No. articles in 2009: 10915\n",
      "No. articles in 2010: 57302\n",
      "No. articles in 2011: 89757\n",
      "No. articles in 2012: 82527\n",
      "No. articles in 2013: 81226\n",
      "No. articles in 2014: 82745\n",
      "No. articles in 2015: 87227\n",
      "No. articles in 2016: 100007\n",
      "No. articles in 2017: 91921\n",
      "No. articles in 2018: 120679\n",
      "No. articles in 2019: 127941\n",
      "No. articles in 2020: 95286\n"
     ]
    }
   ],
   "source": [
    "list_dates = [a['date'] for a in article_list]\n",
    "print(f'Min date {min(list_dates)} and max date {max(list_dates)}')\n",
    "for year in range(2009,2021):\n",
    "    year_count = len([a for a in article_list if (a['date'] <= str(year+1) + '-01-01' and a['date'] >= str(year) + '-01-01')])\n",
    "    print(f'No. articles in {year}: {year_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to pull stock information about the tickers. It should already be downlaoded in `processed-data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling stocks...\n",
      "[                    ] 1% 63 out of 6192 (21)"
     ]
    }
   ],
   "source": [
    "list_tickers = [a['ticker'] for a in article_list]\n",
    "list_tickers = list(dict.fromkeys(list_tickers))\n",
    "stock_data = {}\n",
    "failed_stocks = []\n",
    "end_date = datetime.strptime(max(list_dates), '%Y-%m-%d %H:%M:%S%z') + dt.timedelta(days=5)\n",
    "start_date = datetime.strptime(min(list_dates), '%Y-%m-%d %H:%M:%S%z') - dt.timedelta(days=5)\n",
    "print('pulling stocks...')\n",
    "# data = yf.download(tickers = list_tickers, end=str(end_date.date()), start=str(start_date.date()), progress=True)\n",
    "curr_index = 0\n",
    "TOTAL_TICKERS = len(list_tickers)\n",
    "for t in list_tickers:\n",
    "    arts_ticker = [a['date'] for a in article_list if a['ticker'] == t]\n",
    "    # print(type(arts_ticker[0]))\n",
    "    end_date = datetime.strptime(max(arts_ticker), '%Y-%m-%d %H:%M:%S%z') + dt.timedelta(days=5)\n",
    "    start_date = datetime.strptime(min(arts_ticker), '%Y-%m-%d %H:%M:%S%z') - dt.timedelta(days=5)\n",
    "    try:\n",
    "        data = yf.download(tickers = t, end=str(end_date.date()), start=str(start_date.date()), progress=False, show_errors=False)\n",
    "        if len(data > 0):\n",
    "            stock_data[t] = data\n",
    "        else:\n",
    "            failed_stocks.append(t)\n",
    "    except:\n",
    "        failed_stocks.append(t)\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_TICKERS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d (%d)\" % ('='*int(20*j), 100*j, curr_index, TOTAL_TICKERS, len(failed_stocks)))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1\n",
    "print(\"Failed stocks = \" + str(failed_stocks))\n",
    "# for a in article_list:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates the aforementioned file so you dont spend 2 hours downloading every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump stock data (probably dont do this tho lol, it takes up a fair bit of space i won't lie)\n",
    "# print(failed_stocks)\n",
    "# stock_list_data = [s.to_json() for s in stock_list_data]\n",
    "# dict_stock = dict(zip(stock_list_tickers, stock_list_data))\n",
    "for s in stock_data:\n",
    "    with open('./processed-data/' + s + '.json', 'w') as json_file:\n",
    "        json.dump(stock_data[s].to_json(), json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stock are private, so we are unable to pull stock information about these tickers. This segment removes any of the articles with these tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list of articles with associated market info...\n"
     ]
    }
   ],
   "source": [
    "print('Generating list of articles with associated market info...')\n",
    "failed_stocks = ['AAN', 'AAV', 'AAVL', 'ABAC', 'ABCW', 'ABDC', 'ABGB', 'ABTL', 'ABX', 'ABY', 'ACAS', 'ACAT', 'ACCU', 'ACE', 'ACG', 'ACHN', 'ACMP', 'ACPW', 'ACSF', 'ACT', 'ACTS', 'ACXM', 'ADAT', 'ADEP', 'ADGE', 'ADHD', 'ADK', 'ADMS', 'ADNC', 'ADRA', 'ADVS', 'AEC', 'AEGN', 'AEGR', 'AEPI', 'AETI', 'AF', 'AFA', 'AFC', 'AFFX', 'AFH', 'AFOP', 'AGC', 'AGII', 'AGN', 'AGNCB', 'AGOL', 'AGU', 'AHC', 'AHP', 'AI', 'AIB', 'AIRM', 'AIXG', 'AKAO', 'AKER', 'AKG', 'AKP', 'AKRX', 'AKS', 'ALDR', 'ALDW', 'ALJ', 'ALLB', 'ALQA', 'ALSK', 'ALTV', 'ALU', 'ALXA', 'ALXN', 'AMAG', 'AMBR', 'AMCC', 'AMCO', 'AMDA', 'AMFW', 'AMIC', 'AMID', 'AMPS', 'AMRB', 'AMRE', 'AMRI', 'AMSG', 'AMTG', 'AMZG', 'ANAC', 'ANAD', 'ANCI', 'AND', 'ANH', 'ANW', 'AOI', 'AOL', 'APAGF', 'APC', 'APF', 'API', 'APL', 'APOL', 'APP', 'APPY', 'APRI', 'APSA', 'AQQ', 'AQXP', 'ARCI', 'ARCX', 'ARDM', 'AREX', 'ARGS', 'ARIA', 'ARIS', 'ARMH', 'ARO', 'ARPI', 'ARQL', 'ARRS', 'ARRY', 'ARTX', 'ASBI', 'ASCMA', 'ASFI', 'ASMI', 'ASNA', 'ASPX', 'AST', 'AT', 'ATE', 'ATHN', 'ATK', 'ATL', 'ATLS', 'ATML', 'ATNY', 'ATRM', 'ATTU', 'ATU', 'ATV', 'ATW', 'AUMA', 'AUMAU', 'AUQ', 'AUXL', 'AV', 'AVG', 'AVH', 'AVHI', 'AVIV', 'AVL', 'AVNR', 'AVOL', 'AVP', 'AVX', 'AXE', 'AXJS', 'AXLL', 'AXN', 'AXPW', 'AXX', 'AYR', 'AZIA', 'BAA', 'BABS', 'BABY', 'BAF', 'BAGR', 'BALT', 'BAMM', 'BAS', 'BASI', 'BBCN', 'BBF', 'BBG', 'BBK', 'BBLU', 'BBNK', 'BBRC', 'BBRY', 'BBT', 'BBX', 'BCA', 'BCOM', 'BCR', 'BDBD', 'BDCV', 'BDE', 'BDGE', 'BEAT', 'BEE', 'BEL', 'BF', 'BFR', 'BFY', 'BGCA', 'BGG', 'BHBK', 'BHI', 'BHL', 'BID', 'BIK', 'BIN', 'BIND', 'BIOA', 'BIOD', 'BIOS', 'BIRT', 'BITA', 'BKJ', 'BKK', 'BKMU', 'BKS', 'BKYF', 'BLOX', 'BLT', 'BLVD', 'BLVDU', 'BMR', 'BMTC', 'BNCL', 'BNCN', 'BOBE', 'BOCH', 'BOFI', 'BONA', 'BONE', 'BONT', 'BORN', 'BOTA', 'BOXC', 'BPFH', 'BPFHW', 'BPI', 'BPL', 'BPOPN', 'BQH', 'BRAF', 'BRAQ', 'BRAZ', 'BRCD', 'BRCM', 'BRDR', 'BREW', 'BRK', 'BRKS', 'BRLI', 'BRSS', 'BRXX', 'BSCG', 'BSD', 'BSDM', 'BSE', 'BSFT', 'BSI', 'BSTC', 'BT', 'BTE', 'BTUI', 'BUNL', 'BUNT', 'BVA', 'BVSN', 'BVX', 'BWC', 'BWINA', 'BWINB', 'BWLD', 'BWS', 'BXE', 'BXS', 'BZC', 'BZM', 'CAB', 'CACGU', 'CACQ', 'CADC', 'CADT', 'CAFE', 'CAK', 'CAM', 'CAP', 'CAPN', 'CARB', 'CARO', 'CART', 'CAS', 'CASM', 'CATM', 'CAW', 'CBAK', 'CBB', 'CBDE', 'CBF', 'CBG', 'CBIN', 'CBK', 'CBLI', 'CBM', 'CBMG', 'CBMX', 'CBNJ', 'CBPO', 'CBPX', 'CBR', 'CBRX', 'CBS', 'CBSHP', 'CBST', 'CCC', 'CCCL', 'CCCR', 'CCE', 'CCG', 'CCSC', 'CCV', 'CCX', 'CCXE', 'CDC', 'CDI', 'CECO', 'CEL', 'CELGZ', 'CEMP', 'CERE', 'CERU', 'CETV', 'CFD', 'CFN', 'CFNL', 'CFP', 'CFRXW', 'CFRXZ', 'CGG', 'CGI', 'CGIX', 'CH', 'CHA', 'CHEV', 'CHFC', 'CHK', 'CHKE', 'CHL', 'CHLN', 'CHMT', 'CHOC', 'CHOP', 'CHSP', 'CHU', 'CHXF', 'CHYR', 'CIE', 'CIFC', 'CIMT', 'CISG', 'CIU', 'CJES', 'CKEC', 'CKH', 'CKP', 'CKSW', 'CLAC', 'CLC', 'CLCT', 'CLD', 'CLDN', 'CLGX', 'CLI', 'CLMS', 'CLNT', 'CLNY', 'CLRX', 'CLTX', 'CLUB', 'CLY', 'CMCSK', 'CMD', 'CMFN', 'CMGE', 'CMLP', 'CMN', 'CMSB', 'CNBKA', 'CNCO', 'CNDA', 'CNDO', 'CNIT', 'CNNX', 'CNTF', 'CNV', 'CNW', 'CNYD', 'COB', 'COBK', 'COCO', 'CODE', 'COH', 'COOL', 'COR', 'CORE', 'COSI', 'COT', 'COV', 'COVR', 'COVS', 'CPAH', 'CPGI', 'CPHD', 'CPHR', 'CPL', 'CPN', 'CPST', 'CPTA', 'CRAY', 'CRBQ', 'CRC', 'CRCM', 'CRD', 'CRDC', 'CRDS', 'CRDT', 'CRED', 'CREE', 'CRME', 'CRR', 'CRRC', 'CRRS', 'CRV', 'CRWN', 'CRZO', 'CSC', 'CSFL', 'CSG', 'CSH', 'CSJ', 'CSOD', 'CSRE', 'CSS', 'CST', 'CSUN', 'CTCT', 'CTF', 'CTL', 'CTNN', 'CTRL', 'CTRX', 'CTV', 'CTWS', 'CU', 'CUB', 'CUI', 'CUNB', 'CUO', 'CUR', 'CVA', 'CVC', 'CVD', 'CVOL', 'CVSL', 'CVTI', 'CWEI', 'CXA', 'CXO', 'CXP', 'CY', 'CYBX', 'CYN', 'CYNI', 'CYOU', 'CYT', 'CYTX', 'CZFC', 'CZZ', 'DAEG', 'DAKP', 'DANG', 'DARA', 'DATA', 'DATE', 'DBBR', 'DBMX', 'DBU', 'DBUK', 'DCA', 'DCIX', 'DCM', 'DCT', 'DDC', 'DDR', 'DEG', 'DEJ', 'DEPO', 'DEST', 'DF', 'DFRG', 'DFT', 'DGAS', 'DGI', 'DGSE', 'DHRM', 'DIVI', 'DKT', 'DLBL', 'DLPH', 'DM', 'DMD', 'DMND', 'DNB', 'DNBF', 'DNKN', 'DNO', 'DNR', 'DO', 'DOM', 'DOVR', 'DPLO', 'DPM', 'DPRX', 'DPW', 'DRAD', 'DRAM', 'DRC', 'DRII', 'DRL', 'DRNA', 'DRWI', 'DRYS', 'DSCI', 'DSCO', 'DSE', 'DSKX', 'DSKY', 'DSUM', 'DTLK', 'DTSI', 'DTV', 'DUC', 'DV', 'DVCR', 'DVD', 'DW', 'DWA', 'DWRE', 'DWTI', 'DXB', 'DXJF', 'DXJR', 'DXKW', 'DXM', 'DXPS', 'DYAX', 'DYN', 'EAC', 'EBIO', 'EBSB', 'ECA', 'ECR', 'ECT', 'ECTE', 'EDE', 'EDR', 'EDS', 'EE', 'EEHB', 'EEI', 'EEME', 'EEML', 'EFF', 'EFII', 'EFUT', 'EGAS', 'EGI', 'EGLT', 'EGOV', 'EGRW', 'EGT', 'EHIC', 'EIGI', 'EIV', 'EJ', 'ELGX', 'ELLI', 'ELNK', 'ELOS', 'ELRC', 'ELX', 'EMBB', 'EMCD', 'EMCI', 'EMCR', 'EMDI', 'EMES', 'EMEY', 'EMQ', 'EMSA', 'EMXX', 'ENBL', 'ENFC', 'ENGN', 'ENH', 'ENI', 'ENL', 'ENOC', 'ENRJ', 'ENT', 'ENVI', 'ENY', 'ENZY', 'EOC', 'EOPN', 'EOX', 'EPAX', 'EPE', 'EPIQ', 'EPRS', 'EQM', 'EQY', 'ERA', 'ERB', 'ERO', 'EROS', 'ERS', 'ESBF', 'ESCR', 'ESL', 'ESR', 'ESSX', 'ESV', 'ESYS', 'ETAK', 'ETE', 'ETF', 'ETFC', 'ETH', 'ETM', 'ETRM', 'EU', 'EV', 'EVAL', 'EVAR', 'EVBS', 'EVDY', 'EVEP', 'EVJ', 'EVLV', 'EVRY', 'EWCS', 'EWHS', 'EWSS', 'EXA', 'EXAM', 'EXAR', 'EXFO', 'EXL', 'EXLP', 'EXXI', 'FAC', 'FAV', 'FBNK', 'FBSS', 'FCAU', 'FCE', 'FCF', 'FCH', 'FCHI', 'FCLF', 'FCS', 'FCSC', 'FDEF', 'FDI', 'FDML', 'FDO', 'FEIC', 'FELP', 'FES', 'FEYE', 'FFG', 'FGP', 'FHCO', 'FHY', 'FI', 'FIG', 'FISH', 'FLIR', 'FLML', 'FLTX', 'FLXN', 'FLY', 'FMD', 'FMER', 'FNBC', 'FNFG', 'FNFV', 'FNJN', 'FNSR', 'FOMX', 'FPO', 'FPRX', 'FRAN', 'FRED', 'FREE', 'FRM', 'FRP', 'FRS', 'FRSH', 'FSAM', 'FSBK', 'FSC', 'FSGI', 'FSIC', 'FSL', 'FSNN', 'FSRV', 'FSYS', 'FTD', 'FTR', 'FTT', 'FUEL', 'FULL', 'FUR', 'FWM', 'FWV', 'FXCB', 'FXCM', 'GAI', 'GAINO', 'GALTU', 'GARS', 'GBSN', 'GCA', 'GCAP', 'GCH', 'GCVRZ', 'GDAY', 'GDEF', 'GDF', 'GDP', 'GEVA', 'GFA', 'GFIG', 'GFNCP', 'GFY', 'GG', 'GGAC', 'GGE', 'GGM', 'GGOV', 'GGP', 'GHDX', 'GHI', 'GIG', 'GIMO', 'GK', 'GKNT', 'GLDC', 'GLDX', 'GLOG', 'GLPW', 'GLUU', 'GMCR', 'GMFS', 'GMK', 'GMLP', 'GMO', 'GMT', 'GMZ', 'GNC', 'GNI', 'GNMK', 'GNVC', 'GOMO', 'GOODO', 'GOODP', 'GOV', 'GPIC', 'GPM', 'GPOR', 'GPX', 'GRAM', 'GRH', 'GRIF', 'GRN', 'GRO', 'GRT', 'GSB', 'GSH', 'GSI', 'GSOL', 'GST', 'GSVC', 'GTAA', 'GTI', 'GTIV', 'GTT', 'GTU', 'GTWN', 'GTXI', 'GUID', 'GUR', 'GURX', 'GWL', 'GWPH', 'GWR', 'GY', 'GYEN', 'GZT', 'HABT', 'HAR', 'HAWKB', 'HBHC', 'HBK', 'HBNK', 'HBOS', 'HCAC', 'HCAP', 'HCBK', 'HCHC', 'HCLP', 'HCN', 'HCOM', 'HCP', 'HDRA', 'HDRAU', 'HDS', 'HDY', 'HEB', 'HELI', 'HEOP', 'HF', 'HFBC', 'HFFC', 'HGG', 'HGI', 'HGR', 'HGT', 'HH', 'HIIQ', 'HILL', 'HILO', 'HK', 'HKOR', 'HKTV', 'HLS', 'HLSS', 'HME', 'HMPR', 'HMSY', 'HNH', 'HNR', 'HNSN', 'HNT', 'HOS', 'HOTRW', 'HPJ', 'HPT', 'HPTX', 'HPY', 'HRC', 'HRS', 'HRT', 'HSEA', 'HSGX', 'HSNI', 'HSOL', 'HSP', 'HTCH', 'HTF', 'HTR', 'HTS', 'HTWO', 'HTWR', 'HTZ', 'HUB', 'HVB', 'HW', 'HWAY', 'HWCC', 'HYGS', 'HYH', 'IACI', 'IBCA', 'IBCC', 'IBKC', 'IBLN', 'ICA', 'ICB', 'ICEL', 'ICI', 'ICN', 'ICON', 'IDHB', 'IDI', 'IDSY', 'IDTI', 'IDXJ', 'IEC', 'IFMI', 'IFON', 'IFT', 'IG', 'IGLD', 'IGTE', 'IGU', 'IID', 'IILG', 'IJNK', 'IKAN', 'IKGH', 'IKNX', 'IL', 'IM', 'IMDZ', 'IMI', 'IMMU', 'IMN', 'IMNP', 'IMPR', 'IMRS', 'IMS', 'IMUC', 'INAP', 'INB', 'INCR', 'IND', 'INF', 'INFA', 'ININ', 'INP', 'INPH', 'INS', 'INSY', 'INTL', 'INVN', 'INWK', 'INXN', 'INXX', 'INY', 'IOC', 'IOIL', 'IOT', 'IPCI', 'IPCM', 'IPD', 'IPF', 'IPHS', 'IPK', 'IPU', 'IPW', 'IQNT', 'IRC', 'IRDMB', 'IRDMZ', 'IRE', 'IRET', 'IRF', 'IRR', 'ISCA', 'ISF', 'ISH', 'ISIL', 'ISIS', 'ISLE', 'ISNS', 'ISRL', 'ISSI', 'IST', 'ITC', 'ITF', 'ITG', 'ITLT', 'ITLY', 'ITR', 'IVAN', 'IVOP', 'IXYS', 'JAH', 'JASN', 'JASO', 'JAXB', 'JCOM', 'JCP', 'JDD', 'JDSU', 'JEC', 'JFC', 'JGBD', 'JGBL', 'JGBS', 'JGBT', 'JGW', 'JIVE', 'JJA', 'JJM', 'JJN', 'JJP', 'JJT', 'JJU', 'JMEI', 'JMLP', 'JMP', 'JNS', 'JO', 'JOEZ', 'JONE', 'JOY', 'JPEP', 'JPP', 'JRN', 'JSC', 'JST', 'JTA', 'JTD', 'JTP', 'JUNR', 'JW', 'JYN', 'KATE', 'KBIO', 'KBSF', 'KBWC', 'KBWI', 'KCAP', 'KCC', 'KCG', 'KEF', 'KEG', 'KEM', 'KFH', 'KFI', 'KFX', 'KHI', 'KIN', 'KITE', 'KKD', 'KME', 'KNL', 'KNM', 'KONA', 'KONE', 'KOOL', 'KORS', 'KROO', 'KRU', 'KST', 'KSU', 'KTEC', 'KUTV', 'KWT', 'KYO', 'KYTH', 'KZ', 'LABC', 'LABL', 'LACO', 'LAS', 'LBF', 'LBIX', 'LBMH', 'LBY', 'LDL', 'LDR', 'LDRH', 'LEI', 'LEVY', 'LEVYU', 'LG', 'LGCY', 'LGF', 'LINE', 'LION', 'LIOX', 'LIQD', 'LLDM', 'LLEM', 'LLEX', 'LLSC', 'LLTC', 'LM', 'LMCA', 'LMCB', 'LMCK', 'LMIA', 'LMLP', 'LMNX', 'LMOS', 'LMRK', 'LNBB', 'LNCO', 'LNKD', 'LOCK', 'LOCM', 'LOGM', 'LOJN', 'LONG', 'LOOK', 'LORL', 'LPHI', 'LPT', 'LPTN', 'LRAD', 'LRE', 'LSC', 'LSG', 'LTM', 'LTS', 'LTXB', 'LUX', 'LVLT', 'LVNTA', 'LWC', 'LXFT', 'LXK', 'MAGS', 'MAMS', 'MBFI', 'MBLX', 'MBLY', 'MBRG', 'MBTF', 'MBVT', 'MCC', 'MCF', 'MCGC', 'MCOX', 'MCP', 'MCRL', 'MCUR', 'MCV', 'MCZ', 'MDAS', 'MDCA', 'MDCO', 'MDD', 'MDGN', 'MDLY', 'MDP', 'MDSO', 'MDSY', 'MDVN', 'MDVXU', 'MDW', 'MEA', 'MEET', 'MEG', 'MELA', 'MELR', 'MEN', 'MENT', 'MEP', 'MERU', 'MES', 'METR', 'MFI', 'MFLX', 'MFNC', 'MFRI', 'MFRM', 'MFSF', 'MFT', 'MGCD', 'MGH', 'MGLN', 'MGN', 'MGT', 'MHE', 'MHFI', 'MHGC', 'MHR', 'MIE', 'MIFI', 'MIK', 'MIL', 'MILL', 'MINI', 'MJN', 'MKTO', 'MLHR', 'MLNK', 'MLNX', 'MLPJ', 'MLPL', 'MM', 'MMAC', 'MNE', 'MNGA', 'MNI', 'MNK', 'MNRK', 'MNTA', 'MOBI', 'MOC', 'MOCO', 'MOG', 'MOKO', 'MOLG', 'MON', 'MONY', 'MORE', 'MPEL', 'MPET', 'MPO', 'MRD', 'MRH', 'MRKT', 'MRVC', 'MSBF', 'MSF', 'MSG', 'MSLI', 'MSO', 'MSON', 'MSP', 'MSTX', 'MTK', 'MTS', 'MTSC', 'MTSL', 'MTSN', 'MTT', 'MTU', 'MUH', 'MUS', 'MVC', 'MVG', 'MVNR', 'MW', 'MWE', 'MWIV', 'MWV', 'MXIM', 'MXWL', 'MY', 'MYCC', 'MYF', 'MYL', 'MYOS', 'MZF', 'NADL', 'NAME', 'NANO', 'NAO', 'NATL', 'NAV', 'NBBC', 'NBG', 'NBL', 'NBS', 'NBTF', 'NCB', 'NCFT', 'NCI', 'NCIT', 'NCQ', 'NDRO', 'NE', 'NEOT', 'NETE', 'NEWM', 'NEWS', 'NFEC', 'NGHC', 'NGHCP', 'NGLS', 'NHF', 'NHTB', 'NJ', 'NJV', 'NKA', 'NKY', 'NLNK', 'NMBL', 'NMO', 'NMRX', 'NMY', 'NNA', 'NNC', 'NOR', 'NORD', 'NPBC', 'NPD', 'NPP', 'NPSP', 'NRCIA', 'NRF', 'NRX', 'NSAM', 'NSH', 'NSPH', 'NSR', 'NTI', 'NTK', 'NTL', 'NTLS', 'NTN', 'NTRSP', 'NTT', 'NTX', 'NU', 'NUTR', 'NVDQ', 'NVGN', 'NVSL', 'NVX', 'NWHM', 'NWY', 'NXQ', 'NXR', 'NXTDW', 'NXTM', 'NYLD', 'NYMTP', 'NYNY', 'NYV', 'OAK', 'OAKS', 'OB', 'OCIR', 'OCLS', 'OCR', 'OCRX', 'OGXI', 'OHAI', 'OHGI', 'OHRP', 'OIBR', 'OILT', 'OKS', 'OKSB', 'OLO', 'OMAM', 'OME', 'OMED', 'OMG', 'OMN', 'ONEF', 'ONFC', 'ONNN', 'ONP', 'ONTY', 'ONVI', 'OPB', 'OPHT', 'OPWR', 'OPXA', 'ORB', 'ORBC', 'ORBK', 'OREX', 'ORIT', 'ORM', 'ORPN', 'OSGB', 'OSHC', 'OSIR', 'OSM', 'OSN', 'OTEL', 'OTIV', 'OUTR', 'OVTI', 'OWW', 'OXFD', 'OXLCO', 'OZM', 'OZRK', 'PACD', 'PAF', 'PAGG', 'PAH', 'PAL', 'PARN', 'PAY', 'PBCP', 'PBIB', 'PBM', 'PBMD', 'PBY', 'PCI', 'PCL', 'PCLN', 'PCMI', 'PCO', 'PCP', 'PCYC', 'PDII', 'PDLI', 'PE', 'PEGI', 'PEIX', 'PENX', 'PEOP', 'PER', 'PERF', 'PERM', 'PES', 'PETM', 'PETX', 'PFBI', 'PFK', 'PFNX', 'PFPT', 'PGI', 'PGM', 'PGN', 'PGNX', 'PHF', 'PHII', 'PHIIK', 'PHMD', 'PICO', 'PIH', 'PIP', 'PIR', 'PJC', 'PKD', 'PKO', 'PKY', 'PLCM', 'PLKI', 'PLMT', 'PLND', 'PLNR', 'PLPM', 'PLT', 'PLTM', 'PMBC', 'PMC', 'PMCS', 'PMFG', 'PNRA', 'PNTR', 'PNX', 'POL', 'POM', 'POPE', 'POT', 'POWR', 'POZN', 'PPHM', 'PPHMP', 'PPO', 'PPP', 'PPR', 'PPS', 'PQ', 'PRAH', 'PRAN', 'PRB', 'PRCP', 'PRE', 'PRGN', 'PRGX', 'PRLS', 'PRSC', 'PRTO', 'PRXI', 'PRXL', 'PRY', 'PSAU', 'PSBH', 'PSDV', 'PSEM', 'PSG', 'PSTB', 'PSTR', 'PSUN', 'PTBI', 'PTIE', 'PTLA', 'PTM', 'PTP', 'PTRY', 'PTX', 'PULB', 'PVA', 'PVTB', 'PWE', 'PWRD', 'PWX', 'PXMC', 'PXR', 'PXSC', 'PZI', 'Q', 'QADA', 'QCAN', 'QDEM', 'QDXU', 'QEH', 'QEM', 'QEP', 'QEPM', 'QGBR', 'QIHU', 'QKOR', 'QLGC', 'QLIK', 'QLTB', 'QLTC', 'QLTI', 'QLTY', 'QSII', 'QTM', 'QTS', 'QTWN', 'QTWW', 'QUNR', 'QVCA', 'QVCB', 'QXUS', 'RAI', 'RALY', 'RAS', 'RATE', 'RAVN', 'RAX', 'RBC', 'RBL', 'RBPAA', 'RBS', 'RBY', 'RCAP', 'RCPI', 'RCPT', 'RDC', 'RDEN', 'RDS', 'RECN', 'REE', 'REMY', 'REN', 'RENT', 'RESI', 'REXI', 'REXX', 'RFT', 'RGDO', 'RGDX', 'RGSE', 'RHP', 'RHS', 'RHT', 'RIC', 'RICE', 'RIF', 'RIGP', 'RIO', 'RIOM', 'RIT', 'RIVR', 'RJET', 'RJF', 'RKT', 'RKUS', 'RLD', 'RLH', 'RLOC', 'RLOG', 'RLYP', 'RNA', 'RNDY', 'RNET', 'RNF', 'RNN', 'ROC', 'ROIAK', 'ROIQ', 'ROIQU', 'ROIQW', 'ROKA', 'RORO', 'ROSE', 'ROVI', 'ROX', 'RP', 'RPAI', 'RPRX', 'RPRXW', 'RPRXZ', 'RPTP', 'RPX', 'RRST', 'RSE', 'RSH', 'RSO', 'RST', 'RSTI', 'RT', 'RTEC', 'RTGN', 'RTI', 'RTIX', 'RTK', 'RTR', 'RTRX', 'RUBI', 'RUK', 'RVBD', 'RVLT', 'RVM', 'RWC', 'RWV', 'RWXL', 'RXDX', 'RXII', 'RYL', 'S', 'SAAS', 'SAEX', 'SAJA', 'SALT', 'SAPE', 'SARA', 'SBBX', 'SBGL', 'SBRAP', 'SBSA', 'SBY', 'SCAI', 'SCHB', 'SCHF', 'SCHL', 'SCHP', 'SCLN', 'SCMP', 'SCOK', 'SCPB', 'SCSC', 'SCSS', 'SCTY', 'SCU', 'SCVL', 'SCZ', 'SD', 'SDLP', 'SDR', 'SDRL', 'SDT', 'SE', 'SEIC', 'SEMG', 'SEMI', 'SERV', 'SEV', 'SFB', 'SFG', 'SFL', 'SFLA', 'SFLY', 'SFN', 'SFNC', 'SFS', 'SFXE', 'SGAR', 'SGB', 'SGBK', 'SGF', 'SGG', 'SGM', 'SGNL', 'SGNT', 'SGOC', 'SGY', 'SGYP', 'SGYPU', 'SGYPW', 'SHLD', 'SHLO', 'SHM', 'SHOR', 'SHOS', 'SIAL', 'SIBC', 'SIFI', 'SIGM', 'SIMG', 'SINA', 'SINO', 'SIRO', 'SKBI', 'SKH', 'SKIS', 'SKUL', 'SLCT', 'SLH', 'SLI', 'SLTC', 'SLW', 'SLXP', 'SMACU', 'SMI', 'SMK', 'SMRT', 'SMTP', 'SMTX', 'SN', 'SNAK', 'SNBC', 'SNC', 'SNDK', 'SNH', 'SNOW', 'SNR', 'SNSS', 'SNTA', 'SORL', 'SPA', 'SPAN', 'SPAR', 'SPEX', 'SPF', 'SPKE', 'SPLS', 'SPNC', 'SPP', 'SPPR', 'SPPRO', 'SPRT', 'SPU', 'SPW', 'SQBG', 'SQBK', 'SQI', 'SQNM', 'SRSC', 'SSE', 'SSFN', 'SSH', 'SSI', 'SSLT', 'SSN', 'SSNI', 'SSRG', 'SSRI', 'SSS', 'SSW', 'STAY', 'STCK', 'STEM', 'STI', 'STJ', 'STML', 'STMP', 'STNR', 'STO', 'STR', 'STRN', 'STRZA', 'STRZB', 'STS', 'SUBK', 'SUNE', 'SUSQ', 'SUTR', 'SVLC', 'SWHC', 'SWY', 'SXCP', 'SXE', 'SYA', 'SYKE', 'SYMC', 'SYMX', 'SYNC', 'SYRG', 'SYRX', 'SYT', 'SYUT', 'SYX', 'SZYM', 'TAHO', 'TAOM', 'TAS', 'TASR', 'TAT', 'TAX', 'TAXI', 'TBAR', 'TBIO', 'TCAP', 'TCBIP', 'TCCA', 'TCHI', 'TCK', 'TCO', 'TCP', 'TCPI', 'TCRD', 'TDA', 'TDD', 'TDI', 'TE', 'TEAR', 'TECD', 'TECU', 'TEG', 'TERP', 'TESO', 'TEU', 'TFM', 'TFSCU', 'TGC', 'TGD', 'TGE', 'THOR', 'THRX', 'THTI', 'TI', 'TICC', 'TIF', 'TIK', 'TIME', 'TINY', 'TISA', 'TIVO', 'TKAI', 'TKMR', 'TLF', 'TLI', 'TLL', 'TLLP', 'TLM', 'TLMR', 'TLO', 'TLP', 'TLR', 'TMH', 'TMK', 'TNAV', 'TNDQ', 'TNGO', 'TOF', 'TOO', 'TOT', 'TOWR', 'TPI', 'TPLM', 'TPRE', 'TPUB', 'TRAK', 'TRCB', 'TRCH', 'TRCO', 'TRF', 'TRGT', 'TRIL', 'TRIV', 'TRK', 'TRLA', 'TRMR', 'TRND', 'TROV', 'TROVU', 'TROVW', 'TRR', 'TRTL', 'TRW', 'TRXC', 'TSL', 'TSLF', 'TSO', 'TSRA', 'TSRE', 'TSS', 'TST', 'TSU', 'TSYS', 'TTF', 'TTFS', 'TTHI', 'TTPH', 'TTS', 'TUBE', 'TUES', 'TVIZ', 'TWC', 'TWMC', 'TXTR', 'TYC', 'TYPE', 'UACL', 'UAM', 'UBC', 'UBIC', 'UBNK', 'UBSH', 'UCD', 'UCFC', 'UCP', 'UDF', 'UFS', 'UHN', 'UIL', 'ULTI', 'ULTR', 'UMX', 'UN', 'UNIS', 'UNT', 'UNXL', 'UPIP', 'UPL', 'URZ', 'USAG', 'USAT', 'USBI', 'USCR', 'USG', 'USMD', 'USMI', 'USTR', 'UTEK', 'UTIW', 'UWTI', 'VA', 'VAL', 'VAR', 'VCO', 'VDSI', 'VGGL', 'VIAB', 'VIAS', 'VICL', 'VIEW', 'VII', 'VIMC', 'VIP', 'VISI', 'VISN', 'VLCCF', 'VLTC', 'VMEM', 'VNTV', 'VOLC', 'VRD', 'VRML', 'VRNG', 'VRTB', 'VRTU', 'VSAR', 'VSCI', 'VSCP', 'VSI', 'VSLR', 'VSR', 'VTAE', 'VTG', 'VTL', 'VTSS', 'VTTI', 'VVUS', 'VYFC', 'WAC', 'WAGE', 'WAIR', 'WAVX', 'WBAI', 'WBC', 'WBIH', 'WBMD', 'WCG', 'WDR', 'WDTI', 'WEBK', 'WEET', 'WFBI', 'WFD', 'WFM', 'WFT', 'WG', 'WGA', 'WGBS', 'WGP', 'WHX', 'WHZ', 'WIBC', 'WIFI', 'WILN', 'WIN', 'WITE', 'WLB', 'WLH', 'WLRHU', 'WLT', 'WMAR', 'WMGI', 'WMLP', 'WNR', 'WNRL', 'WPCS', 'WPG', 'WPPGY', 'WPT', 'WPX', 'WR', 'WRES', 'WSH', 'WSTC', 'WTR', 'WTSL', 'WUBA', 'WWAV', 'WYN', 'XBKS', 'XCO', 'XEC', 'XGTI', 'XGTIW', 'XIV', 'XL', 'XLRN', 'XLS', 'XNY', 'XON', 'XONE', 'XOOM', 'XOVR', 'XRA', 'XRS', 'XUE', 'XXV', 'YAO', 'YDKN', 'YDLE', 'YGE', 'YHOO', 'YOD', 'YOKU', 'YRCW', 'YUMA', 'YUME', 'YZC', 'ZA', 'ZAGG', 'ZAYO', 'ZBB', 'ZFC', 'ZFGN', 'ZINC', 'ZIONW', 'ZIXI', 'ZLTQ', 'ZMH', 'ZN', 'ZPIN', 'ZQK', 'ZSPH', 'ZU', 'ZX']\n",
    "article_list_updated = [a for a in article_list if not a['ticker'] in failed_stocks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment pulls the stock data from the file rather than YAHOO FINANCE API. It's much much much quicker so if you have the file, run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling file no 4144"
     ]
    }
   ],
   "source": [
    "# recreate list of stock information\n",
    "ticker_path = './processed-data/'\n",
    "pathlist = Path(ticker_path).rglob('*.json')\n",
    "stock_data = {}\n",
    "i = 0\n",
    "for path in pathlist:\n",
    "    with open(str(path)) as json_file:\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"pulling file no %d\" % (i))\n",
    "        sys.stdout.flush()#\n",
    "        i += 1\n",
    "        data = json.load(json_file)\n",
    "        data = json.loads(data)\n",
    "        datetime_dict = {}\n",
    "        for line in data:\n",
    "            datetime_line = {}\n",
    "            if line == 'Close':\n",
    "                for k in data[line]:\n",
    "                    # print(str(datetime.fromtimestamp(float(k)/1000.0)))\n",
    "                    datetime_line[datetime.fromtimestamp(float(k)/1000.0).date()] = data[line][k]\n",
    "                datetime_dict[line] = datetime_line \n",
    "        stock_data[os.path.basename(str(path))[:-5]] = DataFrame.from_dict(datetime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the stock data all pulled, we now need to assign these to the appropriate article. It's important to note that there is not stock market information available for every single day, so if there is a day missing, we just take the next available day with information and call this day $t$. Doing this for each article is incredibly time consuming and takes around 30 hours, so instead, we create a lookup table here. Inside, it has the adjusted stock data required using the stock ticker and the date the article was released as the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100% 4144 out of 4145"
     ]
    }
   ],
   "source": [
    "day_t_data = {}\n",
    "curr_index = 0\n",
    "TOTAL_ARTS = len(stock_data)\n",
    "for t in stock_data:\n",
    "    # print(t)\n",
    "    ticker_date_data = {}\n",
    "    start = min(stock_data[t].index)\n",
    "    end = max(stock_data[t].index)\n",
    "    curr_date = start\n",
    "    while curr_date < end:\n",
    "        day_t = curr_date\n",
    "        while not day_t in stock_data[t].index and day_t <= end:\n",
    "            day_t += dt.timedelta(days=1)\n",
    "        from_stock = day_t - dt.timedelta(days=2)\n",
    "        to_stock = day_t + dt.timedelta(days=1)\n",
    "        while not from_stock in stock_data[t].index and from_stock >= start:\n",
    "            from_stock -= dt.timedelta(days=1)\n",
    "        while not to_stock in stock_data[t].index and to_stock <= end:\n",
    "            to_stock += dt.timedelta(days=1)\n",
    "        if(from_stock > start and to_stock < end):\n",
    "            ticker_date_data[curr_date] = {'day_t': day_t, 'from_stock': stock_data[t]['Close'][from_stock], 'to_stock': stock_data[t]['Close'][to_stock]}\n",
    "        curr_date += dt.timedelta(days=1)\n",
    "    day_t_data[t] = ticker_date_data\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_ARTS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d\" % ('='*int(20*j), 100*j, curr_index, TOTAL_ARTS))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the tools at our disposal, we now need to compile the updated list of articles. This segment puts them in a file for later use, so we don't even need to do any of the things previously ever again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1397891\n",
      "1048485\n",
      "Done, assigning stock data to articles...\n"
     ]
    }
   ],
   "source": [
    "article_list_mrkt = []\n",
    "print(len(article_list))\n",
    "print(len(article_list_updated))\n",
    "curr_index = 0\n",
    "TOTAL_ARTS = len(article_list_updated)\n",
    "print('Done, assigning stock data to articles...')\n",
    "with open('archive/analyst_ratings_mrkt.csv', 'w',newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['headline','date','ticker','open','close'])\n",
    "    for a in article_list_updated:\n",
    "        article_date = datetime.strptime(a['date'], '%Y-%m-%d %H:%M:%S%z')\n",
    "        day_t = article_date.date()\n",
    "        if (article_date.hour > 15):\n",
    "            day_t = day_t + dt.timedelta(days=1)\n",
    "        if (day_t in day_t_data[a['ticker']]):\n",
    "            day_t = day_t_data[a['ticker']][day_t]['day_t']\n",
    "            from_stock = day_t_data[a['ticker']][day_t]['from_stock']\n",
    "            to_stock = day_t_data[a['ticker']][day_t]['to_stock']\n",
    "            new_art = {\n",
    "                'headline': a['headline'],\n",
    "                'mrkt_info': {\n",
    "                    'open': from_stock,\n",
    "                    'close': to_stock\n",
    "                },\n",
    "                'date': a['date'],\n",
    "                'ticker': a['ticker']\n",
    "            }\n",
    "            csvwriter.writerow([a['headline'], a['date'], a['ticker'], str(from_stock), str(to_stock)])\n",
    "            # article_list_mrkt.append(new_art)\n",
    "    # else:\n",
    "    #     print(\"ticker \" + t + \" date \" + str(day_t) + \" not in range\")\n",
    "    # sys.stdout.write('\\r')\n",
    "    # j = (curr_index + 1) / TOTAL_ARTS\n",
    "    # sys.stdout.write(\"%d out of %d articles processed\" % (curr_index, TOTAL_ARTS))\n",
    "    # sys.stdout.flush()\n",
    "    # curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# START HERE IF `archive/analyst_ratings_mrkt.csv` EXISTS\n",
    "Now that we have everything at our disposal, we can actually begin testing some things. Let's first read the file into the notebook so we can begin doing some computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1027533 lines generating 1027533 usable headlines\n"
     ]
    }
   ],
   "source": [
    "# import market data articles\n",
    "#loop through list of files\n",
    "article_list = []\n",
    "file_name = './archive/analyst_ratings_mrkt.csv'\n",
    "with open(file_name) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            new_art = {\n",
    "                'headline': row[0],\n",
    "                'date': row[1],\n",
    "                'mrkt_info': {\n",
    "                    'open': row[3],\n",
    "                    'close': row[4]\n",
    "                },\n",
    "                'ticker': row[2]\n",
    "            }\n",
    "            article_list.append(new_art)\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count-1} lines generating {len(article_list)} usable headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The paper 'predicting stock returns with text data' used a rolling window method to train and validate window. They train models of the matrix and minimise a loss function. The tuning parameters are $(\\alpha_+, \\alpha_-, \\kappa, \\lambda)$. In each case, a limited number of choices were used.\n",
    "- For the two $\\alpha$ params, they are set such that the number of words in each group (both positive and negative) is either 25, 50, or 100.\n",
    "- For $\\kappa$, five choices are considered, at: 86%, 88%, 90%, 92% and 94% quantiles of the count distribution each year\n",
    "- For $\\lambda$, three choices: 1, 5, and 10\n",
    "- This totals 45 configurations\n",
    "\n",
    "The loss function they used is the $\\ell^1$-norm of the differences between estimated article sentiment scores and the corresponding standardised return ranks for all events in the validation sample.\n",
    "\n",
    "They had data all the way from 1989, but we don't have that luxury, with articles spanning 2009-2020. For this reason, we will use slightly different window sizes. We will attempt to estimate and validate the model around 11 times, meaning the whole window will be a three years total total. The training sample will be two years and the validation the following year. We then move the window along by four months, giving us enough evaluations\n",
    "\n",
    "**IMPORTANT NOTES**\n",
    "- It is entirely possible that the news articles in a certain month will carry certain sentiment, so investigation may be necessary into random 8 months of the year.\n",
    "- 2009 and 2010 are comparatively smaller datasets, so I will combine these two years to create a single 'year' `(!!to implement. I am currently ignoring 2009)`\n",
    "- 2020 has a similar number of headlines as the other years, but only has headlines up to June. The training and validation samples in this year will just be half that of other years..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-39c7bc35a193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcurr_month\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mopening_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_year\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_month\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlist_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M:%S%z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mutc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUTC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'article_list' is not defined"
     ]
    }
   ],
   "source": [
    "# reset global variables\n",
    "sgn = []\n",
    "y = []\n",
    "dates = []\n",
    "global_bow = {}\n",
    "d = []\n",
    "kappa_configs   = [86, 88, 90, 92, 94]\n",
    "alpha_configs   = [25,50,100]\n",
    "lambda_configs  = [1,5,10]\n",
    "curr_year = 2010\n",
    "curr_month = 1\n",
    "opening_date = datetime(curr_year,curr_month,1,0,0,0,0)\n",
    "list_dates = [a['date'] for a in article_list]\n",
    "end_date = datetime.strptime(max(list_dates), '%Y-%m-%d %H:%M:%S%z')\n",
    "utc = pytz.UTC\n",
    "\n",
    "\n",
    "while(datetime(curr_year, curr_month, 1, 0,0,0,0).date() < end_date.date()):\n",
    "    #SELECT ARTICLES\n",
    "    val_date_start = datetime(curr_year+2,curr_month,1,0,0,0,0)\n",
    "    val_date_end = datetime(curr_year+3,curr_month,1,0,0,0,0)\n",
    "    print('Selecting articles by date...')\n",
    "    print(\"New training window: \" + str(curr_year) + '-' + str(curr_month) + '-01' + \" to \" + str(curr_year+2) + '-' + str(curr_month) + '-01')\n",
    "    print(\"New validation window: \" + str(curr_year+2) + '-' + str(curr_month) + '-01' + \" to \"  + str(curr_year+3) + '-' + str(curr_month) + '-01')\n",
    "    training_arts   = [a for a in article_list if (a['date'] >= str(curr_year) + '-' + str(curr_month) + '-01' and a['date'] < str(curr_year+2) + '-' + str(curr_month) + '-01')]\n",
    "    validation_arts = [a for a in article_list if (a['date'] >= str(curr_year+2) + '-' + str(curr_month) + '-01' and a['date'] < str(curr_year+3) + '-' + str(curr_month) + '-01')]\n",
    "\n",
    "    #PRE-PROCESS\n",
    "    print('Preprocessing data...')\n",
    "    train_d = []\n",
    "    train_sgn = []\n",
    "    train_y = []\n",
    "    val_d = []\n",
    "    val_sgn =[]\n",
    "    val_y = []\n",
    "    for train_a in training_arts:\n",
    "        train_bow = text_to_bow(train_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(train_a)\n",
    "        train_d.append(train_bow)\n",
    "        train_sgn.append(sgn_a)\n",
    "        train_y.append(returns)\n",
    "    for val_a in validation_arts:\n",
    "        val_bow = text_to_bow(val_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(val_a)\n",
    "        val_d.append(val_bow)\n",
    "        val_y.append(returns)\n",
    "    \n",
    "    train_pi = sum(sgn_i > 0 for sgn_i in train_sgn)/len(train_sgn)\n",
    "\n",
    "    #PARAM GRID\n",
    "    print('Beginning trials')\n",
    "    trials = []\n",
    "    for alpha in alpha_configs:\n",
    "        for KAPPA in kappa_configs:\n",
    "            for LAM in lambda_configs:\n",
    "                #TRAINING\n",
    "                print('training...')\n",
    "                (pos_j, total_j, f) = calc_f(train_d, train_sgn)\n",
    "                kappa_percentile = np.percentile(np.array(list(total_j.values())),KAPPA) # return the nth percentile of all appearances for KAPPA\n",
    "\n",
    "                #calculate alpha vals\n",
    "                ALPHA_PLUS = ALPHA_MINUS = 0\n",
    "                while(len([w for w in total_j if f[w] >= train_pi + ALPHA_PLUS and total_j[w] >= kappa_percentile]) >= alpha):\n",
    "                    ALPHA_PLUS += 0.01\n",
    "                while(len([w for w in total_j if f[w] <= train_pi - ALPHA_MINUS and total_j[w] >= kappa_percentile]) >= alpha):\n",
    "                    ALPHA_MINUS += 0.01\n",
    "                sentiment_words = [w for w in total_j if ((f[w] >= train_pi + ALPHA_PLUS or f[w] <= train_pi - ALPHA_MINUS) and total_j[w] >= kappa_percentile)]\n",
    "                print(len(sentiment_words))\n",
    "\n",
    "                p           = calc_p(train_y)\n",
    "                (s, d_s)    = calc_s(sentiment_words, train_d)\n",
    "                h           = calc_h(sentiment_words, train_d, s, d_s)\n",
    "                O           = calc_o(p,h)\n",
    "\n",
    "                #VALIDATING\n",
    "                print('validating...')\n",
    "                error_arr = np.array(0)\n",
    "                val_p = calc_p(val_y)\n",
    "                for val_index in range(len(val_d)):\n",
    "                    est_p = 0.5\n",
    "                    val_bow = val_d[val_index]\n",
    "\n",
    "                    testing_s = sum(val_bow.get(w,0) for w in sentiment_words)\n",
    "                    if (testing_s > 0):\n",
    "                        est_p = fminbound(equation_to_solve, 0, 1, (O,val_bow, sentiment_words,testing_s,LAM))\n",
    "                    error_arr = np.append(error_arr, est_p - val_p[val_index])\n",
    "                normalised_error = np.linalg.norm(error_arr, 1)\n",
    "                trials.append({\n",
    "                    'alpha_plus': ALPHA_PLUS,\n",
    "                    'alpha_minus': ALPHA_MINUS,\n",
    "                    'kappa': KAPPA,\n",
    "                    'lam': LAM,\n",
    "                    'o': O,\n",
    "                    'sentiment_words': sentiment_words,\n",
    "                    'norm_err': normalised_error\n",
    "                })\n",
    "                print('Trial complete')\n",
    "    \n",
    "    #RECORD BEST CONFIG\n",
    "    print(\"Saving best config...\")\n",
    "    best_config = min(trials, key=lambda x:x['norm_err'])\n",
    "    with open('data/configurations.csv', 'a') as csv_file:\n",
    "        csv_file.write([str(opening_date), str(best_config['alpha_plus']), str(best_config['alpha_minus']), str(best_config['kappa']), str(best_config['lam'])])\n",
    "    with open('data/word-lists/' + str(opening_date) + '.csv', 'w') as csv_file:\n",
    "        #write header\n",
    "        csv_file.write(['word', 'O+ value', 'O- value'])\n",
    "    with open('data/word-lists/' + str(opening_date) + '.csv', 'a') as csv_file:\n",
    "        for ind in range(len(best_config['sentiment_words'])):\n",
    "            csv_file.write([str(best_config['sentiment_words'][ind]), str(best_config['o'][ind][0]), str(best_config['o'][ind][1])])\n",
    "\n",
    "    #record start date, sentiment words, O, and params probably\n",
    "    #i think maybe start date and params in a different file to sentiment words and O.\n",
    "\n",
    "    #MOVE WINDOW\n",
    "    curr_month += 4\n",
    "    if (curr_month > 12):\n",
    "        curr_month = 1\n",
    "        curr_year += 1\n",
    "    # opening_date += relativedelta(months=4)\n",
    "\n",
    "#2020 SPECIAL CASE\n",
    "\n",
    "\n",
    "TOTAL_ARTS = len(article_list)\n",
    "curr_index = 0\n",
    "curr_thousand = 0\n",
    "for a in article_list:\n",
    "    raw_html = a['headline']\n",
    "    if(raw_html):\n",
    "        bow_art = text_to_bow(raw_html)\n",
    "        (returns, sgn_a) = calc_returns(a)\n",
    "        dates.append(a['date'])\n",
    "        d.append(bow_art)\n",
    "        sgn.append(sgn_a)\n",
    "        y.append(returns)\n",
    "    if curr_index >= curr_thousand:\n",
    "        curr_thousand += 10000\n",
    "        sys.stdout.write('\\r')\n",
    "        j = (curr_index + 1) / TOTAL_ARTS\n",
    "        sys.stdout.write(\"%d out of %d articles processed\" % (curr_index, TOTAL_ARTS))\n",
    "        sys.stdout.flush()\n",
    "    curr_index += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
