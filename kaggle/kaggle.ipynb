{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sklearn\n",
    "from scipy.optimize import fminbound\n",
    "from sklearn import preprocessing\n",
    "# import scikit-learn\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# from textblob import TextBlob as tb\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "est = pytz.timezone('US/Eastern')\n",
    "import datetime as dt\n",
    "import nltk\n",
    "import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pandas import DataFrame\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import os\n",
    "import yfinance as yf\n",
    "import json\n",
    "import sys\n",
    "# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from itertools import islice\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "#mulitthreading imports\n",
    "import logging\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the big list of functions for the actual SESTM computation. We'll use this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "#Hyper params -- ROLLING WINDOW IN FUTUERe\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "en_words = set(nltk.corpus.words.words())\n",
    "\n",
    "# the complete sestm function list\n",
    "def calc_returns(article):\n",
    "    # ABSOLUTE VALUE\n",
    "    # returns = float(article['mrkt_info']['close']) - float(article['mrkt_info']['open'])\n",
    "    # PERCENTAGE VALUE\n",
    "    returns = float(article['mrkt_info']['close'])/float(article['mrkt_info']['open']) - 1\n",
    "    sgn_a = -1\n",
    "    if (returns > 0): # add -1 if returns are 0 or less, 1 otherwise\n",
    "        sgn_a = 1\n",
    "    return (returns, sgn_a)\n",
    "\n",
    "def text_to_bow(text):\n",
    "    readable_text = text.lower()\n",
    "    # print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "    # substitute non alphabet chars (new lines become spaces)\n",
    "    readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "    readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "    # sub multiple spaces with one space\n",
    "    readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "    # tokenise text\n",
    "    words = nltk.wordpunct_tokenize(readable_text)\n",
    "    bow_art = {}\n",
    "    # lemmatised_words = []\n",
    "    if len(words) > 0:\n",
    "        # lemmatise, remove non-english, and remove stopwords\n",
    "        for w in words:\n",
    "            lemmatized_word = lemmatizer.lemmatize(w)\n",
    "            rootword = stemmer.stem(lemmatized_word)\n",
    "            if rootword not in en_words and lemmatized_word in en_words:\n",
    "                rootword = lemmatized_word\n",
    "            # rootword = w\n",
    "            if rootword not in STOP_WORDS and rootword in en_words:\n",
    "                # lemmatised_words.append(rootword)\n",
    "                if rootword in bow_art:\n",
    "                    bow_art[rootword] += 1\n",
    "                else:\n",
    "                    bow_art[rootword] = 1\n",
    "        # convert to bag of words\n",
    "        # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # for l in lemmatised_words:\n",
    "        #     if l in bow_art:\n",
    "        #         bow_art[l] += 1\n",
    "        #     else:\n",
    "        #         bow_art[l] = 1\n",
    "    \n",
    "    return bow_art\n",
    "\n",
    "def text_to_bow_bigram(text):\n",
    "    readable_text = text.lower()\n",
    "    # print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "    # substitute non alphabet chars (new lines become spaces)\n",
    "    readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "    readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "    # sub multiple spaces with one space\n",
    "    readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "    # tokenise text\n",
    "    words = nltk.wordpunct_tokenize(readable_text)\n",
    "    bow_art = {}\n",
    "    # lemmatised_words = []\n",
    "    if len(words) > 0:\n",
    "        # lemmatise, remove non-english, and remove stopwords\n",
    "        prev_word = ''\n",
    "        for w in words:\n",
    "            lemmatized_word = lemmatizer.lemmatize(w)\n",
    "            rootword = stemmer.stem(lemmatized_word)\n",
    "            if rootword not in en_words and lemmatized_word in en_words:\n",
    "                rootword = lemmatized_word\n",
    "            # rootword = w\n",
    "            if rootword not in STOP_WORDS and rootword in en_words:\n",
    "                # lemmatised_words.append(rootword)\n",
    "                if prev_word != '':\n",
    "                    new_bigram = prev_word + \" \" + rootword\n",
    "                    if new_bigram in bow_art:\n",
    "                        bow_art[new_bigram] += 1\n",
    "                    else:\n",
    "                        bow_art[new_bigram] = 1\n",
    "                prev_word = rootword\n",
    "        # convert to bag of words\n",
    "        # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # for l in lemmatised_words:\n",
    "        #     if l in bow_art:\n",
    "        #         bow_art[l] += 1\n",
    "        #     else:\n",
    "        #         bow_art[l] = 1\n",
    "    \n",
    "    return bow_art\n",
    "\n",
    "def calc_f(d, sgn):\n",
    "    pos_j = {}  #j occuring in positive article\n",
    "    total_j = {}#j occuring in any article\n",
    "    f = {}      #fraction of positive occurrences\n",
    "    for i in range(len(d)):\n",
    "        for w in d[i]:\n",
    "            # pos_sent = sgn[i]\n",
    "            pos_sent = 0\n",
    "            if (sgn[i] == 1): pos_sent = 1\n",
    "            if w in total_j:\n",
    "                total_j[w] += d[i][w]\n",
    "                pos_j[w] += d[i][w]*pos_sent\n",
    "            else:\n",
    "                total_j[w] = d[i][w]\n",
    "                pos_j[w] = d[i][w]*pos_sent\n",
    "            f[w] = pos_j[w]/total_j[w]\n",
    "    return (pos_j, total_j, f)\n",
    "\n",
    "def gen_sent_word_list(total_j,sgn,f):\n",
    "    pi = sum(sgn_i > 0 for sgn_i in sgn)/len(sgn)\n",
    "    print(pi)\n",
    "    sentiment_words = [] # S\n",
    "    neutral_words = []   # N\n",
    "    for i in total_j:\n",
    "        if ((f[i] >= pi + ALPHA_PLUS or f[i] <= pi - ALPHA_MINUS) and total_j[i] >= KAPPA and len(i) > 1):\n",
    "            sentiment_words.append(i)\n",
    "        else:\n",
    "            neutral_words.append(i)\n",
    "    return(sentiment_words, neutral_words)\n",
    "\n",
    "# Calculates p_i\n",
    "def calc_p(y):\n",
    "    p = [0] * len(y)\n",
    "    for i, x in enumerate(sorted(range(len(y)), key=lambda y_lam: y[y_lam])):\n",
    "        p[x] = float((i+1)/(len(y)))\n",
    "    return p\n",
    "\n",
    "# Calculates s_i\n",
    "def calc_s(sentiment_words, d):\n",
    "    s = []                                          # ith element corresponds to total count of sentiment charged words for document i\n",
    "    d_s = []                                        # ith element corresponds to list of word counts for each of the sentiment charged words for document i\n",
    "    for doc in d:\n",
    "        s.append(sum(doc.get(val,0) for val in sentiment_words))\n",
    "        d_s.append([doc.get(val,0) for val in sentiment_words])\n",
    "    return (s, d_s)\n",
    "\n",
    "# Calculates h_i\n",
    "def calc_h(sentiment_words, d, s, d_s):\n",
    "    h = np.zeros((len(d), len(sentiment_words)))    # ith element corresponds to |S|x1 vector of word frequencies divided by total sentiment words in doc i\n",
    "\n",
    "    for i in range(len(d)):\n",
    "        # subvector of sentiment words in d_i\n",
    "        if (s[i] == 0) :\n",
    "            h[i] = np.zeros(len(sentiment_words)).transpose()\n",
    "        else:\n",
    "            h[i] = np.array([(j/s[i]) for j in d_s[i]]).transpose()\n",
    "    return h\n",
    "\n",
    "# Calculates O\n",
    "def calc_o(p,h):\n",
    "    p_inv = [(1-val) for val in p]\n",
    "    W = np.column_stack((p, p_inv))\n",
    "    W = W.transpose()\n",
    "    ww = np.matmul(W,W.transpose())\n",
    "    w2 = np.matmul(W.transpose(), inv(ww))\n",
    "    O = np.matmul(h.transpose(),w2)\n",
    "    O[O < 0] = 0 # remove negative entries of O\n",
    "    O = O.transpose()\n",
    "    # Normalise O columns to have l1 norm\n",
    "    O[0] = O[0]/np.linalg.norm(O[0], ord=1)\n",
    "    O[1] = O[1]/np.linalg.norm(O[1], ord=1)\n",
    "    # O = sklearn.preprocessing.normalize(O,norm='l1')\n",
    "    O = O.transpose()\n",
    "    return O\n",
    "\n",
    "# lam = 3 is what i normally use\n",
    "def equation_to_solve(p_solve, O, new_bow, sentiment_words, new_s, lam):\n",
    "    # i = 0\n",
    "    # equation = 0\n",
    "\n",
    "    equation = sum([new_bow.get(sentiment_words[i],0)* math.log(p_solve*float(O[i][0]) + (1-p_solve)*float(O[i][1])) for i in range(len(sentiment_words))])/new_s + lam*(p_solve*(1-p_solve))\n",
    "\n",
    "    # for j in sentiment_words:\n",
    "    #     # a = (new_bow.get(j,0) * math.log(new_p*O[i][0] + (1-new_p)*O[i][1]))\n",
    "    #     d_j = new_bow.get(j,0)\n",
    "    #     in_log = p_solve*O[i][0] + (1-p_solve)*O[i][1]\n",
    "    #     if not in_log == 0:\n",
    "    #         equation += d_j * math.log(p_solve*O[i][0] + (1-p_solve)*O[i][1])\n",
    "\n",
    "    #     i += 1\n",
    "    #     # i += 1/new_s + lam * (new_p*(1-new_p))\n",
    "\n",
    "    # if new_s == 0:\n",
    "    #     new_s = 1\n",
    "    # equation /= new_s\n",
    "    # equation += lam*(p_solve*(1-p_solve))\n",
    "\n",
    "    equation *= -1 #flip equation for argmin\n",
    "    return equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here if articles have not been processed\n",
    "Now we will import all of the headlines from kaggle and pull the required stock information to compile a json list of articles like we have normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1400470 lines generating 1397891 usable headlines\n"
     ]
    }
   ],
   "source": [
    "#loop through list of files\n",
    "article_list = []\n",
    "file_name = './archive/analyst_ratings_processed.csv'\n",
    "with open(file_name, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and len(row) == 4:\n",
    "            new_art = {\n",
    "                'headline': row[1],\n",
    "                'date': row[2],\n",
    "                'ticker': row[3]\n",
    "            }\n",
    "            article_list.append(new_art)\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines generating {len(article_list)} usable headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a function to list how many articles we have per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates = [a['date'] for a in article_list]\n",
    "print(f'Min date {min(list_dates)} and max date {max(list_dates)}')\n",
    "for year in range(2009,2021):\n",
    "    year_count = len([a for a in article_list if (a['date'] <= str(year+1) + '-01-01' and a['date'] >= str(year) + '-01-01')])\n",
    "    print(f'No. articles in {year}: {year_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to pull stock information about the tickers. It should already be downlaoded in `processed-data`, but if not, this will generate that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tickers = [a['ticker'] for a in article_list]\n",
    "list_tickers = list(dict.fromkeys(list_tickers))\n",
    "stock_data = {}\n",
    "failed_stocks = []\n",
    "end_date = max(list_dates) + dt.timedelta(days=5)\n",
    "start_date = min(list_dates) - dt.timedelta(days=5)\n",
    "print('pulling stocks...')\n",
    "# data = yf.download(tickers = list_tickers, end=str(end_date.date()), start=str(start_date.date()), progress=True)\n",
    "curr_index = 0\n",
    "TOTAL_TICKERS = len(list_tickers)\n",
    "for t in list_tickers:\n",
    "    arts_ticker = [datetime.strptime(a['date'],'%Y-%m-%d %H:%M:%S%z') for a in article_list if a['ticker'] == t]\n",
    "    # print(type(arts_ticker[0]))\n",
    "    end_date = max(arts_ticker) + dt.timedelta(days=5)\n",
    "    start_date = min(arts_ticker) - dt.timedelta(days=5)\n",
    "    try:\n",
    "        data = yf.download(tickers = t, end=str(end_date.date()), start=str(start_date.date()), progress=False, show_errors=False)\n",
    "        if len(data > 0):\n",
    "            stock_data[t] = data\n",
    "        else:\n",
    "            failed_stocks.append(t)\n",
    "    except:\n",
    "        failed_stocks.append(t)\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_TICKERS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d (%d)\" % ('='*int(20*j), 100*j, curr_index, TOTAL_TICKERS, len(failed_stocks)))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1\n",
    "print(\"Failed stocks = \" + str(failed_stocks))\n",
    "# for a in article_list:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates the aforementioned file so you dont spend 2 hours downloading every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump stock data (probably dont do this tho lol, it takes up a fair bit of space i won't lie)\n",
    "# print(failed_stocks)\n",
    "# stock_list_data = [s.to_json() for s in stock_list_data]\n",
    "# dict_stock = dict(zip(stock_list_tickers, stock_list_data))\n",
    "for s in stock_data:\n",
    "    with open('./processed-data/' + s + '.json', 'w') as json_file:\n",
    "        json.dump(stock_data[s].to_json(), json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stock are private, so we are unable to pull stock information about these tickers. This segment removes any of the articles with these tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list of articles with associated market info...\n"
     ]
    }
   ],
   "source": [
    "print('Generating list of articles with associated market info...')\n",
    "failed_stocks = ['AAN', 'AAV', 'AAVL', 'ABAC', 'ABCW', 'ABDC', 'ABGB', 'ABTL', 'ABX', 'ABY', 'ACAS', 'ACAT', 'ACCU', 'ACE', 'ACG', 'ACHN', 'ACMP', 'ACPW', 'ACSF', 'ACT', 'ACTS', 'ACXM', 'ADAT', 'ADEP', 'ADGE', 'ADHD', 'ADK', 'ADMS', 'ADNC', 'ADRA', 'ADVS', 'AEC', 'AEGN', 'AEGR', 'AEPI', 'AETI', 'AF', 'AFA', 'AFC', 'AFFX', 'AFH', 'AFOP', 'AGC', 'AGII', 'AGN', 'AGNCB', 'AGOL', 'AGU', 'AHC', 'AHP', 'AI', 'AIB', 'AIRM', 'AIXG', 'AKAO', 'AKER', 'AKG', 'AKP', 'AKRX', 'AKS', 'ALDR', 'ALDW', 'ALJ', 'ALLB', 'ALQA', 'ALSK', 'ALTV', 'ALU', 'ALXA', 'ALXN', 'AMAG', 'AMBR', 'AMCC', 'AMCO', 'AMDA', 'AMFW', 'AMIC', 'AMID', 'AMPS', 'AMRB', 'AMRE', 'AMRI', 'AMSG', 'AMTG', 'AMZG', 'ANAC', 'ANAD', 'ANCI', 'AND', 'ANH', 'ANW', 'AOI', 'AOL', 'APAGF', 'APC', 'APF', 'API', 'APL', 'APOL', 'APP', 'APPY', 'APRI', 'APSA', 'AQQ', 'AQXP', 'ARCI', 'ARCX', 'ARDM', 'AREX', 'ARGS', 'ARIA', 'ARIS', 'ARMH', 'ARO', 'ARPI', 'ARQL', 'ARRS', 'ARRY', 'ARTX', 'ASBI', 'ASCMA', 'ASFI', 'ASMI', 'ASNA', 'ASPX', 'AST', 'AT', 'ATE', 'ATHN', 'ATK', 'ATL', 'ATLS', 'ATML', 'ATNY', 'ATRM', 'ATTU', 'ATU', 'ATV', 'ATW', 'AUMA', 'AUMAU', 'AUQ', 'AUXL', 'AV', 'AVG', 'AVH', 'AVHI', 'AVIV', 'AVL', 'AVNR', 'AVOL', 'AVP', 'AVX', 'AXE', 'AXJS', 'AXLL', 'AXN', 'AXPW', 'AXX', 'AYR', 'AZIA', 'BAA', 'BABS', 'BABY', 'BAF', 'BAGR', 'BALT', 'BAMM', 'BAS', 'BASI', 'BBCN', 'BBF', 'BBG', 'BBK', 'BBLU', 'BBNK', 'BBRC', 'BBRY', 'BBT', 'BBX', 'BCA', 'BCOM', 'BCR', 'BDBD', 'BDCV', 'BDE', 'BDGE', 'BEAT', 'BEE', 'BEL', 'BF', 'BFR', 'BFY', 'BGCA', 'BGG', 'BHBK', 'BHI', 'BHL', 'BID', 'BIK', 'BIN', 'BIND', 'BIOA', 'BIOD', 'BIOS', 'BIRT', 'BITA', 'BKJ', 'BKK', 'BKMU', 'BKS', 'BKYF', 'BLOX', 'BLT', 'BLVD', 'BLVDU', 'BMR', 'BMTC', 'BNCL', 'BNCN', 'BOBE', 'BOCH', 'BOFI', 'BONA', 'BONE', 'BONT', 'BORN', 'BOTA', 'BOXC', 'BPFH', 'BPFHW', 'BPI', 'BPL', 'BPOPN', 'BQH', 'BRAF', 'BRAQ', 'BRAZ', 'BRCD', 'BRCM', 'BRDR', 'BREW', 'BRK', 'BRKS', 'BRLI', 'BRSS', 'BRXX', 'BSCG', 'BSD', 'BSDM', 'BSE', 'BSFT', 'BSI', 'BSTC', 'BT', 'BTE', 'BTUI', 'BUNL', 'BUNT', 'BVA', 'BVSN', 'BVX', 'BWC', 'BWINA', 'BWINB', 'BWLD', 'BWS', 'BXE', 'BXS', 'BZC', 'BZM', 'CAB', 'CACGU', 'CACQ', 'CADC', 'CADT', 'CAFE', 'CAK', 'CAM', 'CAP', 'CAPN', 'CARB', 'CARO', 'CART', 'CAS', 'CASM', 'CATM', 'CAW', 'CBAK', 'CBB', 'CBDE', 'CBF', 'CBG', 'CBIN', 'CBK', 'CBLI', 'CBM', 'CBMG', 'CBMX', 'CBNJ', 'CBPO', 'CBPX', 'CBR', 'CBRX', 'CBS', 'CBSHP', 'CBST', 'CCC', 'CCCL', 'CCCR', 'CCE', 'CCG', 'CCSC', 'CCV', 'CCX', 'CCXE', 'CDC', 'CDI', 'CECO', 'CEL', 'CELGZ', 'CEMP', 'CERE', 'CERU', 'CETV', 'CFD', 'CFN', 'CFNL', 'CFP', 'CFRXW', 'CFRXZ', 'CGG', 'CGI', 'CGIX', 'CH', 'CHA', 'CHEV', 'CHFC', 'CHK', 'CHKE', 'CHL', 'CHLN', 'CHMT', 'CHOC', 'CHOP', 'CHSP', 'CHU', 'CHXF', 'CHYR', 'CIE', 'CIFC', 'CIMT', 'CISG', 'CIU', 'CJES', 'CKEC', 'CKH', 'CKP', 'CKSW', 'CLAC', 'CLC', 'CLCT', 'CLD', 'CLDN', 'CLGX', 'CLI', 'CLMS', 'CLNT', 'CLNY', 'CLRX', 'CLTX', 'CLUB', 'CLY', 'CMCSK', 'CMD', 'CMFN', 'CMGE', 'CMLP', 'CMN', 'CMSB', 'CNBKA', 'CNCO', 'CNDA', 'CNDO', 'CNIT', 'CNNX', 'CNTF', 'CNV', 'CNW', 'CNYD', 'COB', 'COBK', 'COCO', 'CODE', 'COH', 'COOL', 'COR', 'CORE', 'COSI', 'COT', 'COV', 'COVR', 'COVS', 'CPAH', 'CPGI', 'CPHD', 'CPHR', 'CPL', 'CPN', 'CPST', 'CPTA', 'CRAY', 'CRBQ', 'CRC', 'CRCM', 'CRD', 'CRDC', 'CRDS', 'CRDT', 'CRED', 'CREE', 'CRME', 'CRR', 'CRRC', 'CRRS', 'CRV', 'CRWN', 'CRZO', 'CSC', 'CSFL', 'CSG', 'CSH', 'CSJ', 'CSOD', 'CSRE', 'CSS', 'CST', 'CSUN', 'CTCT', 'CTF', 'CTL', 'CTNN', 'CTRL', 'CTRX', 'CTV', 'CTWS', 'CU', 'CUB', 'CUI', 'CUNB', 'CUO', 'CUR', 'CVA', 'CVC', 'CVD', 'CVOL', 'CVSL', 'CVTI', 'CWEI', 'CXA', 'CXO', 'CXP', 'CY', 'CYBX', 'CYN', 'CYNI', 'CYOU', 'CYT', 'CYTX', 'CZFC', 'CZZ', 'DAEG', 'DAKP', 'DANG', 'DARA', 'DATA', 'DATE', 'DBBR', 'DBMX', 'DBU', 'DBUK', 'DCA', 'DCIX', 'DCM', 'DCT', 'DDC', 'DDR', 'DEG', 'DEJ', 'DEPO', 'DEST', 'DF', 'DFRG', 'DFT', 'DGAS', 'DGI', 'DGSE', 'DHRM', 'DIVI', 'DKT', 'DLBL', 'DLPH', 'DM', 'DMD', 'DMND', 'DNB', 'DNBF', 'DNKN', 'DNO', 'DNR', 'DO', 'DOM', 'DOVR', 'DPLO', 'DPM', 'DPRX', 'DPW', 'DRAD', 'DRAM', 'DRC', 'DRII', 'DRL', 'DRNA', 'DRWI', 'DRYS', 'DSCI', 'DSCO', 'DSE', 'DSKX', 'DSKY', 'DSUM', 'DTLK', 'DTSI', 'DTV', 'DUC', 'DV', 'DVCR', 'DVD', 'DW', 'DWA', 'DWRE', 'DWTI', 'DXB', 'DXJF', 'DXJR', 'DXKW', 'DXM', 'DXPS', 'DYAX', 'DYN', 'EAC', 'EBIO', 'EBSB', 'ECA', 'ECR', 'ECT', 'ECTE', 'EDE', 'EDR', 'EDS', 'EE', 'EEHB', 'EEI', 'EEME', 'EEML', 'EFF', 'EFII', 'EFUT', 'EGAS', 'EGI', 'EGLT', 'EGOV', 'EGRW', 'EGT', 'EHIC', 'EIGI', 'EIV', 'EJ', 'ELGX', 'ELLI', 'ELNK', 'ELOS', 'ELRC', 'ELX', 'EMBB', 'EMCD', 'EMCI', 'EMCR', 'EMDI', 'EMES', 'EMEY', 'EMQ', 'EMSA', 'EMXX', 'ENBL', 'ENFC', 'ENGN', 'ENH', 'ENI', 'ENL', 'ENOC', 'ENRJ', 'ENT', 'ENVI', 'ENY', 'ENZY', 'EOC', 'EOPN', 'EOX', 'EPAX', 'EPE', 'EPIQ', 'EPRS', 'EQM', 'EQY', 'ERA', 'ERB', 'ERO', 'EROS', 'ERS', 'ESBF', 'ESCR', 'ESL', 'ESR', 'ESSX', 'ESV', 'ESYS', 'ETAK', 'ETE', 'ETF', 'ETFC', 'ETH', 'ETM', 'ETRM', 'EU', 'EV', 'EVAL', 'EVAR', 'EVBS', 'EVDY', 'EVEP', 'EVJ', 'EVLV', 'EVRY', 'EWCS', 'EWHS', 'EWSS', 'EXA', 'EXAM', 'EXAR', 'EXFO', 'EXL', 'EXLP', 'EXXI', 'FAC', 'FAV', 'FBNK', 'FBSS', 'FCAU', 'FCE', 'FCF', 'FCH', 'FCHI', 'FCLF', 'FCS', 'FCSC', 'FDEF', 'FDI', 'FDML', 'FDO', 'FEIC', 'FELP', 'FES', 'FEYE', 'FFG', 'FGP', 'FHCO', 'FHY', 'FI', 'FIG', 'FISH', 'FLIR', 'FLML', 'FLTX', 'FLXN', 'FLY', 'FMD', 'FMER', 'FNBC', 'FNFG', 'FNFV', 'FNJN', 'FNSR', 'FOMX', 'FPO', 'FPRX', 'FRAN', 'FRED', 'FREE', 'FRM', 'FRP', 'FRS', 'FRSH', 'FSAM', 'FSBK', 'FSC', 'FSGI', 'FSIC', 'FSL', 'FSNN', 'FSRV', 'FSYS', 'FTD', 'FTR', 'FTT', 'FUEL', 'FULL', 'FUR', 'FWM', 'FWV', 'FXCB', 'FXCM', 'GAI', 'GAINO', 'GALTU', 'GARS', 'GBSN', 'GCA', 'GCAP', 'GCH', 'GCVRZ', 'GDAY', 'GDEF', 'GDF', 'GDP', 'GEVA', 'GFA', 'GFIG', 'GFNCP', 'GFY', 'GG', 'GGAC', 'GGE', 'GGM', 'GGOV', 'GGP', 'GHDX', 'GHI', 'GIG', 'GIMO', 'GK', 'GKNT', 'GLDC', 'GLDX', 'GLOG', 'GLPW', 'GLUU', 'GMCR', 'GMFS', 'GMK', 'GMLP', 'GMO', 'GMT', 'GMZ', 'GNC', 'GNI', 'GNMK', 'GNVC', 'GOMO', 'GOODO', 'GOODP', 'GOV', 'GPIC', 'GPM', 'GPOR', 'GPX', 'GRAM', 'GRH', 'GRIF', 'GRN', 'GRO', 'GRT', 'GSB', 'GSH', 'GSI', 'GSOL', 'GST', 'GSVC', 'GTAA', 'GTI', 'GTIV', 'GTT', 'GTU', 'GTWN', 'GTXI', 'GUID', 'GUR', 'GURX', 'GWL', 'GWPH', 'GWR', 'GY', 'GYEN', 'GZT', 'HABT', 'HAR', 'HAWKB', 'HBHC', 'HBK', 'HBNK', 'HBOS', 'HCAC', 'HCAP', 'HCBK', 'HCHC', 'HCLP', 'HCN', 'HCOM', 'HCP', 'HDRA', 'HDRAU', 'HDS', 'HDY', 'HEB', 'HELI', 'HEOP', 'HF', 'HFBC', 'HFFC', 'HGG', 'HGI', 'HGR', 'HGT', 'HH', 'HIIQ', 'HILL', 'HILO', 'HK', 'HKOR', 'HKTV', 'HLS', 'HLSS', 'HME', 'HMPR', 'HMSY', 'HNH', 'HNR', 'HNSN', 'HNT', 'HOS', 'HOTRW', 'HPJ', 'HPT', 'HPTX', 'HPY', 'HRC', 'HRS', 'HRT', 'HSEA', 'HSGX', 'HSNI', 'HSOL', 'HSP', 'HTCH', 'HTF', 'HTR', 'HTS', 'HTWO', 'HTWR', 'HTZ', 'HUB', 'HVB', 'HW', 'HWAY', 'HWCC', 'HYGS', 'HYH', 'IACI', 'IBCA', 'IBCC', 'IBKC', 'IBLN', 'ICA', 'ICB', 'ICEL', 'ICI', 'ICN', 'ICON', 'IDHB', 'IDI', 'IDSY', 'IDTI', 'IDXJ', 'IEC', 'IFMI', 'IFON', 'IFT', 'IG', 'IGLD', 'IGTE', 'IGU', 'IID', 'IILG', 'IJNK', 'IKAN', 'IKGH', 'IKNX', 'IL', 'IM', 'IMDZ', 'IMI', 'IMMU', 'IMN', 'IMNP', 'IMPR', 'IMRS', 'IMS', 'IMUC', 'INAP', 'INB', 'INCR', 'IND', 'INF', 'INFA', 'ININ', 'INP', 'INPH', 'INS', 'INSY', 'INTL', 'INVN', 'INWK', 'INXN', 'INXX', 'INY', 'IOC', 'IOIL', 'IOT', 'IPCI', 'IPCM', 'IPD', 'IPF', 'IPHS', 'IPK', 'IPU', 'IPW', 'IQNT', 'IRC', 'IRDMB', 'IRDMZ', 'IRE', 'IRET', 'IRF', 'IRR', 'ISCA', 'ISF', 'ISH', 'ISIL', 'ISIS', 'ISLE', 'ISNS', 'ISRL', 'ISSI', 'IST', 'ITC', 'ITF', 'ITG', 'ITLT', 'ITLY', 'ITR', 'IVAN', 'IVOP', 'IXYS', 'JAH', 'JASN', 'JASO', 'JAXB', 'JCOM', 'JCP', 'JDD', 'JDSU', 'JEC', 'JFC', 'JGBD', 'JGBL', 'JGBS', 'JGBT', 'JGW', 'JIVE', 'JJA', 'JJM', 'JJN', 'JJP', 'JJT', 'JJU', 'JMEI', 'JMLP', 'JMP', 'JNS', 'JO', 'JOEZ', 'JONE', 'JOY', 'JPEP', 'JPP', 'JRN', 'JSC', 'JST', 'JTA', 'JTD', 'JTP', 'JUNR', 'JW', 'JYN', 'KATE', 'KBIO', 'KBSF', 'KBWC', 'KBWI', 'KCAP', 'KCC', 'KCG', 'KEF', 'KEG', 'KEM', 'KFH', 'KFI', 'KFX', 'KHI', 'KIN', 'KITE', 'KKD', 'KME', 'KNL', 'KNM', 'KONA', 'KONE', 'KOOL', 'KORS', 'KROO', 'KRU', 'KST', 'KSU', 'KTEC', 'KUTV', 'KWT', 'KYO', 'KYTH', 'KZ', 'LABC', 'LABL', 'LACO', 'LAS', 'LBF', 'LBIX', 'LBMH', 'LBY', 'LDL', 'LDR', 'LDRH', 'LEI', 'LEVY', 'LEVYU', 'LG', 'LGCY', 'LGF', 'LINE', 'LION', 'LIOX', 'LIQD', 'LLDM', 'LLEM', 'LLEX', 'LLSC', 'LLTC', 'LM', 'LMCA', 'LMCB', 'LMCK', 'LMIA', 'LMLP', 'LMNX', 'LMOS', 'LMRK', 'LNBB', 'LNCO', 'LNKD', 'LOCK', 'LOCM', 'LOGM', 'LOJN', 'LONG', 'LOOK', 'LORL', 'LPHI', 'LPT', 'LPTN', 'LRAD', 'LRE', 'LSC', 'LSG', 'LTM', 'LTS', 'LTXB', 'LUX', 'LVLT', 'LVNTA', 'LWC', 'LXFT', 'LXK', 'MAGS', 'MAMS', 'MBFI', 'MBLX', 'MBLY', 'MBRG', 'MBTF', 'MBVT', 'MCC', 'MCF', 'MCGC', 'MCOX', 'MCP', 'MCRL', 'MCUR', 'MCV', 'MCZ', 'MDAS', 'MDCA', 'MDCO', 'MDD', 'MDGN', 'MDLY', 'MDP', 'MDSO', 'MDSY', 'MDVN', 'MDVXU', 'MDW', 'MEA', 'MEET', 'MEG', 'MELA', 'MELR', 'MEN', 'MENT', 'MEP', 'MERU', 'MES', 'METR', 'MFI', 'MFLX', 'MFNC', 'MFRI', 'MFRM', 'MFSF', 'MFT', 'MGCD', 'MGH', 'MGLN', 'MGN', 'MGT', 'MHE', 'MHFI', 'MHGC', 'MHR', 'MIE', 'MIFI', 'MIK', 'MIL', 'MILL', 'MINI', 'MJN', 'MKTO', 'MLHR', 'MLNK', 'MLNX', 'MLPJ', 'MLPL', 'MM', 'MMAC', 'MNE', 'MNGA', 'MNI', 'MNK', 'MNRK', 'MNTA', 'MOBI', 'MOC', 'MOCO', 'MOG', 'MOKO', 'MOLG', 'MON', 'MONY', 'MORE', 'MPEL', 'MPET', 'MPO', 'MRD', 'MRH', 'MRKT', 'MRVC', 'MSBF', 'MSF', 'MSG', 'MSLI', 'MSO', 'MSON', 'MSP', 'MSTX', 'MTK', 'MTS', 'MTSC', 'MTSL', 'MTSN', 'MTT', 'MTU', 'MUH', 'MUS', 'MVC', 'MVG', 'MVNR', 'MW', 'MWE', 'MWIV', 'MWV', 'MXIM', 'MXWL', 'MY', 'MYCC', 'MYF', 'MYL', 'MYOS', 'MZF', 'NADL', 'NAME', 'NANO', 'NAO', 'NATL', 'NAV', 'NBBC', 'NBG', 'NBL', 'NBS', 'NBTF', 'NCB', 'NCFT', 'NCI', 'NCIT', 'NCQ', 'NDRO', 'NE', 'NEOT', 'NETE', 'NEWM', 'NEWS', 'NFEC', 'NGHC', 'NGHCP', 'NGLS', 'NHF', 'NHTB', 'NJ', 'NJV', 'NKA', 'NKY', 'NLNK', 'NMBL', 'NMO', 'NMRX', 'NMY', 'NNA', 'NNC', 'NOR', 'NORD', 'NPBC', 'NPD', 'NPP', 'NPSP', 'NRCIA', 'NRF', 'NRX', 'NSAM', 'NSH', 'NSPH', 'NSR', 'NTI', 'NTK', 'NTL', 'NTLS', 'NTN', 'NTRSP', 'NTT', 'NTX', 'NU', 'NUTR', 'NVDQ', 'NVGN', 'NVSL', 'NVX', 'NWHM', 'NWY', 'NXQ', 'NXR', 'NXTDW', 'NXTM', 'NYLD', 'NYMTP', 'NYNY', 'NYV', 'OAK', 'OAKS', 'OB', 'OCIR', 'OCLS', 'OCR', 'OCRX', 'OGXI', 'OHAI', 'OHGI', 'OHRP', 'OIBR', 'OILT', 'OKS', 'OKSB', 'OLO', 'OMAM', 'OME', 'OMED', 'OMG', 'OMN', 'ONEF', 'ONFC', 'ONNN', 'ONP', 'ONTY', 'ONVI', 'OPB', 'OPHT', 'OPWR', 'OPXA', 'ORB', 'ORBC', 'ORBK', 'OREX', 'ORIT', 'ORM', 'ORPN', 'OSGB', 'OSHC', 'OSIR', 'OSM', 'OSN', 'OTEL', 'OTIV', 'OUTR', 'OVTI', 'OWW', 'OXFD', 'OXLCO', 'OZM', 'OZRK', 'PACD', 'PAF', 'PAGG', 'PAH', 'PAL', 'PARN', 'PAY', 'PBCP', 'PBIB', 'PBM', 'PBMD', 'PBY', 'PCI', 'PCL', 'PCLN', 'PCMI', 'PCO', 'PCP', 'PCYC', 'PDII', 'PDLI', 'PE', 'PEGI', 'PEIX', 'PENX', 'PEOP', 'PER', 'PERF', 'PERM', 'PES', 'PETM', 'PETX', 'PFBI', 'PFK', 'PFNX', 'PFPT', 'PGI', 'PGM', 'PGN', 'PGNX', 'PHF', 'PHII', 'PHIIK', 'PHMD', 'PICO', 'PIH', 'PIP', 'PIR', 'PJC', 'PKD', 'PKO', 'PKY', 'PLCM', 'PLKI', 'PLMT', 'PLND', 'PLNR', 'PLPM', 'PLT', 'PLTM', 'PMBC', 'PMC', 'PMCS', 'PMFG', 'PNRA', 'PNTR', 'PNX', 'POL', 'POM', 'POPE', 'POT', 'POWR', 'POZN', 'PPHM', 'PPHMP', 'PPO', 'PPP', 'PPR', 'PPS', 'PQ', 'PRAH', 'PRAN', 'PRB', 'PRCP', 'PRE', 'PRGN', 'PRGX', 'PRLS', 'PRSC', 'PRTO', 'PRXI', 'PRXL', 'PRY', 'PSAU', 'PSBH', 'PSDV', 'PSEM', 'PSG', 'PSTB', 'PSTR', 'PSUN', 'PTBI', 'PTIE', 'PTLA', 'PTM', 'PTP', 'PTRY', 'PTX', 'PULB', 'PVA', 'PVTB', 'PWE', 'PWRD', 'PWX', 'PXMC', 'PXR', 'PXSC', 'PZI', 'Q', 'QADA', 'QCAN', 'QDEM', 'QDXU', 'QEH', 'QEM', 'QEP', 'QEPM', 'QGBR', 'QIHU', 'QKOR', 'QLGC', 'QLIK', 'QLTB', 'QLTC', 'QLTI', 'QLTY', 'QSII', 'QTM', 'QTS', 'QTWN', 'QTWW', 'QUNR', 'QVCA', 'QVCB', 'QXUS', 'RAI', 'RALY', 'RAS', 'RATE', 'RAVN', 'RAX', 'RBC', 'RBL', 'RBPAA', 'RBS', 'RBY', 'RCAP', 'RCPI', 'RCPT', 'RDC', 'RDEN', 'RDS', 'RECN', 'REE', 'REMY', 'REN', 'RENT', 'RESI', 'REXI', 'REXX', 'RFT', 'RGDO', 'RGDX', 'RGSE', 'RHP', 'RHS', 'RHT', 'RIC', 'RICE', 'RIF', 'RIGP', 'RIO', 'RIOM', 'RIT', 'RIVR', 'RJET', 'RJF', 'RKT', 'RKUS', 'RLD', 'RLH', 'RLOC', 'RLOG', 'RLYP', 'RNA', 'RNDY', 'RNET', 'RNF', 'RNN', 'ROC', 'ROIAK', 'ROIQ', 'ROIQU', 'ROIQW', 'ROKA', 'RORO', 'ROSE', 'ROVI', 'ROX', 'RP', 'RPAI', 'RPRX', 'RPRXW', 'RPRXZ', 'RPTP', 'RPX', 'RRST', 'RSE', 'RSH', 'RSO', 'RST', 'RSTI', 'RT', 'RTEC', 'RTGN', 'RTI', 'RTIX', 'RTK', 'RTR', 'RTRX', 'RUBI', 'RUK', 'RVBD', 'RVLT', 'RVM', 'RWC', 'RWV', 'RWXL', 'RXDX', 'RXII', 'RYL', 'S', 'SAAS', 'SAEX', 'SAJA', 'SALT', 'SAPE', 'SARA', 'SBBX', 'SBGL', 'SBRAP', 'SBSA', 'SBY', 'SCAI', 'SCHB', 'SCHF', 'SCHL', 'SCHP', 'SCLN', 'SCMP', 'SCOK', 'SCPB', 'SCSC', 'SCSS', 'SCTY', 'SCU', 'SCVL', 'SCZ', 'SD', 'SDLP', 'SDR', 'SDRL', 'SDT', 'SE', 'SEIC', 'SEMG', 'SEMI', 'SERV', 'SEV', 'SFB', 'SFG', 'SFL', 'SFLA', 'SFLY', 'SFN', 'SFNC', 'SFS', 'SFXE', 'SGAR', 'SGB', 'SGBK', 'SGF', 'SGG', 'SGM', 'SGNL', 'SGNT', 'SGOC', 'SGY', 'SGYP', 'SGYPU', 'SGYPW', 'SHLD', 'SHLO', 'SHM', 'SHOR', 'SHOS', 'SIAL', 'SIBC', 'SIFI', 'SIGM', 'SIMG', 'SINA', 'SINO', 'SIRO', 'SKBI', 'SKH', 'SKIS', 'SKUL', 'SLCT', 'SLH', 'SLI', 'SLTC', 'SLW', 'SLXP', 'SMACU', 'SMI', 'SMK', 'SMRT', 'SMTP', 'SMTX', 'SN', 'SNAK', 'SNBC', 'SNC', 'SNDK', 'SNH', 'SNOW', 'SNR', 'SNSS', 'SNTA', 'SORL', 'SPA', 'SPAN', 'SPAR', 'SPEX', 'SPF', 'SPKE', 'SPLS', 'SPNC', 'SPP', 'SPPR', 'SPPRO', 'SPRT', 'SPU', 'SPW', 'SQBG', 'SQBK', 'SQI', 'SQNM', 'SRSC', 'SSE', 'SSFN', 'SSH', 'SSI', 'SSLT', 'SSN', 'SSNI', 'SSRG', 'SSRI', 'SSS', 'SSW', 'STAY', 'STCK', 'STEM', 'STI', 'STJ', 'STML', 'STMP', 'STNR', 'STO', 'STR', 'STRN', 'STRZA', 'STRZB', 'STS', 'SUBK', 'SUNE', 'SUSQ', 'SUTR', 'SVLC', 'SWHC', 'SWY', 'SXCP', 'SXE', 'SYA', 'SYKE', 'SYMC', 'SYMX', 'SYNC', 'SYRG', 'SYRX', 'SYT', 'SYUT', 'SYX', 'SZYM', 'TAHO', 'TAOM', 'TAS', 'TASR', 'TAT', 'TAX', 'TAXI', 'TBAR', 'TBIO', 'TCAP', 'TCBIP', 'TCCA', 'TCHI', 'TCK', 'TCO', 'TCP', 'TCPI', 'TCRD', 'TDA', 'TDD', 'TDI', 'TE', 'TEAR', 'TECD', 'TECU', 'TEG', 'TERP', 'TESO', 'TEU', 'TFM', 'TFSCU', 'TGC', 'TGD', 'TGE', 'THOR', 'THRX', 'THTI', 'TI', 'TICC', 'TIF', 'TIK', 'TIME', 'TINY', 'TISA', 'TIVO', 'TKAI', 'TKMR', 'TLF', 'TLI', 'TLL', 'TLLP', 'TLM', 'TLMR', 'TLO', 'TLP', 'TLR', 'TMH', 'TMK', 'TNAV', 'TNDQ', 'TNGO', 'TOF', 'TOO', 'TOT', 'TOWR', 'TPI', 'TPLM', 'TPRE', 'TPUB', 'TRAK', 'TRCB', 'TRCH', 'TRCO', 'TRF', 'TRGT', 'TRIL', 'TRIV', 'TRK', 'TRLA', 'TRMR', 'TRND', 'TROV', 'TROVU', 'TROVW', 'TRR', 'TRTL', 'TRW', 'TRXC', 'TSL', 'TSLF', 'TSO', 'TSRA', 'TSRE', 'TSS', 'TST', 'TSU', 'TSYS', 'TTF', 'TTFS', 'TTHI', 'TTPH', 'TTS', 'TUBE', 'TUES', 'TVIZ', 'TWC', 'TWMC', 'TXTR', 'TYC', 'TYPE', 'UACL', 'UAM', 'UBC', 'UBIC', 'UBNK', 'UBSH', 'UCD', 'UCFC', 'UCP', 'UDF', 'UFS', 'UHN', 'UIL', 'ULTI', 'ULTR', 'UMX', 'UN', 'UNIS', 'UNT', 'UNXL', 'UPIP', 'UPL', 'URZ', 'USAG', 'USAT', 'USBI', 'USCR', 'USG', 'USMD', 'USMI', 'USTR', 'UTEK', 'UTIW', 'UWTI', 'VA', 'VAL', 'VAR', 'VCO', 'VDSI', 'VGGL', 'VIAB', 'VIAS', 'VICL', 'VIEW', 'VII', 'VIMC', 'VIP', 'VISI', 'VISN', 'VLCCF', 'VLTC', 'VMEM', 'VNTV', 'VOLC', 'VRD', 'VRML', 'VRNG', 'VRTB', 'VRTU', 'VSAR', 'VSCI', 'VSCP', 'VSI', 'VSLR', 'VSR', 'VTAE', 'VTG', 'VTL', 'VTSS', 'VTTI', 'VVUS', 'VYFC', 'WAC', 'WAGE', 'WAIR', 'WAVX', 'WBAI', 'WBC', 'WBIH', 'WBMD', 'WCG', 'WDR', 'WDTI', 'WEBK', 'WEET', 'WFBI', 'WFD', 'WFM', 'WFT', 'WG', 'WGA', 'WGBS', 'WGP', 'WHX', 'WHZ', 'WIBC', 'WIFI', 'WILN', 'WIN', 'WITE', 'WLB', 'WLH', 'WLRHU', 'WLT', 'WMAR', 'WMGI', 'WMLP', 'WNR', 'WNRL', 'WPCS', 'WPG', 'WPPGY', 'WPT', 'WPX', 'WR', 'WRES', 'WSH', 'WSTC', 'WTR', 'WTSL', 'WUBA', 'WWAV', 'WYN', 'XBKS', 'XCO', 'XEC', 'XGTI', 'XGTIW', 'XIV', 'XL', 'XLRN', 'XLS', 'XNY', 'XON', 'XONE', 'XOOM', 'XOVR', 'XRA', 'XRS', 'XUE', 'XXV', 'YAO', 'YDKN', 'YDLE', 'YGE', 'YHOO', 'YOD', 'YOKU', 'YRCW', 'YUMA', 'YUME', 'YZC', 'ZA', 'ZAGG', 'ZAYO', 'ZBB', 'ZFC', 'ZFGN', 'ZINC', 'ZIONW', 'ZIXI', 'ZLTQ', 'ZMH', 'ZN', 'ZPIN', 'ZQK', 'ZSPH', 'ZU', 'ZX']\n",
    "article_list_updated = [a for a in article_list if not a['ticker'] in failed_stocks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment pulls the stock data from the file rather than YAHOO FINANCE API. It's much much much quicker so if you have the file, run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate list of stock information\n",
    "ticker_path = './processed-data/'\n",
    "pathlist = Path(ticker_path).rglob('*.json')\n",
    "stock_data = {}\n",
    "i = 0\n",
    "for path in pathlist:\n",
    "    with open(str(path)) as json_file:\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"pulling file no %d\" % (i))\n",
    "        sys.stdout.flush()#\n",
    "        i += 1\n",
    "        data = json.load(json_file)\n",
    "        data = json.loads(data)\n",
    "        datetime_dict = {}\n",
    "        for line in data:\n",
    "            datetime_line = {}\n",
    "            if line == 'Close':\n",
    "                for k in data[line]:\n",
    "                    # print(str(datetime.fromtimestamp(float(k)/1000.0)))\n",
    "                    datetime_line[datetime.fromtimestamp(float(k)/1000.0).date()] = data[line][k]\n",
    "                datetime_dict[line] = datetime_line \n",
    "        stock_data[os.path.basename(str(path))[:-5]] = DataFrame.from_dict(datetime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the stock data all pulled, we now need to assign these to the appropriate article. It's important to note that there is not stock market information available for every single day, so if there is a day missing, we just take the next available day with information and call this day $t$. Doing this for each article is incredibly time consuming and takes around 30 hours, so instead, we create a lookup table here. Inside, it has the adjusted stock data required using the stock ticker and the date the article was released as the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'day_t_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joshf\\Documents\\thesis\\code\\kaggle\\kaggle.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(day_t_data\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'day_t_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(day_t_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100% 4151 out of 4152"
     ]
    }
   ],
   "source": [
    "day_t_data = {}\n",
    "curr_index = 0\n",
    "TOTAL_ARTS = len(stock_data)\n",
    "for t in stock_data:\n",
    "    # print(t)\n",
    "    ticker_date_data = {}\n",
    "    start = min(stock_data[t].index)\n",
    "    end = max(stock_data[t].index)\n",
    "    curr_date = start\n",
    "    while curr_date < end:\n",
    "        day_t = curr_date\n",
    "        while not day_t in stock_data[t].index and day_t <= end:\n",
    "            day_t += dt.timedelta(days=1)\n",
    "        from_stock = day_t - dt.timedelta(days=2)\n",
    "        to_stock = day_t + dt.timedelta(days=1)\n",
    "        while not from_stock in stock_data[t].index and from_stock >= start:\n",
    "            from_stock -= dt.timedelta(days=1)\n",
    "        while not to_stock in stock_data[t].index and to_stock <= end:\n",
    "            to_stock += dt.timedelta(days=1)\n",
    "        if(from_stock > start and to_stock < end):\n",
    "            ticker_date_data[curr_date] = {'day_t': day_t, 'from_stock': stock_data[t]['Close'][from_stock], 'to_stock': stock_data[t]['Close'][to_stock]}\n",
    "        curr_date += dt.timedelta(days=1)\n",
    "    day_t_data[t] = ticker_date_data\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_ARTS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d\" % ('='*int(20*j), 100*j, curr_index, TOTAL_ARTS))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the tools at our disposal, we now need to compile the updated list of articles. This segment puts them in a file for later use, so we don't even need to do any of the things previously ever again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list_mrkt = []\n",
    "print(len(article_list))\n",
    "print(len(article_list_updated))\n",
    "curr_index = 0\n",
    "TOTAL_ARTS = len(article_list_updated)\n",
    "print('Done, assigning stock data to articles...')\n",
    "with open('archive/analyst_ratings_mrkt.csv', 'w',newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['headline','date','ticker','open','close'])\n",
    "    for a in article_list_updated:\n",
    "        article_date = datetime.strptime(a['date'], '%Y-%m-%d %H:%M:%S%z')\n",
    "        print(day_t)\n",
    "        if (article_date.hour > 15):\n",
    "            day_t = day_t + dt.timedelta(days=1)\n",
    "        if a['ticker'] in list(day_t_data.keys()):\n",
    "            if (day_t in day_t_data[a['ticker']]):\n",
    "                day_t = day_t_data[a['ticker']][day_t]['day_t']\n",
    "                from_stock = day_t_data[a['ticker']][day_t]['from_stock']\n",
    "                to_stock = day_t_data[a['ticker']][day_t]['to_stock']\n",
    "                new_art = {\n",
    "                    'headline': a['headline'],\n",
    "                    'mrkt_info': {\n",
    "                        'open': from_stock,\n",
    "                        'close': to_stock\n",
    "                    },\n",
    "                    'date': a['date'],\n",
    "                    'ticker': a['ticker']\n",
    "                }\n",
    "                csvwriter.writerow([a['headline'], a['date'], a['ticker'], str(from_stock), str(to_stock)])\n",
    "                # article_list_mrkt.append(new_art)\n",
    "    # else:\n",
    "    #     print(\"ticker \" + t + \" date \" + str(day_t) + \" not in range\")\n",
    "    # sys.stdout.write('\\r')\n",
    "    # j = (curr_index + 1) / TOTAL_ARTS\n",
    "    # sys.stdout.write(\"%d out of %d articles processed\" % (curr_index, TOTAL_ARTS))\n",
    "    # sys.stdout.flush()\n",
    "    # curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# START HERE IF `archive/analyst_ratings_mrkt.csv` EXISTS\n",
    "Now that we have everything at our disposal, we can actually begin testing some things. Let's first read the file into the notebook so we can begin doing some computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1027533 lines generating 1027533 usable headlines\n"
     ]
    }
   ],
   "source": [
    "# import market data articles\n",
    "#loop through list of files\n",
    "article_list = []\n",
    "file_name = './archive/analyst_ratings_mrkt.csv'\n",
    "with open(file_name, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            new_art = {\n",
    "                'headline': row[0],\n",
    "                'date': datetime.strptime(row[1], '%Y-%m-%d %H:%M:%S%z'),\n",
    "                'mrkt_info': {\n",
    "                    'open': row[3],\n",
    "                    'close': row[4]\n",
    "                },\n",
    "                'ticker': row[2]\n",
    "            }\n",
    "            article_list.append(new_art)\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count-1} lines generating {len(article_list)} usable headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The paper 'predicting stock returns with text data' used a rolling window method to train and validate window. They train models of the matrix and minimise a loss function. The tuning parameters are $(\\alpha_+, \\alpha_-, \\kappa, \\lambda)$. In each case, a limited number of choices were used.\n",
    "- For the two $\\alpha$ params, they are set such that the number of words in each group (both positive and negative) is either 25, 50, or 100.\n",
    "- For $\\kappa$, five choices are considered, at: 86%, 88%, 90%, 92% and 94% quantiles of the count distribution each year\n",
    "- For $\\lambda$, three choices: 1, 5, and 10\n",
    "- This totals 45 configurations\n",
    "\n",
    "The loss function they used is the $\\ell^1$-norm of the differences between estimated article sentiment scores and the corresponding standardised return ranks for all events in the validation sample.\n",
    "\n",
    "They had data all the way from 1989, but we don't have that luxury, with articles spanning 2009-2020. For this reason, we will use slightly different window sizes. We will attempt to estimate and validate the model around 20 times, meaning the whole window will be a three years total total. The training sample will be two years and the validation the following year. We then move the window along by four months, giving us 19 models total. Importantly, the articles from 2019-05-01 are not used in either window, so are out of sample articles that could be used for testing models later on.\n",
    "\n",
    "**IMPORTANT NOTES**\n",
    "- It is entirely possible that the news articles in a certain month will carry certain sentiment, so investigation may be necessary into random 8 months of the year.\n",
    "- 2009 and 2010 are comparatively smaller datasets, so I will combine these two years to create a single 'year' `(!!to implement. I am currently ignoring 2009)`\n",
    "- 2020 has a similar number of headlines as the other years, but only has headlines up to June. The training and validation samples in this year will just be half that of other years..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi thread version -- Unigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting articles by date...\n",
      "New training window: 2010-01-01 00:00:00+00:00\n",
      "New validation window: 2012-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "15 of 15 trials complete (last trial train: 12s, validation: 722s) [a=100,k=94,kval=218]Saving best config...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/word-lists/2010-1-1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joshf\\Documents\\thesis\\code\\kaggle\\kaggle.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=160'>161</a>\u001b[0m     csvwriter \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(csv_file)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=161'>162</a>\u001b[0m     csvwriter\u001b[39m.\u001b[39mwriterow([\u001b[39mstr\u001b[39m(curr_year) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(curr_month) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39malpha_plus\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39malpha_minus\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39mkappa\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39mlam\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39mnorm_err\u001b[39m\u001b[39m'\u001b[39m])])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=162'>163</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mdata/word-lists/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(curr_year) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(curr_month) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m-1\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m, newline\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m csv_file:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=163'>164</a>\u001b[0m     \u001b[39m#write header\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=164'>165</a>\u001b[0m     csvwriter \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(csv_file)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=165'>166</a>\u001b[0m     csvwriter\u001b[39m.\u001b[39mwriterow([\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mO+ value\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mO- value\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/word-lists/2010-1-1.csv'"
     ]
    }
   ],
   "source": [
    "def validate_window(index, val_d, val_p, sentiment_words, LAM, trials):\n",
    "    error_arr = np.array(0)\n",
    "    # print(str(index) + \" computing lambda of \" + str(LAM))\n",
    "    for val_index in range(len(val_d)):\n",
    "        est_p = 0.5\n",
    "        val_bow = val_d[val_index]\n",
    "\n",
    "        testing_s = sum(val_bow.get(w,0) for w in sentiment_words)\n",
    "        if (testing_s > 0):\n",
    "            est_p = fminbound(equation_to_solve, 0, 1, (O,val_bow, sentiment_words,testing_s,LAM))\n",
    "        error_arr = np.append(error_arr, est_p - val_p[val_index])\n",
    "    normalised_error = np.linalg.norm(error_arr, 1)\n",
    "    lam_trial = {\n",
    "        'alpha': alpha,\n",
    "        'alpha_plus': ALPHA_PLUS,\n",
    "        'alpha_minus': ALPHA_MINUS,\n",
    "        'kappa': KAPPA,\n",
    "        'lam': LAM,\n",
    "        'o': O,\n",
    "        'sentiment_words': sentiment_words,\n",
    "        'norm_err': normalised_error\n",
    "    }\n",
    "    trials.append(lam_trial)\n",
    "\n",
    "# reset global variables\n",
    "sgn = []\n",
    "y = []\n",
    "dates = []\n",
    "global_bow = {}\n",
    "d = []\n",
    "kappa_configs   = [86, 88, 90, 92, 94]\n",
    "alpha_configs   = [25,50,100]\n",
    "lambda_configs  = [1,5,10]\n",
    "model_id = 'stem-timezone'\n",
    "# kappa_configs   = [86]\n",
    "# alpha_configs   = [25]\n",
    "# lambda_configs  = [1]\n",
    "curr_year = 2010\n",
    "curr_month = 1\n",
    "opening_date = datetime(curr_year,curr_month,1,0,0,0,0)\n",
    "list_dates = [a['date'] for a in article_list]\n",
    "# end_date = datetime.strptime(max(list_dates), '%Y-%m-%d %H:%M:%S%z')\n",
    "end_date = max(list_dates)\n",
    "\n",
    "# reset saved config files\n",
    "destination_directory = './data/models/' + model_id\n",
    "if not os.path.exists(destination_directory):\n",
    "    os.mkdir(destination_directory)\n",
    "    os.mkdir(os.path.join(destination_directory,'word-lists'))\n",
    "with open(destination_directory + '/configurations.csv', 'w', newline='') as csv_file:\n",
    "    csvwriter = csv.writer(csv_file)\n",
    "    csvwriter.writerow([('date'), ('no. sentiment words'), ('alpha plus'), ('alpha_minus'), ('kappa'), ('lambda'), ('normalised error')])\n",
    "\n",
    "\n",
    "while(datetime(curr_year+3, curr_month, 1, 0,0,0,0).date() < end_date.date()):\n",
    "    #SELECT ARTICLES\n",
    "    val_date_start = datetime(curr_year+2,curr_month,1,0,0,0,0)\n",
    "    val_date_end = datetime(curr_year+3,curr_month,1,0,0,0,0)\n",
    "    curr_date = datetime(curr_year, curr_month, 1,0,0,0,0,est)\n",
    "    val_date = datetime(curr_year + 2, curr_month, 1,0,0,0,0,est)\n",
    "    val_end = datetime(curr_year + 3, curr_month, 1,0,0,0,0,est)\n",
    "    print('Selecting articles by date...')\n",
    "    print(\"New training window: \" + str(curr_date))\n",
    "    print(\"New validation window: \" + str(val_date))\n",
    "    training_arts   = [a for a in article_list if (a['date'] >= curr_date and a['date'] < val_date)]\n",
    "    validation_arts = [a for a in article_list if (a['date'] >= val_date and a['date'] < val_end)]\n",
    "    # print(training_arts)\n",
    "\n",
    "    #PRE-PROCESS\n",
    "    print('Preprocessing data...')\n",
    "    train_d = []\n",
    "    train_sgn = []\n",
    "    train_y = []\n",
    "    val_d = []\n",
    "    val_sgn =[]\n",
    "    val_y = []\n",
    "    for train_a in training_arts:\n",
    "        train_bow = text_to_bow(train_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(train_a)\n",
    "        train_d.append(train_bow)\n",
    "        train_sgn.append(sgn_a)\n",
    "        train_y.append(returns)\n",
    "    for val_a in validation_arts:\n",
    "        val_bow = text_to_bow(val_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(val_a)\n",
    "        val_d.append(val_bow)\n",
    "        val_y.append(returns)\n",
    "    \n",
    "    # fraction of positively tagged training articles\n",
    "    train_pi = sum(sgn_i > 0 for sgn_i in train_sgn)/len(train_sgn)\n",
    "\n",
    "    # start training\n",
    "    print('Beginning trials')\n",
    "    trials = []\n",
    "    curr_trial = 0\n",
    "    # pre calculations (things not affected by the changes we make)\n",
    "    (pos_j, total_j, f) = calc_f(train_d, train_sgn)\n",
    "    p                   = calc_p(train_y)\n",
    "    val_p               = calc_p(val_y)\n",
    "    #PARAM GRID\n",
    "    for alpha in alpha_configs:\n",
    "        for KAPPA in kappa_configs:\n",
    "            #TRAINING\n",
    "            train_time_0 = time.time()\n",
    "            kappa_percentile = np.percentile(np.array(list(total_j.values())),KAPPA) # return the nth percentile of all appearances for KAPPA\n",
    "\n",
    "            #calculate alpha vals\n",
    "            ALPHA_PLUS  = train_pi/2\n",
    "            ALPHA_MINUS = train_pi/2\n",
    "            delta_plus  = train_pi/4\n",
    "            delta_minus  = train_pi/4\n",
    "            delta_limit = 0.001\n",
    "            while(delta_plus > delta_limit):\n",
    "                no_pos_words = len([w for w in total_j if f[w] >= train_pi + ALPHA_PLUS and total_j[w] >= kappa_percentile])\n",
    "                if no_pos_words == alpha:\n",
    "                    delta_plus = 0\n",
    "                elif (no_pos_words > alpha):\n",
    "                    ALPHA_PLUS += delta_plus\n",
    "                    delta_plus /= 2\n",
    "                else:\n",
    "                    ALPHA_PLUS -= delta_plus\n",
    "                    delta_plus /= 2\n",
    "            while(delta_minus > delta_limit):\n",
    "                no_neg_words = len([w for w in total_j if f[w] <= train_pi - ALPHA_MINUS and total_j[w] >= kappa_percentile])\n",
    "                if no_neg_words == alpha:\n",
    "                    delta_minus = 0\n",
    "                elif (no_neg_words > alpha):\n",
    "                    ALPHA_MINUS += delta_minus\n",
    "                    delta_minus /= 2\n",
    "                else:\n",
    "                    ALPHA_MINUS -= delta_minus\n",
    "                    delta_minus /= 2\n",
    "            sentiment_words = [w for w in total_j if ((f[w] >= train_pi + ALPHA_PLUS or f[w] <= train_pi - ALPHA_MINUS) and total_j[w] >= kappa_percentile)]\n",
    "\n",
    "            (s, d_s)    = calc_s(sentiment_words, train_d)\n",
    "            h           = calc_h(sentiment_words, train_d, s, d_s)\n",
    "            O           = calc_o(p,h)\n",
    "            train_time_1 = time.time()\n",
    "            # for LAM in lambda_configs:\n",
    "\n",
    "            #VALIDATING\n",
    "            # start multithreading here\n",
    "            t0 = time.time()\n",
    "            threads = []\n",
    "            for index in range(3):\n",
    "                logging.info(\"Main    : create and start thread %d.\", index)\n",
    "                x = threading.Thread(target=validate_window, args=(index,val_d, val_p, sentiment_words,lambda_configs[index],trials))\n",
    "                threads.append(x)\n",
    "                x.start()\n",
    "\n",
    "            for index, thread in enumerate(threads):\n",
    "                logging.info(\"Main    : before joining thread %d.\", index)\n",
    "                thread.join()\n",
    "                logging.info(\"Main    : thread %d done\", index)\n",
    "            curr_trial += 1\n",
    "            t1 = time.time()\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"%d of %d trials complete (last trial train: %ds, validation: %ds) [a=%d,k=%d,kval=%d]\" % (curr_trial, 15, train_time_1-train_time_0, t1-t0, alpha, KAPPA, kappa_percentile))\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    #RECORD BEST CONFIG\n",
    "    print(\"Saving best config...\")\n",
    "    best_config = min(trials, key=lambda x:x['norm_err'])\n",
    "    with open(destination_directory + '/configurations.csv', 'a', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str(curr_year) + '-' + str(curr_month) + '-1', str(best_config['alpha']), str(best_config['alpha_plus']), str(best_config['alpha_minus']), str(best_config['kappa']), str(best_config['lam']), str(best_config['norm_err'])])\n",
    "    with open(destination_directory + '/word-lists/' + str(curr_year) + '-' + str(curr_month) + '-1' + '.csv', 'w', newline='') as csv_file:\n",
    "        #write header\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow(['word', 'O+ value', 'O- value'])\n",
    "    with open(destination_directory + '/word-lists/' + str(curr_year) + '-' + str(curr_month) + '-1' + '.csv', 'a', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        for ind in range(len(best_config['sentiment_words'])):\n",
    "            csvwriter.writerow([str(best_config['sentiment_words'][ind]), str(best_config['o'][ind][0]), str(best_config['o'][ind][1])])\n",
    "\n",
    "    #record start date, sentiment words, O, and params probably\n",
    "    #i think maybe start date and params in a different file to sentiment words and O.\n",
    "\n",
    "    #MOVE WINDOW\n",
    "    curr_month += 4\n",
    "    if (curr_month > 12):\n",
    "        curr_month = 1\n",
    "        curr_year += 1\n",
    "    # opening_date += relativedelta(months=4)\n",
    "\n",
    "# #2020 SPECIAL CASE\n",
    "\n",
    "\n",
    "# TOTAL_ARTS = len(article_list)\n",
    "# curr_index = 0\n",
    "# curr_thousand = 0\n",
    "# for a in article_list:\n",
    "#     raw_html = a['headline']\n",
    "#     if(raw_html):\n",
    "#         bow_art = text_to_bow(raw_html)\n",
    "#         (returns, sgn_a) = calc_returns(a)\n",
    "#         dates.append(a['date'])\n",
    "#         d.append(bow_art)\n",
    "#         sgn.append(sgn_a)\n",
    "#         y.append(returns)\n",
    "#     if curr_index >= curr_thousand:\n",
    "#         curr_thousand += 10000\n",
    "#         sys.stdout.write('\\r')\n",
    "#         j = (curr_index + 1) / TOTAL_ARTS\n",
    "#         sys.stdout.write(\"%d out of %d articles processed\" % (curr_index, TOTAL_ARTS))\n",
    "#         sys.stdout.flush()\n",
    "#     curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Thread -- Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting articles by date...\n",
      "New training window: 2010-01-01 00:00:00+00:00\n",
      "New validation window: 2012-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 26\n",
      "no neg 25\n",
      "Alpha + // -  0.15541949824286752 0.14100834953998054\n",
      "51\n",
      "1 of 15 trials complete (last trial train: 3s, validation: 19s) [a=25,k=86,kval=49]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.14619248003777394 0.12441913194704167\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 21s) [a=25,k=88,kval=60]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12649278414615903 0.10782991435410279\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 1s, validation: 21s) [a=25,k=90,kval=77]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1223454797479243 0.08916704456204651\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 1s, validation: 21s) [a=25,k=92,kval=95]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1016089577567507 0.062209565973520835\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 27s) [a=25,k=94,kval=120]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11197721875233752 0.09124069676116388\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 3s, validation: 51s) [a=50,k=86,kval=49]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10057213165719202 0.07413306611844565\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 3s, validation: 55s) [a=50,k=88,kval=60]no pos 50\n",
      "no neg 49\n",
      "Alpha + // -  0.07257782696910763 0.06026665612802214\n",
      "101\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 56s) [a=50,k=90,kval=77]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.049022434019758865 0.0476940005796993\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 3s, validation: 84s) [a=50,k=92,kval=95]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.023198983977625477 0.029031130787643057\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 3s, validation: 95s) [a=50,k=94,kval=120]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.043546696181464575 0.051322891928154694\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 7s, validation: 189s) [a=100,k=86,kval=49]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.03175279929898459 0.03447446781032613\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 6s, validation: 202s) [a=100,k=88,kval=60]no pos 99\n",
      "no neg 100\n",
      "Alpha + // -  0.016764004805279044 0.021773348090732288\n",
      "201\n",
      "13 of 15 trials complete (last trial train: 6s, validation: 214s) [a=100,k=90,kval=77]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.010368260995586805 0.00343448645478813\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 7s, validation: 234s) [a=100,k=92,kval=95]no pos 79\n",
      "no neg 81\n",
      "Alpha + // -  1.2656568598128425e-07 1.2656568598128425e-07\n",
      "160\n",
      "15 of 15 trials complete (last trial train: 5s, validation: 194s) [a=100,k=94,kval=120]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2010-05-01 00:00:00+00:00\n",
      "New validation window: 2012-05-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.15159800442985455 0.16405811438299328\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 12s) [a=25,k=86,kval=49]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1401762369728107 0.14536794945328516\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 18s) [a=25,k=88,kval=60]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13790486276260314 0.12875446951576688\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 18s) [a=25,k=90,kval=80]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12823529826771943 0.11123243989416555\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 20s) [a=25,k=92,kval=101]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12330317141126869 0.06437723475788344\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 21s) [a=25,k=94,kval=136]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12304358578724496 0.1225244145391975\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 5s, validation: 38s) [a=50,k=86,kval=49]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11227078239026043 0.08981662591220833\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 5s, validation: 46s) [a=50,k=88,kval=60]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09241248215244557 0.07476065971883238\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 3s, validation: 49s) [a=50,k=90,kval=80]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.0706072897344528 0.05970469352545642\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 4s, validation: 53s) [a=50,k=92,kval=101]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.04568706982817534 0.03011193238675193\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 4s, validation: 82s) [a=50,k=94,kval=136]no pos 101\n",
      "no neg 100\n",
      "Alpha + // -  0.05408291560439967 0.0706072897344528\n",
      "201\n",
      "11 of 15 trials complete (last trial train: 7s, validation: 117s) [a=100,k=86,kval=49]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.04464872733208044 0.05035961106060237\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 7s, validation: 128s) [a=100,k=88,kval=60]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.03491426643119081 0.027191594116485047\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 9s, validation: 201s) [a=100,k=90,kval=80]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.0176518224336132 0.012979281201186178\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 7s, validation: 213s) [a=100,k=92,kval=101]no pos 93\n",
      "no neg 89\n",
      "Alpha + // -  1.2675079298033376e-07 1.2675079298033376e-07\n",
      "182\n",
      "15 of 15 trials complete (last trial train: 7s, validation: 281s) [a=100,k=94,kval=136]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2010-09-01 00:00:00+00:00\n",
      "New validation window: 2012-09-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1859477532426546 0.16322080562410796\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 8s, validation: 16s) [a=25,k=86,kval=52]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.15237385335161976 0.14049385800556127\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 19s) [a=25,k=88,kval=64]no pos 25\n",
      "no neg 26\n",
      "Alpha + // -  0.1415269010791316 0.11663739783696954\n",
      "49\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 26s) [a=25,k=90,kval=83]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1358451641744949 0.10130278640198787\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 4s, validation: 29s) [a=25,k=92,kval=105]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13016342726985825 0.06611475670849942\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 3s, validation: 34s) [a=25,k=94,kval=143]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.1358451641744949 0.11647560654505172\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 9s, validation: 53s) [a=50,k=86,kval=52]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11570082423987399 0.10330430735703035\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 9s, validation: 60s) [a=50,k=88,kval=64]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10743647965131156 0.07244214553411753\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 6s, validation: 64s) [a=50,k=90,kval=83]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.08470953203276488 0.0599164982670776\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 6s, validation: 66s) [a=50,k=92,kval=105]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.0599164982670776 0.036156507574960625\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 6s, validation: 70s) [a=50,k=94,kval=143]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07851127359134306 0.06611475670849942\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 12s, validation: 156s) [a=100,k=86,kval=52]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.0599164982670776 0.04958606753137457\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 11s, validation: 172s) [a=100,k=88,kval=64]no pos 99\n",
      "no neg 100\n",
      "Alpha + // -  0.04125728385460482 0.03357389989103486\n",
      "201\n",
      "13 of 15 trials complete (last trial train: 12s, validation: 211s) [a=100,k=90,kval=83]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.02066086147140607 0.021693904544976373\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 14s, validation: 297s) [a=100,k=92,kval=105]no pos 93\n",
      "no neg 100\n",
      "Alpha + // -  1.2610389081668744e-07 0.0033573899891034863\n",
      "193\n",
      "15 of 15 trials complete (last trial train: 12s, validation: 294s) [a=100,k=94,kval=143]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2011-01-01 00:00:00+00:00\n",
      "New validation window: 2013-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.16209635991084076 0.14943258179280633\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 4s, validation: 20s) [a=25,k=86,kval=53]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.15082559738579016 0.11903951430952367\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 5s, validation: 22s) [a=25,k=88,kval=69]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1405679371101822 0.11144124743870303\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 24s) [a=25,k=90,kval=87]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1296770879286726 0.09725781594650446\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 4s, validation: 25s) [a=25,k=92,kval=109]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11751986093535954 0.056733725968794266\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 4s, validation: 29s) [a=25,k=94,kval=146]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12765088342978712 0.10637573619148925\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 5s, validation: 42s) [a=50,k=86,kval=53]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11751986093535954 0.09117920244984794\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 9s, validation: 44s) [a=50,k=88,kval=69]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10384298056788235 0.07547611758348524\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 6s, validation: 51s) [a=50,k=90,kval=87]no pos 51\n",
      "no neg 50\n",
      "Alpha + // -  0.08403146432594132 0.05521407259463013\n",
      "99\n",
      "9 of 15 trials complete (last trial train: 5s, validation: 59s) [a=50,k=92,kval=109]no pos 50\n",
      "no neg 52\n",
      "Alpha + // -  0.060786134966565285 0.030756532755635217\n",
      "102\n",
      "10 of 15 trials complete (last trial train: 8s, validation: 72s) [a=50,k=94,kval=146]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.0785154243318135 0.0628123394654508\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 11s, validation: 143s) [a=100,k=86,kval=53]no pos 100\n",
      "no neg 101\n",
      "Alpha + // -  0.06585164621377905 0.047010195162832766\n",
      "201\n",
      "12 of 15 trials complete (last trial train: 12s, validation: 161s) [a=100,k=88,kval=69]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.04558960122492397 0.033432374231610905\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 12s, validation: 208s) [a=100,k=90,kval=87]no pos 101\n",
      "no neg 96\n",
      "Alpha + // -  0.022958415636389326 0.018708382444940386\n",
      "193\n",
      "14 of 15 trials complete (last trial train: 11s, validation: 290s) [a=100,k=92,kval=109]no pos 100\n",
      "no neg 95\n",
      "Alpha + // -  0.004558960122492396 1.2366970818393002e-07\n",
      "195\n",
      "15 of 15 trials complete (last trial train: 13s, validation: 293s) [a=100,k=94,kval=146]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2011-05-01 00:00:00+00:00\n",
      "New validation window: 2013-05-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1700963753182204 0.14782184997892964\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 3s, validation: 14s) [a=25,k=86,kval=53]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.16199654792211465 0.13820330494605404\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 15s) [a=25,k=88,kval=66]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.14598673283449942 0.11542254039450671\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 16s) [a=25,k=90,kval=86]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13769706573379745 0.08909810135716306\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 18s) [a=25,k=92,kval=108]no pos 25\n",
      "no neg 24\n",
      "Alpha + // -  0.12757228148866528 0.0605575062370623\n",
      "51\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 24s) [a=25,k=94,kval=140]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12959723833769174 0.10555087575550284\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 6s, validation: 37s) [a=50,k=86,kval=53]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12554732463963886 0.0971979287532688\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 5s, validation: 42s) [a=50,k=88,kval=66]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11339758354548027 0.06758293483625721\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 5s, validation: 51s) [a=50,k=90,kval=86]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09175585722151026 0.05619255256048352\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 6s, validation: 53s) [a=50,k=92,kval=108]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.07087348971592515 0.038980419343758836\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 5s, validation: 60s) [a=50,k=94,kval=140]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.08302323081008377 0.06024246625853639\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 10s, validation: 146s) [a=100,k=86,kval=53]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07796083868751767 0.047333366345992875\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 17s, validation: 136s) [a=100,k=88,kval=66]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.056698791772740134 0.037461701706989016\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 9s, validation: 179s) [a=100,k=90,kval=86]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.026324439037343633 0.0242994821883172\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 11s, validation: 203s) [a=100,k=92,kval=108]no pos 100\n",
      "no neg 95\n",
      "Alpha + // -  0.007087348971592517 1.2359355767983602e-07\n",
      "195\n",
      "15 of 15 trials complete (last trial train: 9s, validation: 283s) [a=100,k=94,kval=140]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2011-09-01 00:00:00+00:00\n",
      "New validation window: 2013-09-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1594895887825601 0.15897176544235697\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 18s) [a=25,k=86,kval=52]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13981230185484161 0.14395488857646654\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 21s) [a=25,k=88,kval=64]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12842018837037306 0.13670536181362292\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 24s) [a=25,k=90,kval=80]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12091174993742784 0.09735078795818602\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 24s) [a=25,k=92,kval=101]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11184984148387331 0.066281387545999\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 3s, validation: 25s) [a=25,k=94,kval=132]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12013501492712317 0.11366222317458421\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 4s, validation: 45s) [a=50,k=86,kval=52]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11288548816427954 0.09217255455615486\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 4s, validation: 47s) [a=50,k=88,kval=64]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10731888725709601 0.07974479439128004\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 49s) [a=50,k=90,kval=80]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.0849230277933112 0.05385362738112418\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 3s, validation: 54s) [a=50,k=92,kval=101]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.05799621410274912 0.04142586721624937\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 4s, validation: 67s) [a=50,k=94,kval=132]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07870914771087381 0.06421009418518653\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 8s, validation: 123s) [a=100,k=86,kval=52]no pos 99\n",
      "no neg 100\n",
      "Alpha + // -  0.06974900873702225 0.04608627727807743\n",
      "199\n",
      "12 of 15 trials complete (last trial train: 7s, validation: 138s) [a=100,k=88,kval=64]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.047121923958483664 0.03572981047401508\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 10s, validation: 204s) [a=100,k=90,kval=80]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.0331406937729995 0.026150078680257413\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 8s, validation: 239s) [a=100,k=92,kval=101]no pos 97\n",
      "no neg 100\n",
      "Alpha + // -  1.2642171391677664e-07 0.002330205030914027\n",
      "197\n",
      "15 of 15 trials complete (last trial train: 8s, validation: 305s) [a=100,k=94,kval=132]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2012-01-01 00:00:00+00:00\n",
      "New validation window: 2014-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.18386150242021426 0.163432446595746\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 4s, validation: 20s) [a=25,k=86,kval=50]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.16552773437261453 0.1466701443807977\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 22s) [a=25,k=88,kval=62]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.14247956882706062 0.11733611550463814\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 24s) [a=25,k=90,kval=78]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11995522522572383 0.09376412801486708\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 26s) [a=25,k=92,kval=93]no pos 25\n",
      "no neg 26\n",
      "Alpha + // -  0.10843114245294685 0.07485525565009346\n",
      "49\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 26s) [a=25,k=94,kval=128]no pos 50\n",
      "no neg 52\n",
      "Alpha + // -  0.12047904716994096 0.10782227617452456\n",
      "99\n",
      "6 of 15 trials complete (last trial train: 4s, validation: 48s) [a=50,k=86,kval=50]no pos 49\n",
      "no neg 50\n",
      "Alpha + // -  0.11066518677509751 0.09324030607064995\n",
      "99\n",
      "7 of 15 trials complete (last trial train: 3s, validation: 50s) [a=50,k=88,kval=62]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10057381328968984 0.07647800385570164\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 3s, validation: 53s) [a=50,k=90,kval=78]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.081716223297873 0.05395366025436486\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 4s, validation: 61s) [a=50,k=92,kval=93]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.05028690664484492 0.035619892206765154\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 4s, validation: 77s) [a=50,k=94,kval=128]no pos 100\n",
      "no neg 101\n",
      "Alpha + // -  0.08106144586760157 0.06972701835441297\n",
      "199\n",
      "11 of 15 trials complete (last trial train: 7s, validation: 129s) [a=100,k=86,kval=50]no pos 99\n",
      "no neg 100\n",
      "Alpha + // -  0.06737979473035019 0.04609633109110785\n",
      "201\n",
      "12 of 15 trials complete (last trial train: 8s, validation: 164s) [a=100,k=88,kval=62]no pos 101\n",
      "no neg 100\n",
      "Alpha + // -  0.046939740691232835 0.03208409408329949\n",
      "199\n",
      "13 of 15 trials complete (last trial train: 8s, validation: 211s) [a=100,k=90,kval=78]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.03457224831833088 0.016762302214948305\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 7s, validation: 248s) [a=100,k=92,kval=93]no pos 100\n",
      "no neg 99\n",
      "Alpha + // -  0.00013095548605428364 1.2788621684988636e-07\n",
      "199\n",
      "15 of 15 trials complete (last trial train: 9s, validation: 285s) [a=100,k=94,kval=128]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2012-05-01 00:00:00+00:00\n",
      "New validation window: 2014-05-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1664118425147837 0.16225154645191409\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 25s) [a=25,k=86,kval=49]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13832984409041396 0.1580912503890445\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 27s) [a=25,k=88,kval=58]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11752836377606599 0.13312947401182695\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 29s) [a=25,k=90,kval=74]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1112879196817616 0.08736621732026144\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 31s) [a=25,k=92,kval=90]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.08944636535169623 0.08275088887551547\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 32s) [a=25,k=94,kval=127]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11388810472105508 0.1060875496031746\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 4s, validation: 59s) [a=50,k=86,kval=49]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09984710550887022 0.09568680944600062\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 3s, validation: 59s) [a=50,k=88,kval=58]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.08112577322595704 0.0813857917298864\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 5s, validation: 65s) [a=50,k=90,kval=74]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.07072503306878307 0.05876418188803299\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 3s, validation: 71s) [a=50,k=92,kval=90]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.04992355275443511 0.03744266456582633\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 3s, validation: 97s) [a=50,k=94,kval=127]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07072503306878307 0.06513463523430205\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 9s, validation: 167s) [a=100,k=86,kval=49]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.05720407086445689 0.05785411712428027\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 8s, validation: 193s) [a=100,k=88,kval=58]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.03744266456582633 0.03822272007761438\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 8s, validation: 235s) [a=100,k=90,kval=74]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.024701757873288203 0.01040074015717398\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 9s, validation: 260s) [a=100,k=92,kval=90]no pos 100\n",
      "no neg 91\n",
      "Alpha + // -  0.0036402590550108933 1.269621601217527e-07\n",
      "191\n",
      "15 of 15 trials complete (last trial train: 8s, validation: 255s) [a=100,k=94,kval=127]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2012-09-01 00:00:00+00:00\n",
      "New validation window: 2014-09-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.14718772919101014 0.17136857041524756\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 29s) [a=25,k=86,kval=47]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13667431996308085 0.1598038202645253\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 29s) [a=25,k=88,kval=55]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12616091073515154 0.13036627442632326\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 30s) [a=25,k=90,kval=69]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11775018335280811 0.0988260467425354\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 32s) [a=25,k=92,kval=90]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.09462068305136367 0.06728581905874749\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 34s) [a=25,k=94,kval=124]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11840727142955368 0.123269723197471\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 3s, validation: 58s) [a=50,k=86,kval=47]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10303141043370709 0.10027164051137567\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 3s, validation: 62s) [a=50,k=88,kval=55]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09041531936019195 0.08147892151645206\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 5s, validation: 71s) [a=50,k=90,kval=69]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.057823750753611125 0.06097777352198991\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 4s, validation: 88s) [a=50,k=92,kval=90]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.043104977834510116 0.033642909529373745\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 5s, validation: 110s) [a=50,k=94,kval=124]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07359386459550507 0.06518313721316163\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 8s, validation: 166s) [a=100,k=86,kval=47]no pos 101\n",
      "no neg 100\n",
      "Alpha + // -  0.051457054972503406 0.048887352909871236\n",
      "199\n",
      "12 of 15 trials complete (last trial train: 7s, validation: 205s) [a=100,k=88,kval=55]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.033642909529373745 0.03180306291448612\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 7s, validation: 261s) [a=100,k=90,kval=69]no pos 101\n",
      "no neg 100\n",
      "Alpha + // -  0.020853177798077976 0.013141761534911619\n",
      "201\n",
      "14 of 15 trials complete (last trial train: 9s, validation: 285s) [a=100,k=92,kval=90]no pos 100\n",
      "no neg 90\n",
      "Alpha + // -  0.005782375075361113 1.2833751498937128e-07\n",
      "190\n",
      "15 of 15 trials complete (last trial train: 8s, validation: 260s) [a=100,k=94,kval=124]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2013-01-01 00:00:00+00:00\n",
      "New validation window: 2015-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 28\n",
      "no neg 25\n",
      "Alpha + // -  0.1423720037487113 0.18060940044027884\n",
      "53\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 30s) [a=25,k=86,kval=46]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1386072142913768 0.163808525980718\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 29s) [a=25,k=88,kval=56]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13020677706159633 0.1386072142913768\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 30s) [a=25,k=90,kval=67]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11970623052437084 0.10290535606481002\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 33s) [a=25,k=92,kval=86]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.09240480952758452 0.08400437229780411\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 3s, validation: 33s) [a=25,k=94,kval=125]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12180633983181594 0.12600655844670614\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 3s, validation: 57s) [a=50,k=86,kval=46]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10763060200656151 0.10185530141108746\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 3s, validation: 60s) [a=50,k=88,kval=56]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09660502814247472 0.08715453625897178\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 63s) [a=50,k=90,kval=67]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.06300327922335307 0.06615344318452072\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 3s, validation: 75s) [a=50,k=92,kval=86]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.04672743209065353 0.03780196753401185\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 4s, validation: 87s) [a=50,k=94,kval=125]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.0777040443754688 0.06825355249196584\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 9s, validation: 151s) [a=100,k=86,kval=46]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.06720349783824328 0.05880306060846287\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 7s, validation: 170s) [a=100,k=88,kval=56]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.04698994575408417 0.03780196753401185\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 8s, validation: 189s) [a=100,k=90,kval=67]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.02443017780301373 0.023593415500828573\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 10s, validation: 254s) [a=100,k=92,kval=86]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.00708786891262722 0.003150163961167654\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 9s, validation: 292s) [a=100,k=94,kval=125]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2013-05-01 00:00:00+00:00\n",
      "New validation window: 2015-05-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.14941049453778713 0.17846253514235683\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 26s) [a=25,k=86,kval=44]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.14188809116696105 0.16186136908260274\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 26s) [a=25,k=88,kval=52]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12865903696309447 0.14941049453778713\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 28s) [a=25,k=90,kval=64]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11205787090334036 0.11828330817574814\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 3s, validation: 28s) [a=25,k=92,kval=82]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.09130641332864768 0.09960699635852475\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 31s) [a=25,k=94,kval=116]no pos 50\n",
      "no neg 49\n",
      "Alpha + // -  0.11724573529701351 0.12214646862182788\n",
      "99\n",
      "6 of 15 trials complete (last trial train: 4s, validation: 52s) [a=50,k=86,kval=44]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10790757938840183 0.11102029802460572\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 5s, validation: 53s) [a=50,k=88,kval=52]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09338155908611695 0.10168214211599402\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 58s) [a=50,k=90,kval=64]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.07159252863268967 0.07470524726889356\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 4s, validation: 71s) [a=50,k=92,kval=82]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.04565320666432384 0.04772835242179311\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 5s, validation: 77s) [a=50,k=94,kval=116]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07366767439015892 0.0801525048822504\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 8s, validation: 154s) [a=100,k=86,kval=44]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.060698013405976026 0.06173558628471065\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 8s, validation: 163s) [a=100,k=88,kval=52]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.04150291514938531 0.04461563378558921\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 9s, validation: 198s) [a=100,k=90,kval=64]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.027236288066784117 0.02282660333216192\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 12s, validation: 292s) [a=100,k=92,kval=82]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.010764818616871813 0.007003616931458773\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 13s, validation: 311s) [a=100,k=94,kval=116]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2013-09-01 00:00:00+00:00\n",
      "New validation window: 2015-09-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.15618818242124702 0.17262904372874674\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 6s, validation: 29s) [a=25,k=86,kval=41]no pos 28\n",
      "no neg 25\n",
      "Alpha + // -  0.14055900334580937 0.16132595157984067\n",
      "53\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 33s) [a=25,k=88,kval=50]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12947178279656005 0.14257309415097386\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 5s, validation: 34s) [a=25,k=90,kval=61]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11714113681593527 0.12947178279656005\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 3s, validation: 34s) [a=25,k=92,kval=77]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.0945349525181232 0.10172782934015431\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 3s, validation: 38s) [a=25,k=94,kval=107]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12279268289038829 0.1315268904599975\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 7s, validation: 62s) [a=50,k=86,kval=41]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11303092148906033 0.10725093118564247\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 6s, validation: 68s) [a=50,k=88,kval=50]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09607628326570129 0.09864516784499812\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 6s, validation: 74s) [a=50,k=90,kval=61]no pos 51\n",
      "no neg 49\n",
      "Alpha + // -  0.07389243463090156 0.07269153126360725\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 8s, validation: 87s) [a=50,k=92,kval=77]no pos 50\n",
      "no neg 49\n",
      "Alpha + // -  0.05497412999695207 0.04784660419375953\n",
      "101\n",
      "10 of 15 trials complete (last trial train: 6s, validation: 94s) [a=50,k=94,kval=107]no pos 100\n",
      "no neg 98\n",
      "Alpha + // -  0.07398387588374859 0.08166305961563977\n",
      "201\n",
      "11 of 15 trials complete (last trial train: 12s, validation: 178s) [a=100,k=86,kval=41]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.06884610672515495 0.06806741358705562\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 12s, validation: 196s) [a=100,k=88,kval=50]no pos 100\n",
      "no neg 99\n",
      "Alpha + // -  0.05035013775421779 0.04784660419375953\n",
      "201\n",
      "13 of 15 trials complete (last trial train: 12s, validation: 216s) [a=100,k=90,kval=61]no pos 100\n",
      "no neg 101\n",
      "Alpha + // -  0.039047045605311755 0.030380948598522767\n",
      "201\n",
      "14 of 15 trials complete (last trial train: 15s, validation: 275s) [a=100,k=92,kval=77]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.02055107663437461 0.012330645980624765\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 13s, validation: 308s) [a=100,k=94,kval=107]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2014-01-01 00:00:00+00:00\n",
      "New validation window: 2016-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1498151003977079 0.19233019645651686\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 4s, validation: 32s) [a=25,k=86,kval=38]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13766793009519102 0.17006038423523595\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 7s, validation: 31s) [a=25,k=88,kval=46]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12147170302516856 0.13766793009519102\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 41s) [a=25,k=90,kval=58]no pos 25\n",
      "no neg 24\n",
      "Alpha + // -  0.10932453272265172 0.11943543573782345\n",
      "49\n",
      "4 of 15 trials complete (last trial train: 3s, validation: 40s) [a=25,k=92,kval=71]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.0844607935096875 0.10223868337951687\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 4s, validation: 42s) [a=25,k=94,kval=101]no pos 51\n",
      "no neg 50\n",
      "Alpha + // -  0.11808434971805087 0.127545288176427\n",
      "101\n",
      "6 of 15 trials complete (last trial train: 6s, validation: 81s) [a=50,k=86,kval=38]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10287134849943963 0.11843491044953934\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 6s, validation: 89s) [a=50,k=88,kval=46]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09312830565262922 0.10426321176326968\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 8s, validation: 82s) [a=50,k=90,kval=58]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.07920967301432866 0.0890792488851236\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 5s, validation: 75s) [a=50,k=92,kval=71]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.05871132312883147 0.056686794745078664\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 5s, validation: 94s) [a=50,k=94,kval=101]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07996887115823596 0.09464670194044383\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 10s, validation: 166s) [a=100,k=86,kval=38]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07288302181510113 0.07085849343134831\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 10s, validation: 204s) [a=100,k=88,kval=46]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.056686794745078664 0.05162547378569664\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 12s, validation: 232s) [a=100,k=90,kval=58]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.04049056767505618 0.04049056767505618\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 10s, validation: 267s) [a=100,k=92,kval=71]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.018347288477759835 0.014171698686269666\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 12s, validation: 314s) [a=100,k=94,kval=101]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2014-05-01 00:00:00+00:00\n",
      "New validation window: 2016-05-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1706736254310053 0.19677665049692383\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 5s, validation: 23s) [a=25,k=86,kval=34]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1526023003853695 0.1706736254310053\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 23s) [a=25,k=88,kval=40]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11645965029409776 0.1445706003650869\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 24s) [a=25,k=90,kval=53]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.0998942690022649 0.12850720032452168\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 4s, validation: 30s) [a=25,k=92,kval=70]no pos 24\n",
      "no neg 25\n",
      "Alpha + // -  0.08106323160882428 0.11244380028395647\n",
      "49\n",
      "5 of 15 trials complete (last trial train: 3s, validation: 40s) [a=25,k=94,kval=95]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12649927531945102 0.14256267536001624\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 7s, validation: 43s) [a=50,k=86,kval=34]no pos 50\n",
      "no neg 51\n",
      "Alpha + // -  0.0998942690022649 0.1199111390586122\n",
      "101\n",
      "7 of 15 trials complete (last trial train: 5s, validation: 56s) [a=50,k=88,kval=40]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.08985464397691163 0.10240417525860321\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 5s, validation: 65s) [a=50,k=90,kval=53]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.0790620470746569 0.08282690645916437\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 7s, validation: 78s) [a=50,k=92,kval=70]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.05822982514704888 0.06826945017240214\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 6s, validation: 88s) [a=50,k=94,kval=95]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.08219942989507976 0.09136058773071463\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 12s, validation: 175s) [a=100,k=86,kval=34]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07328926268507878 0.07328926268507878\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 22s, validation: 183s) [a=100,k=88,kval=40]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.06023775015211953 0.057225862644513556\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 11s, validation: 191s) [a=100,k=90,kval=53]no pos 100\n",
      "no neg 99\n",
      "Alpha + // -  0.04706074230634338 0.0434406192152312\n",
      "199\n",
      "14 of 15 trials complete (last trial train: 11s, validation: 237s) [a=100,k=92,kval=70]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.021585193804509496 0.02459708131211548\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 10s, validation: 303s) [a=100,k=94,kval=95]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2014-09-01 00:00:00+00:00\n",
      "New validation window: 2016-09-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.17677392739273923 0.2008794629462946\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 30s) [a=25,k=86,kval=32]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.16873874887488743 0.17677392739273923\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 30s) [a=25,k=88,kval=39]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1526683918391839 0.14613980929342932\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 32s) [a=25,k=90,kval=50]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12454526702670266 0.12956725360036\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 3s, validation: 34s) [a=25,k=92,kval=68]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.09240455295529551 0.11550569119411938\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 44s) [a=25,k=94,kval=95]no pos 50\n",
      "no neg 49\n",
      "Alpha + // -  0.13659803480348032 0.1431821583628547\n",
      "101\n",
      "6 of 15 trials complete (last trial train: 5s, validation: 61s) [a=50,k=86,kval=32]no pos 49\n",
      "no neg 50\n",
      "Alpha + // -  0.11074865855077631 0.12856285628562855\n",
      "99\n",
      "7 of 15 trials complete (last trial train: 6s, validation: 63s) [a=50,k=88,kval=39]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.1004397314731473 0.11148810193519351\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 5s, validation: 77s) [a=50,k=90,kval=50]no pos 50\n",
      "no neg 49\n",
      "Alpha + // -  0.0828627784653465 0.0918377403593478\n",
      "99\n",
      "9 of 15 trials complete (last trial train: 5s, validation: 86s) [a=50,k=92,kval=68]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.06428142814281428 0.06076603754125412\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 6s, validation: 95s) [a=50,k=94,kval=95]no pos 95\n",
      "no neg 100\n",
      "Alpha + // -  0.08574858163878823 0.09943533415841582\n",
      "195\n",
      "11 of 15 trials complete (last trial train: 10s, validation: 174s) [a=100,k=86,kval=32]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07708749390564056 0.08035178517851783\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 11s, validation: 190s) [a=100,k=88,kval=39]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.0632770308280828 0.06126823619861985\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 10s, validation: 202s) [a=100,k=90,kval=50]no pos 99\n",
      "no neg 100\n",
      "Alpha + // -  0.04457221516180852 0.048713269764476444\n",
      "199\n",
      "14 of 15 trials complete (last trial train: 10s, validation: 278s) [a=100,k=92,kval=68]no pos 101\n",
      "no neg 100\n",
      "Alpha + // -  0.027415314058337605 0.028123124812481245\n",
      "199\n",
      "15 of 15 trials complete (last trial train: 11s, validation: 319s) [a=100,k=94,kval=95]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2015-01-01 00:00:00+00:00\n",
      "New validation window: 2017-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1815361950417394 0.191510711252824\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 3s, validation: 27s) [a=25,k=86,kval=33]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1625846142406787 0.17056422720954637\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 29s) [a=25,k=88,kval=39]no pos 25\n",
      "no neg 26\n",
      "Alpha + // -  0.14812156573460605 0.1422740786613963\n",
      "51\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 31s) [a=25,k=90,kval=49]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12368400101744885 0.12368400101744885\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 3s, validation: 31s) [a=25,k=92,kval=67]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.09376045238419509 0.1066026420059665\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 3s, validation: 39s) [a=25,k=94,kval=92]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.13565342047075032 0.14762283992405184\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 5s, validation: 53s) [a=50,k=86,kval=33]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.1211903719646777 0.12368400101744885\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 6s, validation: 55s) [a=50,k=88,kval=39]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10373496859527968 0.095755355626412\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 5s, validation: 62s) [a=50,k=90,kval=49]no pos 50\n",
      "no neg 49\n",
      "Alpha + // -  0.08777574265754433 0.08212380090946894\n",
      "101\n",
      "9 of 15 trials complete (last trial train: 5s, validation: 74s) [a=50,k=92,kval=67]no pos 51\n",
      "no neg 50\n",
      "Alpha + // -  0.060733382709628664 0.059847097266507505\n",
      "99\n",
      "10 of 15 trials complete (last trial train: 6s, validation: 78s) [a=50,k=94,kval=92]no pos 100\n",
      "no neg 96\n",
      "Alpha + // -  0.08777574265754433 0.09402844445182931\n",
      "201\n",
      "11 of 15 trials complete (last trial train: 10s, validation: 155s) [a=100,k=86,kval=33]no pos 100\n",
      "no neg 99\n",
      "Alpha + // -  0.07879867806756821 0.07591261901298649\n",
      "201\n",
      "12 of 15 trials complete (last trial train: 11s, validation: 174s) [a=100,k=88,kval=39]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.05872496419276049 0.051953202796328854\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 10s, validation: 186s) [a=100,k=90,kval=49]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.041892968086555245 0.03540953254935027\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 9s, validation: 227s) [a=100,k=92,kval=67]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.02381415745396444 0.023938838906603\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 17s, validation: 307s) [a=100,k=94,kval=92]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2015-05-01 00:00:00+00:00\n",
      "New validation window: 2017-05-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.18040820664399407 0.19844902730839348\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 20s) [a=25,k=86,kval=32]no pos 25\n",
      "no neg 26\n",
      "Alpha + // -  0.15635377909146156 0.17225193419938564\n",
      "51\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 20s) [a=25,k=88,kval=39]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.14332429750050643 0.14432656531519528\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 22s) [a=25,k=90,kval=49]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12277780729938485 0.132299351538929\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 25s) [a=25,k=92,kval=67]no pos 25\n",
      "no neg 26\n",
      "Alpha + // -  0.10223131709826332 0.11316119932790548\n",
      "48\n",
      "5 of 15 trials complete (last trial train: 3s, validation: 29s) [a=25,k=94,kval=90]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.14131976187112874 0.1508413061106728\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 4s, validation: 39s) [a=50,k=86,kval=32]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12127440557735156 0.1343038871683067\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 5s, validation: 47s) [a=50,k=88,kval=39]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10674152226436316 0.1092471918010853\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 56s) [a=50,k=90,kval=49]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09321090676606361 0.08694673292425828\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 4s, validation: 63s) [a=50,k=92,kval=67]no pos 50\n",
      "no neg 49\n",
      "Alpha + // -  0.06665080967680893 0.06871664122600589\n",
      "101\n",
      "10 of 15 trials complete (last trial train: 5s, validation: 69s) [a=50,k=94,kval=90]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.09346147371973582 0.10273245100560775\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 8s, validation: 124s) [a=100,k=86,kval=32]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.08318822861917505 0.08719729987793048\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 18s, validation: 138s) [a=100,k=88,kval=39]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.06364400623274234 0.06564854186212006\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 8s, validation: 166s) [a=100,k=90,kval=49]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.04810885510506509 0.05036395768811502\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 8s, validation: 205s) [a=100,k=92,kval=67]no pos 100\n",
      "no neg 101\n",
      "Alpha + // -  0.021047624108465977 0.02929027363768997\n",
      "198\n",
      "15 of 15 trials complete (last trial train: 11s, validation: 271s) [a=100,k=94,kval=90]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2015-09-01 00:00:00+00:00\n",
      "New validation window: 2017-09-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.18906640202487748 0.2011344702392314\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 3s, validation: 36s) [a=25,k=86,kval=32]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.15990190384018896 0.16845011882535627\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 4s, validation: 36s) [a=25,k=88,kval=39]no pos 23\n",
      "no neg 24\n",
      "Alpha + // -  0.1517623660104333 0.14648307721760617\n",
      "53\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 38s) [a=25,k=90,kval=50]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1287260609531081 0.1287260609531081\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 40s) [a=25,k=92,kval=67]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.09051051160765412 0.10458992452440033\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 3s, validation: 43s) [a=25,k=94,kval=95]no pos 51\n",
      "no neg 49\n",
      "Alpha + // -  0.14299047731629622 0.14648307721760617\n",
      "103\n",
      "6 of 15 trials complete (last trial train: 4s, validation: 67s) [a=50,k=86,kval=32]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.1267147162507158 0.12973173330430424\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 4s, validation: 69s) [a=50,k=88,kval=39]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10810977775358689 0.102578579822008\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 76s) [a=50,k=90,kval=50]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.0884991669052618 0.08246513279808486\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 4s, validation: 104s) [a=50,k=92,kval=67]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.06024605928884477 0.06436303047655405\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 5s, validation: 131s) [a=50,k=94,kval=95]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.09855589041722339 0.09403036483684067\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 7s, validation: 158s) [a=100,k=86,kval=32]no pos 100\n",
      "no neg 99\n",
      "Alpha + // -  0.08347080514928101 0.07900678370072804\n",
      "201\n",
      "12 of 15 trials complete (last trial train: 8s, validation: 219s) [a=100,k=88,kval=39]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.06838571988133867 0.061848849598563654\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 8s, validation: 261s) [a=100,k=90,kval=50]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.051289289911004 0.042238238750238584\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 8s, validation: 298s) [a=100,k=92,kval=67]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.028158825833492395 0.02916449818468855\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 9s, validation: 408s) [a=100,k=94,kval=95]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2016-01-01 00:00:00+00:00\n",
      "New validation window: 2018-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.17985107452404026 0.1991571220718186\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 3s, validation: 71s) [a=25,k=86,kval=35]no pos 25\n",
      "no neg 26\n",
      "Alpha + // -  0.16460945803895208 0.171040879396116\n",
      "49\n",
      "2 of 15 trials complete (last trial train: 4s, validation: 69s) [a=25,k=88,kval=43]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.15241616485088158 0.14987589543670024\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 71s) [a=25,k=90,kval=53]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13615844060012086 0.12193293188070525\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 75s) [a=25,k=92,kval=71]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.10770742316128966 0.10364299209859947\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 4s, validation: 86s) [a=25,k=94,kval=102]no pos 46\n",
      "no neg 50\n",
      "Alpha + // -  0.1464193677138002 0.1463195182568463\n",
      "101\n",
      "6 of 15 trials complete (last trial train: 5s, validation: 129s) [a=50,k=86,kval=35]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.1280295784747405 0.11380406975532491\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 7s, validation: 134s) [a=50,k=88,kval=43]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11380406975532491 0.10161077656725438\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 5s, validation: 144s) [a=50,k=90,kval=53]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.09348191444187402 0.08941748337918384\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 5s, validation: 154s) [a=50,k=92,kval=71]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.07315975912842315 0.0614745198231889\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 6s, validation: 173s) [a=50,k=94,kval=102]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.10059466880158184 0.09348191444187402\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 9s, validation: 285s) [a=100,k=86,kval=35]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.08230472901947604 0.07722419019111333\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 8s, validation: 294s) [a=100,k=88,kval=43]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.06807922030006043 0.05588592711198991\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 8s, validation: 337s) [a=100,k=90,kval=53]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.0525200701381996 0.04356562045321031\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 8s, validation: 377s) [a=100,k=92,kval=71]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.03327752932577581 0.02438658637614105\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 9s, validation: 481s) [a=100,k=94,kval=102]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2016-05-01 00:00:00+00:00\n",
      "New validation window: 2018-05-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.17426421069432116 0.17831686675697977\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 79s) [a=25,k=86,kval=39]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.15602725841235732 0.16007991447501596\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 81s) [a=25,k=88,kval=49]no pos 26\n",
      "no neg 25\n",
      "Alpha + // -  0.14792676969971225 0.12968499400507622\n",
      "49\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 80s) [a=25,k=90,kval=61]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12157968187975896 0.115500697785771\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 85s) [a=25,k=92,kval=81]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1013164015664658 0.09726374550380716\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 88s) [a=25,k=94,kval=114]no pos 51\n",
      "no neg 50\n",
      "Alpha + // -  0.13979672212479388 0.12689879296199844\n",
      "99\n",
      "6 of 15 trials complete (last trial train: 4s, validation: 144s) [a=50,k=86,kval=39]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11803360782493266 0.1134743697544417\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 4s, validation: 151s) [a=50,k=88,kval=49]no pos 49\n",
      "no neg 50\n",
      "Alpha + // -  0.10444852787221623 0.0962505814881425\n",
      "99\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 157s) [a=50,k=90,kval=61]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.08611894133149592 0.08611894133149592\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 4s, validation: 169s) [a=50,k=92,kval=81]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.0699083170808614 0.06408262399078962\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 5s, validation: 201s) [a=50,k=94,kval=114]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.09270450743331618 0.08865185137065756\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 8s, validation: 321s) [a=100,k=86,kval=39]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07902679322184332 0.07396097314352001\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 8s, validation: 340s) [a=100,k=88,kval=49]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.06864186206128056 0.05724376688505317\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 8s, validation: 394s) [a=100,k=90,kval=61]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.05084816903617003 0.04255288865791563\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 9s, validation: 452s) [a=100,k=92,kval=81]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.03039492046993974 0.022289608344622472\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 8s, validation: 531s) [a=100,k=94,kval=114]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2016-09-01 00:00:00+00:00\n",
      "New validation window: 2018-09-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.181552954305089 0.15911382512131395\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 58s) [a=25,k=86,kval=45]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.14993418136431505 0.15503398345153666\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 59s) [a=25,k=88,kval=55]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13871461677242755 0.14279445844220484\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 61s) [a=25,k=90,kval=72]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12239525009331842 0.11296061623195845\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 3s, validation: 73s) [a=25,k=92,kval=94]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.11321560633631952 0.09383635840487746\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 84s) [a=25,k=94,kval=135]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.1376946563549832 0.12239525009331842\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 4s, validation: 129s) [a=50,k=86,kval=45]no pos 48\n",
      "no neg 50\n",
      "Alpha + // -  0.12483919577618155 0.11015572508398656\n",
      "98\n",
      "7 of 15 trials complete (last trial train: 5s, validation: 148s) [a=50,k=88,kval=55]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.11270562612759737 0.10199604174443201\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 155s) [a=50,k=90,kval=72]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.0958762792397661 0.07751699172576833\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 5s, validation: 183s) [a=50,k=92,kval=94]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.08044937792592076 0.05813774379432625\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 4s, validation: 213s) [a=50,k=94,kval=135]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.10811580424909792 0.08669663548276724\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 9s, validation: 353s) [a=100,k=86,kval=45]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.08975651673510018 0.07445711047343537\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 8s, validation: 401s) [a=100,k=88,kval=55]no pos 100\n",
      "no neg 101\n",
      "Alpha + // -  0.07330965500381052 0.05880497620119687\n",
      "201\n",
      "13 of 15 trials complete (last trial train: 8s, validation: 436s) [a=100,k=90,kval=72]no pos 101\n",
      "no neg 100\n",
      "Alpha + // -  0.053537836857988304 0.030598812523329605\n",
      "201\n",
      "14 of 15 trials complete (last trial train: 9s, validation: 470s) [a=100,k=92,kval=94]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.0348698967713777 0.013068242848505352\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 9s, validation: 632s) [a=100,k=94,kval=135]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2017-01-01 00:00:00+00:00\n",
      "New validation window: 2019-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.17701399129119813 0.2056922356903307\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 3s, validation: 23s) [a=25,k=86,kval=44]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.16218041660199156 0.17009165643623503\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 2s, validation: 24s) [a=25,k=88,kval=55]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1424023170163828 0.1582247966848698\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 3s, validation: 28s) [a=25,k=90,kval=71]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12855764730645672 0.13053545726501758\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 2s, validation: 55s) [a=25,k=92,kval=96]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1147129775965306 0.09889049792804361\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 57s) [a=25,k=94,kval=135]no pos 49\n",
      "no neg 50\n",
      "Alpha + // -  0.13951394672350645 0.1369633396303404\n",
      "101\n",
      "6 of 15 trials complete (last trial train: 5s, validation: 52s) [a=50,k=86,kval=44]no pos 50\n",
      "no neg 49\n",
      "Alpha + // -  0.12596177173584558 0.12450122957955724\n",
      "99\n",
      "7 of 15 trials complete (last trial train: 4s, validation: 99s) [a=50,k=88,kval=55]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.1147129775965306 0.10828509523120776\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 5s, validation: 110s) [a=50,k=90,kval=71]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.10185721286588491 0.08479860197329742\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 4s, validation: 145s) [a=50,k=92,kval=96]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.07626929652700365 0.05340086888114355\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 5s, validation: 152s) [a=50,k=94,kval=135]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.10482392780372624 0.08405692323883708\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 8s, validation: 280s) [a=100,k=86,kval=44]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.09493487801092187 0.06897612230481043\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 10s, validation: 302s) [a=100,k=88,kval=55]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.07812349336315445 0.052906416391503334\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 9s, validation: 317s) [a=100,k=90,kval=71]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.054018934493193824 0.034117221785175046\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 10s, validation: 336s) [a=100,k=92,kval=96]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.03733116296783647 0.007911239834243489\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 9s, validation: 449s) [a=100,k=94,kval=135]Saving best config...\n",
      "Selecting articles by date...\n",
      "New training window: 2017-05-01 00:00:00+00:00\n",
      "New validation window: 2019-05-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1955269575469531 0.2793242250670759\n",
      "50\n",
      "1 of 15 trials complete (last trial train: 2s, validation: 31s) [a=25,k=86,kval=44]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1875462654021795 0.22146420701746727\n",
      "50\n",
      "2 of 15 trials complete (last trial train: 3s, validation: 62s) [a=25,k=88,kval=54]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.1755752271850191 0.16160901593166532\n",
      "50\n",
      "3 of 15 trials complete (last trial train: 2s, validation: 64s) [a=25,k=90,kval=71]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.13567176646115114 0.11172969002683034\n",
      "50\n",
      "4 of 15 trials complete (last trial train: 3s, validation: 75s) [a=25,k=92,kval=91]no pos 25\n",
      "no neg 25\n",
      "Alpha + // -  0.12658437677286397 0.09576830573728315\n",
      "50\n",
      "5 of 15 trials complete (last trial train: 2s, validation: 86s) [a=25,k=94,kval=131]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.14963797771450493 0.16659694852214882\n",
      "100\n",
      "6 of 15 trials complete (last trial train: 5s, validation: 124s) [a=50,k=86,kval=44]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.13966211253353794 0.13966211253353794\n",
      "100\n",
      "7 of 15 trials complete (last trial train: 4s, validation: 132s) [a=50,k=88,kval=54]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.12669348779828082 0.10399839451158091\n",
      "100\n",
      "8 of 15 trials complete (last trial train: 4s, validation: 139s) [a=50,k=90,kval=71]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.1089863271020644 0.07082864278486567\n",
      "100\n",
      "9 of 15 trials complete (last trial train: 5s, validation: 231s) [a=50,k=92,kval=91]no pos 50\n",
      "no neg 50\n",
      "Alpha + // -  0.0942719259601381 0.05087691242293167\n",
      "100\n",
      "10 of 15 trials complete (last trial train: 4s, validation: 236s) [a=50,k=94,kval=131]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.11447305295159627 0.09576830573728315\n",
      "200\n",
      "11 of 15 trials complete (last trial train: 9s, validation: 452s) [a=100,k=86,kval=44]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.1022526181049117 0.06983105626676897\n",
      "200\n",
      "12 of 15 trials complete (last trial train: 9s, validation: 462s) [a=100,k=88,kval=54]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.09078037314679965 0.047884152868641575\n",
      "200\n",
      "13 of 15 trials complete (last trial train: 10s, validation: 468s) [a=100,k=90,kval=71]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.06185036412199536 0.025937249470514186\n",
      "200\n",
      "14 of 15 trials complete (last trial train: 9s, validation: 514s) [a=100,k=92,kval=91]no pos 100\n",
      "no neg 100\n",
      "Alpha + // -  0.046886566350544875 0.007232502256201071\n",
      "200\n",
      "15 of 15 trials complete (last trial train: 11s, validation: 619s) [a=100,k=94,kval=131]Saving best config...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def validate_window(index, val_d, val_p, sentiment_words, LAM, trials):\n",
    "    error_arr = np.array(0)\n",
    "    # print(str(index) + \" computing lambda of \" + str(LAM))\n",
    "    for val_index in range(len(val_d)):\n",
    "        est_p = 0.5\n",
    "        val_bow = val_d[val_index]\n",
    "\n",
    "        testing_s = sum(val_bow.get(w,0) for w in sentiment_words)\n",
    "        if (testing_s > 0):\n",
    "            est_p = fminbound(equation_to_solve, 0, 1, (O,val_bow, sentiment_words,testing_s,LAM))\n",
    "        error_arr = np.append(error_arr, est_p - val_p[val_index])\n",
    "    normalised_error = np.linalg.norm(error_arr, 1)\n",
    "    lam_trial = {\n",
    "        'alpha': alpha,\n",
    "        'alpha_plus': ALPHA_PLUS,\n",
    "        'alpha_minus': ALPHA_MINUS,\n",
    "        'kappa': KAPPA,\n",
    "        'lam': LAM,\n",
    "        'o': O,\n",
    "        'sentiment_words': sentiment_words,\n",
    "        'norm_err': normalised_error\n",
    "    }\n",
    "    trials.append(lam_trial)\n",
    "\n",
    "# reset global variables\n",
    "sgn = []\n",
    "y = []\n",
    "dates = []\n",
    "global_bow = {}\n",
    "d = []\n",
    "kappa_configs   = [86, 88, 90, 92, 94]\n",
    "alpha_configs   = [25,50,100]\n",
    "lambda_configs  = [1,5,10]\n",
    "KAPPA_BIGRAM = 90\n",
    "# kappa_configs   = [86]\n",
    "# alpha_configs   = [25]\n",
    "# lambda_configs  = [1]\n",
    "curr_year = 2010\n",
    "curr_month = 1\n",
    "opening_date = datetime(curr_year,curr_month,1,0,0,0,0)\n",
    "list_dates = [a['date'] for a in article_list]\n",
    "# end_date = datetime.strptime(max(list_dates), '%Y-%m-%d %H:%M:%S%z')\n",
    "end_date = max(list_dates)\n",
    "\n",
    "destination_directory = './data/models/' + model_id\n",
    "if not os.path.exists(destination_directory):\n",
    "    os.mkdir(destination_directory)\n",
    "    os.mkdir(os.path.join(destination_directory,'word-lists'))\n",
    "\n",
    "# reset saved config files\n",
    "with open(destination_directory + '/configurations.csv', 'w', newline='') as csv_file:\n",
    "    csvwriter = csv.writer(csv_file)\n",
    "    csvwriter.writerow([('date'), ('no. sentiment words'), ('alpha plus'), ('alpha_minus'), ('kappa'), ('lambda'), ('normalised error')])\n",
    "\n",
    "\n",
    "while(datetime(curr_year+3, curr_month, 1, 0,0,0,0).date() < end_date.date()):\n",
    "    #SELECT ARTICLES\n",
    "    val_date_start = datetime(curr_year+2,curr_month,1,0,0,0,0)\n",
    "    val_date_end = datetime(curr_year+3,curr_month,1,0,0,0,0)\n",
    "    curr_date = datetime(curr_year, curr_month, 1,0,0,0,0,est)\n",
    "    val_date = datetime(curr_year + 2, curr_month, 1,0,0,0,0,est)\n",
    "    val_end = datetime(curr_year + 3, curr_month, 1,0,0,0,0,est)\n",
    "    print('Selecting articles by date...')\n",
    "    print(\"New training window: \" + str(curr_date))\n",
    "    print(\"New validation window: \" + str(val_date))\n",
    "    training_arts   = [a for a in article_list if (a['date'] >= curr_date and a['date'] < val_date)]\n",
    "    validation_arts = [a for a in article_list if (a['date'] >= val_date and a['date'] < val_end)]\n",
    "    # print(training_arts)\n",
    "\n",
    "    #PRE-PROCESS\n",
    "    print('Preprocessing data...')\n",
    "    train_d = []\n",
    "    train_d_bigram = []\n",
    "    train_sgn = []\n",
    "    train_y = []\n",
    "    val_d = []\n",
    "    val_sgn =[]\n",
    "    val_y = []\n",
    "    for train_a in training_arts:\n",
    "        train_bow = text_to_bow(train_a['headline'])\n",
    "        train_bow_bigram = text_to_bow_bigram(train_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(train_a)\n",
    "        train_d.append(train_bow)\n",
    "        train_d_bigram.append(train_bow_bigram)\n",
    "        train_sgn.append(sgn_a)\n",
    "        train_y.append(returns)\n",
    "    for val_a in validation_arts:\n",
    "        val_bow = text_to_bow_bigram(val_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(val_a)\n",
    "        val_d.append(val_bow)\n",
    "        val_y.append(returns)\n",
    "    \n",
    "    # fraction of positively tagged training articles\n",
    "    train_pi = sum(sgn_i > 0 for sgn_i in train_sgn)/len(train_sgn)\n",
    "\n",
    "    # start training\n",
    "    print('Beginning trials')\n",
    "    trials = []\n",
    "    curr_trial = 0\n",
    "    # pre calculations (things not affected by the changes we make)\n",
    "    (pos_j, total_j, f) = calc_f(train_d, train_sgn)\n",
    "    (pos_j_bigram, total_j_bigram, f_bigram) = calc_f(train_d_bigram, train_sgn)\n",
    "    p                   = calc_p(train_y)\n",
    "    val_p               = calc_p(val_y)\n",
    "    #PARAM GRID\n",
    "    kappa_percentile_bigram = np.percentile(np.array(list(total_j.values())),KAPPA_BIGRAM) # return the nth percentile of all appearances for KAPPA\n",
    "    bigrams_to_remove = []\n",
    "    mutual_info = {}\n",
    "    for w in total_j_bigram:\n",
    "        component_words = w.split()\n",
    "        if not (total_j[component_words[0]] >= kappa_percentile_bigram and total_j[component_words[1]] >= kappa_percentile_bigram):\n",
    "            bigrams_to_remove.append(w)\n",
    "        else:\n",
    "            mutual_info[w] = total_j_bigram[w] / (total_j[component_words[0]] * total_j[component_words[1]])\n",
    "    mutual_info_percentile = np.percentile(np.array(list(mutual_info.values())),95) # return the nth percentile of all appearances for KAPPA\n",
    "    bigrams_to_remove.extend([w for w in mutual_info if mutual_info[w] <= mutual_info_percentile])\n",
    "    for b in bigrams_to_remove:\n",
    "        pos_j_bigram.pop(b)\n",
    "        total_j_bigram.pop(b)\n",
    "        f_bigram.pop(b)\n",
    "\n",
    "    for alpha in alpha_configs:\n",
    "        for KAPPA in kappa_configs:\n",
    "            #TRAINING\n",
    "            train_time_0 = time.time()\n",
    "            kappa_percentile = np.percentile(np.array(list(total_j_bigram.values())),KAPPA) # return the nth percentile of all appearances for KAPPA\n",
    "\n",
    "            #calculate alpha vals (NOW WITH QUICKER SEARCHING)\n",
    "            ALPHA_PLUS  = train_pi/2\n",
    "            ALPHA_MINUS = train_pi/2\n",
    "            delta_plus  = train_pi/4\n",
    "            delta_minus  = train_pi/4\n",
    "            delta_limit = 0.0000001\n",
    "\n",
    "            while(delta_plus > delta_limit):\n",
    "                no_pos_words = len([w for w in total_j_bigram if f_bigram[w] >= train_pi + ALPHA_PLUS and total_j_bigram[w] >= kappa_percentile])\n",
    "                if no_pos_words == alpha:\n",
    "                    delta_plus = 0\n",
    "                elif (no_pos_words > alpha):\n",
    "                    ALPHA_PLUS += delta_plus\n",
    "                    delta_plus /= 2\n",
    "                else:\n",
    "                    ALPHA_PLUS -= delta_plus\n",
    "                    delta_plus /= 2\n",
    "            while(delta_minus > delta_limit):\n",
    "                no_neg_words = len([w for w in total_j_bigram if f_bigram[w] <= train_pi - ALPHA_MINUS and total_j_bigram[w] >= kappa_percentile])\n",
    "                if no_neg_words == alpha:\n",
    "                    delta_minus = 0\n",
    "                elif (no_neg_words > alpha):\n",
    "                    ALPHA_MINUS += delta_minus\n",
    "                    delta_minus /= 2\n",
    "                else:\n",
    "                    ALPHA_MINUS -= delta_minus\n",
    "                    delta_minus /= 2\n",
    "            print('no pos '  + str(no_pos_words))\n",
    "            print('no neg ' + str(no_neg_words))\n",
    "            print('Alpha + // - ', ALPHA_PLUS, ALPHA_MINUS)\n",
    "            # sentiment_words = [w for w in total_j_bigram]\n",
    "            sentiment_words = [w for w in total_j_bigram if ((f_bigram[w] >= train_pi + ALPHA_PLUS or f_bigram[w] <= train_pi - ALPHA_MINUS) and total_j_bigram[w] >= kappa_percentile)]\n",
    "            print(len(sentiment_words))\n",
    "\n",
    "            (s, d_s)    = calc_s(sentiment_words, train_d_bigram)\n",
    "            h           = calc_h(sentiment_words, train_d_bigram, s, d_s)\n",
    "            O           = calc_o(p,h)\n",
    "            train_time_1 = time.time()\n",
    "            # for LAM in lambda_configs:\n",
    "\n",
    "            #VALIDATING\n",
    "            # start multithreading here\n",
    "            t0 = time.time()\n",
    "            threads = []\n",
    "            for index in range(3):\n",
    "                logging.info(\"Main    : create and start thread %d.\", index)\n",
    "                x = threading.Thread(target=validate_window, args=(index,val_d, val_p, sentiment_words,lambda_configs[index],trials))\n",
    "                threads.append(x)\n",
    "                x.start()\n",
    "\n",
    "            for index, thread in enumerate(threads):\n",
    "                logging.info(\"Main    : before joining thread %d.\", index)\n",
    "                thread.join()\n",
    "                logging.info(\"Main    : thread %d done\", index)\n",
    "            curr_trial += 1\n",
    "            t1 = time.time()\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"%d of %d trials complete (last trial train: %ds, validation: %ds) [a=%d,k=%d,kval=%d]\" % (curr_trial, 15, train_time_1-train_time_0, t1-t0, alpha, KAPPA, kappa_percentile))\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    #RECORD BEST CONFIG\n",
    "    print(\"Saving best config...\")\n",
    "    best_config = min(trials, key=lambda x:x['norm_err'])\n",
    "    with open(destination_directory + '/configurations.csv', 'a', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str(curr_year) + '-' + str(curr_month) + '-1', str(best_config['alpha']), str(best_config['alpha_plus']), str(best_config['alpha_minus']), str(best_config['kappa']), str(best_config['lam']), str(best_config['norm_err'])])\n",
    "    with open(destination_directory + '/word-lists/' + str(curr_year) + '-' + str(curr_month) + '-1' + '.csv', 'w', newline='') as csv_file:\n",
    "        #write header\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow(['word', 'O+ value', 'O- value'])\n",
    "    with open(destination_directory + '/word-lists/' + str(curr_year) + '-' + str(curr_month) + '-1' + '.csv', 'a', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        for ind in range(len(best_config['sentiment_words'])):\n",
    "            csvwriter.writerow([str(best_config['sentiment_words'][ind]), str(best_config['o'][ind][0]), str(best_config['o'][ind][1])])\n",
    "\n",
    "    #record start date, sentiment words, O, and params probably\n",
    "    #i think maybe start date and params in a different file to sentiment words and O.\n",
    "\n",
    "    #MOVE WINDOW\n",
    "    curr_month += 4\n",
    "    if (curr_month > 12):\n",
    "        curr_month = 1\n",
    "        curr_year += 1\n",
    "    # opening_date += relativedelta(months=4)\n",
    "\n",
    "# #2020 SPECIAL CASE\n",
    "\n",
    "\n",
    "# TOTAL_ARTS = len(article_list)\n",
    "# curr_index = 0\n",
    "# curr_thousand = 0\n",
    "# for a in article_list:\n",
    "#     raw_html = a['headline']\n",
    "#     if(raw_html):\n",
    "#         bow_art = text_to_bow(raw_html)\n",
    "#         (returns, sgn_a) = calc_returns(a)\n",
    "#         dates.append(a['date'])\n",
    "#         d.append(bow_art)\n",
    "#         sgn.append(sgn_a)\n",
    "#         y.append(returns)\n",
    "#     if curr_index >= curr_thousand:\n",
    "#         curr_thousand += 10000\n",
    "#         sys.stdout.write('\\r')\n",
    "#         j = (curr_index + 1) / TOTAL_ARTS\n",
    "#         sys.stdout.write(\"%d out of %d articles processed\" % (curr_index, TOTAL_ARTS))\n",
    "#         sys.stdout.flush()\n",
    "#     curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the data\n",
    "\n",
    "With the data all collected, we can do a little visualisation and see what outputs we actually got. First, let's observe the metadata of the dataset by seeing the distribution of articles by year, and then month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-05 10:30:00-04:00\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(article_list[0]['date'])\n",
    "print(article_list[1]['date'].month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise year\n",
    "\n",
    "list_dates = [a['date'] for a in article_list]\n",
    "years = []\n",
    "year_counts = []\n",
    "for year in range(2009,2021):\n",
    "    curr_year = datetime(year, 1, 1,0,0,0,0,est)\n",
    "    curr_year_1 = datetime(year+1, 1, 1,0,0,0,0,est)\n",
    "    year_counts.append(len([a for a in article_list if (a['date'] <= curr_year_1  and a['date'] >= curr_year)]))\n",
    "    years.append(year)\n",
    "\n",
    "month_counts = []\n",
    "months = []\n",
    "for month in range(1, 13):\n",
    "    month_counts.append(len([a for a in article_list if a['date'].month == month]))\n",
    "    months.append(month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.bar(years, year_counts)\n",
    "plt.title(\"Article count by year\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.bar(months, month_counts)\n",
    "plt.title(\"Article count by month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-1-1 & 100 & 0.0529 & 0.0487 & 92 & 5 & 20402.2 & 0.24727\\\\\n",
      "2010-5-1 & 100 & 0.0405 & 0.0384 & 94 & 5 & 20714.66 & 0.24926\\\\\n",
      "2010-9-1 & 25 & 0.1116 & 0.1043 & 92 & 5 & 20426.81 & 0.24675\\\\\n",
      "2011-1-1 & 25 & 0.1023 & 0.1064 & 92 & 1 & 19963.67 & 0.24574\\\\\n",
      "2011-5-1 & 50 & 0.1002 & 0.0921 & 90 & 5 & 19075.08 & 0.24598\\\\\n",
      "2011-9-1 & 100 & 0.0425 & 0.0404 & 94 & 5 & 19255.78 & 0.24653\\\\\n",
      "2012-1-1 & 100 & 0.0754 & 0.0744 & 88 & 5 & 20474.7 & 0.24744\\\\\n",
      "2012-5-1 & 100 & 0.051 & 0.0489 & 92 & 10 & 21839.81 & 0.24961\\\\\n",
      "2012-9-1 & 100 & 0.0536 & 0.0473 & 92 & 10 & 22243.55 & 0.24931\\\\\n",
      "2013-1-1 & 50 & 0.0536 & 0.0672 & 94 & 5 & 21546.5 & 0.24702\\\\\n",
      "2013-5-1 & 100 & 0.084 & 0.0913 & 86 & 5 & 22095.56 & 0.24552\\\\\n",
      "2013-9-1 & 100 & 0.0688 & 0.076 & 88 & 10 & 23415.73 & 0.24885\\\\\n",
      "2014-1-1 & 100 & 0.0395 & 0.0375 & 94 & 5 & 24819.11 & 0.24818\\\\\n",
      "2014-5-1 & 100 & 0.0823 & 0.0954 & 86 & 5 & 24060.98 & 0.24652\\\\\n",
      "2014-9-1 & 100 & 0.0392 & 0.0412 & 94 & 5 & 23536.73 & 0.24904\\\\\n",
      "2015-1-1 & 100 & 0.0778 & 0.0898 & 86 & 5 & 22801.46 & 0.24805\\\\\n",
      "2015-5-1 & 100 & 0.0471 & 0.0571 & 92 & 5 & 23642.32 & 0.24871\\\\\n",
      "2015-9-1 & 100 & 0.0603 & 0.0734 & 90 & 5 & 26361.23 & 0.24772\\\\\n",
      "2016-1-1 & 100 & 0.0478 & 0.0518 & 92 & 5 & 29950.54 & 0.24818\\\\\n"
     ]
    }
   ],
   "source": [
    "curr_year = 2010\n",
    "curr_month = 1\n",
    "\n",
    "config_data = []\n",
    "path = './data/models/stemming/configurations.csv'\n",
    "with open(str(path), encoding='utf-8') as csv_file:\n",
    "    #date,no. sentiment words,alpha plus,alpha_minus,kappa,lambda,normalised error\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_no = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        if line_no > 0:\n",
    "            config_data.append([row[0],row[1],round(float(row[2]),4),round(float(row[3]),4),row[4],row[5],round(float(row[6]),2),row[6]])\n",
    "        line_no +=  1\n",
    "index = 0\n",
    "\n",
    "while(index < len(config_data)):\n",
    "    #SELECT ARTICLES\n",
    "    val_date_start = datetime(curr_year+2,curr_month,1,0,0,0,0)\n",
    "    val_date_end = datetime(curr_year+3,curr_month,1,0,0,0,0)\n",
    "    curr_date = datetime(curr_year, curr_month, 1,0,0,0,0,est)\n",
    "    val_date = datetime(curr_year + 2, curr_month, 1,0,0,0,0,est)\n",
    "    val_end = datetime(curr_year + 3, curr_month, 1,0,0,0,0,est)\n",
    "    # training_arts   = [a for a in article_list if (a['date'] >= curr_date and a['date'] < val_date)]\n",
    "    validation_arts = [a for a in article_list if (a['date'] >= val_date and a['date'] < val_end)]\n",
    "    config_data[index][7]  = round(float(config_data[index][7])/len(validation_arts),5)\n",
    "\n",
    "    curr_month += 4\n",
    "    if (curr_month > 12):\n",
    "        curr_month = 1\n",
    "        curr_year += 1\n",
    "    index += 1\n",
    "\n",
    "for row in config_data:\n",
    "    print(str(row[0]) + ' & '  + str(row[1]) + ' & '  + str(row[2]) + ' & '  + str(row[3]) + ' & '  + str(row[4]) + ' & '  +str(row[5]) + ' & '  +str(row[6]) + ' & '  +str(row[7]) + '\\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the actual word data we pulled. We'll load it in wordcloud form, with the top 50 words of each sentiment represented by the average tone over all 19 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high & 0.010033554431779335 & 19\n",
      "mover & 0.007526084353512696 & 19\n",
      "gainer & 0.013524412572822343 & 19\n",
      "upgrade & 0.016794951772062014 & 19\n",
      "rais & 0.011851335286498023 & 18\n",
      "repurchase & 0.00029833383030760405 & 17\n",
      "volume & 0.0027424810346107606 & 16\n",
      "rumor & 0.000780093836058141 & 16\n",
      "author & 6.813530302230894e-05 & 16\n",
      "outperform & 0.0028570636749356837 & 15\n",
      "spike & 0.0021885802468981385 & 15\n",
      "solid & 0.0004322402200864615 & 15\n",
      "higher & 0.006567234447939277 & 15\n",
      "buy & 0.008343607042205062 & 14\n",
      "green & 0.0007113377382261722 & 14\n",
      "overweight & 0.0016754338272568956 & 13\n",
      "soar & 0.001171020355831584 & 13\n",
      "strong & 0.0005446015638093819 & 13\n",
      "lift & 0.0006586259437203682 & 12\n",
      "strength & 0.0001468968678047511 & 11\n",
      "jump & 0.0008559254623020766 & 11\n",
      "mention & 9.679605226212383e-05 & 11\n",
      "special & 0.00015433463272212857 & 11\n",
      "stake & 0.0008152933904297229 & 10\n",
      "boost & 0.00032449978065458813 & 10\n",
      "pop & 0.00035887324957033034 & 10\n",
      "narrow & 0.0004710606095422343 & 10\n",
      "dynamic & 5.192311429043353e-05 & 10\n",
      "chatter & 0.0008732984556378201 & 10\n",
      "expansion & 0.00024290218930304424 & 9\n",
      "add & 0.00019090273667751292 & 8\n",
      "final & 0.00021559381862979327 & 8\n",
      "dividend & 0.000862015680765785 & 8\n",
      "unconfirm & 0.00013999738245016992 & 8\n",
      "f & 0.00032127241631314715 & 8\n",
      "rally & 0.00044032724736777477 & 8\n",
      "proceed & 1.358742896325985e-07 & 8\n",
      "steel & 0.001327569605802362 & 8\n",
      "beat & 0.00040422492134342095 & 8\n",
      "upside & 0.0002705540918303914 & 8\n",
      "test & 2.072096066807115e-05 & 8\n",
      "base & 0.000406787102679504 & 8\n",
      "micron & 0.0006650709082892396 & 8\n",
      "outfitter & 0.00016727581512620678 & 8\n",
      "southern & 0.00031405596696140975 & 8\n",
      "upbeat & 0.00024273971489285095 & 7\n",
      "call & 0.0011045830623378134 & 7\n",
      "surge & 0.0003426672410245005 & 7\n",
      "attract & 8.817591731470758e-05 & 7\n",
      "dish & 0.0003093576277275972 & 7\n",
      "miss & -0.0014806143858688572 & 19\n",
      "lower & -0.016773040274824097 & 19\n",
      "loser & -0.016850640304565363 & 19\n",
      "cut & -0.0030349015078132793 & 19\n",
      "fall & -0.00434480314447805 & 19\n",
      "weak & -0.0011911663868944653 & 19\n",
      "downgrade & -0.02394601163514049 & 19\n",
      "underweight & -0.0007514117061003177 & 19\n",
      "low & -0.0052906587357928045 & 17\n",
      "public & -0.0006085351200168031 & 17\n",
      "neutral & -0.003326602027110995 & 16\n",
      "negative & -0.000793897472944129 & 16\n",
      "offer & -0.002485940731159663 & 16\n",
      "concern & -0.0004845077493578288 & 15\n",
      "disappoint & -0.0007594257649109188 & 15\n",
      "common & -0.0007308898079254235 & 15\n",
      "impact & -0.0004575948072578866 & 14\n",
      "remove & -0.0005932299930084661 & 14\n",
      "loss & -0.000699342582110352 & 14\n",
      "dip & -0.0004777271891495123 & 13\n",
      "tumble & -0.0006777210087775024 & 13\n",
      "resign & -0.0005338513752769846 & 13\n",
      "pressure & -0.00046926599218579757 & 12\n",
      "resume & -0.0006614837631682477 & 12\n",
      "drop & -0.0007403419224690289 & 12\n",
      "shelf & -0.0002703441952607576 & 12\n",
      "worst & -0.0029479077902279526 & 11\n",
      "secondary & -0.0001935989722249246 & 11\n",
      "sell & -0.0009609179109561998 & 11\n",
      "plunge & -0.0004210097188377235 & 10\n",
      "lose & -7.872748429673167e-05 & 10\n",
      "fitch & -0.0008130444874792471 & 10\n",
      "adobe & -0.001184700915108884 & 10\n",
      "perform & -0.001148220705412375 & 10\n",
      "valuation & -0.0001434158845285705 & 10\n",
      "warn & -0.0003523127434089621 & 9\n",
      "four & -3.063150608488561e-05 & 9\n",
      "beyond & -0.00023047115795111532 & 9\n",
      "halt & -0.00022542535214161339 & 9\n",
      "laboratory & -0.0003606842960839103 & 9\n",
      "prelim & -0.0001200566667019826 & 9\n",
      "downbeat & -0.0002900250882640283 & 9\n",
      "delay & -8.179550189474901e-05 & 8\n",
      "loan & -5.137715666647771e-05 & 8\n",
      "downside & -0.000128585443036064 & 8\n",
      "bath & -0.00034257510948296514 & 8\n",
      "gap & -0.0009138159491380915 & 8\n",
      "paper & -0.00023206303182938997 & 8\n",
      "price & -0.0004844470186778 & 8\n",
      "build & -3.128584585839264e-05 & 8\n",
      "Top 100 sentiment positive words\n",
      "[('upgrade', 0.016794951772062014), ('gainer', 0.013524412572822343), ('rais', 0.011851335286498023), ('high', 0.010033554431779335), ('buy', 0.008343607042205062), ('mover', 0.007526084353512696), ('higher', 0.006567234447939277), ('outperform', 0.0028570636749356837)]\n",
      "Top 100 sentiment negative words\n",
      "[('downgrade', -0.02394601163514049), ('loser', -0.016850640304565363), ('lower', -0.016773040274824097), ('low', -0.0052906587357928045), ('fall', -0.00434480314447805), ('neutral', -0.003326602027110995), ('cut', -0.0030349015078132793), ('worst', -0.0029479077902279526)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_top_words(data, n=2, order=False, reverse=True):\n",
    "    \"\"\"Get top n words by tone. \n",
    "\n",
    "    Returns a dictionary or an `OrderedDict` if `order` is true.\n",
    "    \"\"\" \n",
    "    top = sorted(data.items(), key=lambda x: x[1][''], reverse=reverse)[:n]\n",
    "    if order:\n",
    "        return OrderedDict(top)\n",
    "    return dict(top)\n",
    "\n",
    "pathlist = Path('./data/models/stemming/word-lists/').rglob('*.csv')\n",
    "word_list_tone = {}\n",
    "word_list_count = {}\n",
    "for path in pathlist:\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        line = 0\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if line > 0:\n",
    "                if (row[0] in word_list_tone):\n",
    "                    word_list_tone[row[0]] += (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] += 1\n",
    "                else:\n",
    "                    word_list_tone[row[0]] = (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] = 1\n",
    "            line += 1\n",
    "\n",
    "word_list_count = {k: v for k, v in sorted(word_list_count.items(), reverse = True, key=lambda item: item[1])}\n",
    "word_list_tone = {k: v for k, v in sorted(word_list_tone.items(), key=lambda item: item[1])}\n",
    "for w in word_list_tone:\n",
    "    word_list_tone[w] /= 19\n",
    "# print(word_list_count)\n",
    "\n",
    "high_word_counts    = [w for w in word_list_count if word_list_count[w] > 14]\n",
    "# for w in high_word_counts:\n",
    "#     print(w, word_list_tone[w], word_list_count[w])\n",
    "pos_word_list_count = [w for w in word_list_count if word_list_tone[w] > 0][:50]\n",
    "neg_word_list_count = [w for w in word_list_count if word_list_tone[w] < 0][:50]\n",
    "for w in pos_word_list_count:\n",
    "    # in_lm = 0\n",
    "    # in_h4 = 0\n",
    "    # if w in positive_words_lm:\n",
    "    #     in_lm = 1\n",
    "    # if w in positive_words_h4:\n",
    "    #     in_h4 = 1\n",
    "    print(w + \" & \" + str(word_list_tone[w]) + \" & \" + str(word_list_count[w]))\n",
    "for w in neg_word_list_count:\n",
    "    print(w + \" & \" + str(word_list_tone[w]) + \" & \" + str(word_list_count[w]))\n",
    "\n",
    "print(\"Top 100 sentiment positive words\")\n",
    "# pos_words = dict(sorted(word_list_tone.items(), key = itemgetter(1), reverse = True)[:100])\n",
    "pos_words = dict(sorted(word_list_tone.items(), key = itemgetter(1), reverse = True)[:100])\n",
    "print(sorted(word_list_tone.items(), key = itemgetter(1), reverse = True)[:8])\n",
    "pos_words = {w: pos_words[w] for w in pos_words if pos_words[w] > 0}\n",
    "# wordcloud = WordCloud(width=800, height=400,colormap='summer', collocations=True,prefer_horizontal=1, background_color='black')\n",
    "# wordcloud.generate_from_frequencies(frequencies = pos_words)\n",
    "# plt.figure(figsize=(15,12))\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Top 100 sentiment negative words\")\n",
    "#pos_words = dict(sorted(word_list_tone.items(), key = itemgetter(1), reverse = True)[:100])\n",
    "neg_words = dict(sorted(word_list_tone.items(), key = itemgetter(1), reverse = False)[:100])\n",
    "print(sorted(word_list_tone.items(), key = itemgetter(1), reverse = False)[:8])\n",
    "neg_words = {w: neg_words[w] for w in neg_words if neg_words[w] < 0}\n",
    "for w in neg_words:\n",
    "    neg_words[w] = abs(neg_words[w])\n",
    "# wordcloud = WordCloud(background_color='white', colormap='forest',prefer_horizontal=1)\n",
    "# wordcloud = WordCloud(width=800, height=400,colormap='autumn', collocations=True,prefer_horizontal=1, background_color='black', max_words=100)\n",
    "# wordcloud.generate_from_frequencies(frequencies = neg_words)\n",
    "# plt.figure(figsize=(15,12))\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of sample Modelling\n",
    "Ok so we have trained the model and visualised the data. Let's now build some portfolios from the out of sample data and compare the success of the model against some other popular dictionaries.\n",
    "\n",
    "We will construct the sentiment score for these articles by aggregating counts of words listed in their positive sentiment dictionary (weighted tf-idf by LM)  and subtracting weighted counts of words in their negative dictionaries. For our SESTM, we average scores from multiple articles for the same firm in the same day.\n",
    "\n",
    "What I think they mean (and this might be wrong) is that they will take the best 50 sentiment stocks and long them, and vice versa for the 50 words sentiment stocks for that say (short them). At the start of the next trading day, they 'liquidate' the stocks, and evaluate whether they made money or not, and then this is stored. This is done for both lexicon methods.\n",
    "\n",
    "There are a few strategies that will be used to evaluate the methods:\n",
    "- Sharpe ratio, which compares the success over the volatility of the stocks\n",
    "- Potential Granger causality to prove that the headlines ARE actually the cause and it's not a fluke\n",
    "\n",
    "## Fama-French models\n",
    "The Fama-French three-factor model is a statistical model that describes stock returns. They explain phenomena using a small number of underlying causes or factors, and looks like this:\n",
    "$$R_a = R_f + \\beta_1(R_m - R_f)+ \\beta_2 \\cdot SMB + \\beta_3 \\cdot HML + \\alpha$$\n",
    "where:\n",
    "- $R_a$: expected return on asset\n",
    "- $R_f$: Risk free rate\n",
    "- $\\beta_{1,2,3}$: Factor coefficient\n",
    "- $R_m - R_f$: Market risk premium\n",
    "- $SMB$: (small minus big): Excess returns of small cap over large cap\n",
    "- $HML$: (high minus low): Excess returns of value stocks over growth stocks\n",
    "- $\\alpha$: intercept\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following if `kaggle/processed-data-raw/` doesn't contain a list of json files with stock data for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect out of sample articles\n",
    "oos_start_year = 2018\n",
    "oos_start_month = 5\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,pytz.est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "\n",
    "# collect stock info\n",
    "list_tickers = [a['ticker'] for a in oos_arts]\n",
    "list_tickers = list(dict.fromkeys(list_tickers))\n",
    "stock_data = {}\n",
    "failed_stocks = []\n",
    "print('pulling stocks...')\n",
    "# data = yf.download(tickers = list_tickers, end=str(end_date.date()), start=str(start_date.date()), progress=True)\n",
    "curr_index = 0\n",
    "TOTAL_TICKERS = len(list_tickers)\n",
    "for t in list_tickers:\n",
    "    arts_ticker = [a['date'] for a in article_list if a['ticker'] == t]\n",
    "    # print(type(arts_ticker[0]))\n",
    "    end_date_stock_data = max(arts_ticker) + dt.timedelta(days=5)\n",
    "    start_date_stock_data = min(arts_ticker) - dt.timedelta(days=5)\n",
    "    try:\n",
    "        data = yf.download(tickers = t, end=str(end_date_stock_data.date()), start=str(start_date_stock_data.date()), progress=False, show_errors=False)\n",
    "        if len(data > 0):\n",
    "            stock_data[t] = data\n",
    "        else:\n",
    "            failed_stocks.append(t)\n",
    "    except:\n",
    "        failed_stocks.append(t)\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_TICKERS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d (%d)\" % ('='*int(20*j), 100*j, curr_index, TOTAL_TICKERS, len(failed_stocks)))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save stocks to a file (so you dont have to do that again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save stocks to file\n",
    "for s in stock_data:\n",
    "    with open('./processed-data-raw/' + s + '.json', 'w') as json_file:\n",
    "        json.dump(stock_data[s].to_json(), json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling file no 3363"
     ]
    }
   ],
   "source": [
    "# load stocks from file\n",
    "\n",
    "ticker_path = './processed-data-raw/'\n",
    "# recreate list of stock information\n",
    "pathlist = Path(ticker_path).rglob('*.json')\n",
    "stock_data = {}\n",
    "i = 0\n",
    "for path in pathlist:\n",
    "    with open(str(path)) as json_file:\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"pulling file no %d\" % (i))\n",
    "        sys.stdout.flush()#\n",
    "        i += 1\n",
    "        data = json.load(json_file)\n",
    "        data = json.loads(data)\n",
    "        datetime_dict = {}\n",
    "        for line in data:\n",
    "            datetime_line = {}\n",
    "            for k in data[line]:\n",
    "                # print(str(datetime.fromtimestamp(float(k)/1000.0)))\n",
    "                datetime_line[str(datetime.fromtimestamp(float(k)/1000.0).date())] = data[line][k]\n",
    "            datetime_dict[line] = datetime_line \n",
    "        stock_data[os.path.basename(str(path))[:-5]] = DataFrame.from_dict(datetime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to generate a list of possible dates (market days, essentially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02 09:00:00-04:56\n",
      "2019-01-03 09:00:00-04:56\n",
      "2019-01-04 09:00:00-04:56\n",
      "2019-01-07 09:00:00-04:56\n",
      "2019-01-08 09:00:00-04:56\n",
      "2019-01-09 09:00:00-04:56\n",
      "2019-01-10 09:00:00-04:56\n",
      "2019-01-11 09:00:00-04:56\n",
      "2019-01-14 09:00:00-04:56\n",
      "2019-01-15 09:00:00-04:56\n",
      "2019-01-16 09:00:00-04:56\n",
      "2019-01-17 09:00:00-04:56\n",
      "2019-01-18 09:00:00-04:56\n",
      "2019-01-22 09:00:00-04:56\n",
      "2019-01-23 09:00:00-04:56\n",
      "2019-01-24 09:00:00-04:56\n",
      "2019-01-25 09:00:00-04:56\n",
      "2019-01-28 09:00:00-04:56\n",
      "2019-01-29 09:00:00-04:56\n",
      "2019-01-30 09:00:00-04:56\n",
      "2019-01-31 09:00:00-04:56\n",
      "2019-02-01 09:00:00-04:56\n",
      "2019-02-04 09:00:00-04:56\n",
      "2019-02-05 09:00:00-04:56\n",
      "2019-02-06 09:00:00-04:56\n",
      "2019-02-07 09:00:00-04:56\n",
      "2019-02-08 09:00:00-04:56\n",
      "2019-02-11 09:00:00-04:56\n",
      "2019-02-12 09:00:00-04:56\n",
      "2019-02-13 09:00:00-04:56\n",
      "2019-02-14 09:00:00-04:56\n",
      "2019-02-15 09:00:00-04:56\n",
      "2019-02-19 09:00:00-04:56\n",
      "2019-02-20 09:00:00-04:56\n",
      "2019-02-21 09:00:00-04:56\n",
      "2019-02-22 09:00:00-04:56\n",
      "2019-02-25 09:00:00-04:56\n",
      "2019-02-26 09:00:00-04:56\n",
      "2019-02-27 09:00:00-04:56\n",
      "2019-02-28 09:00:00-04:56\n",
      "2019-03-01 09:00:00-04:56\n",
      "2019-03-04 09:00:00-04:56\n",
      "2019-03-05 09:00:00-04:56\n",
      "2019-03-06 09:00:00-04:56\n",
      "2019-03-07 09:00:00-04:56\n",
      "2019-03-08 09:00:00-04:56\n",
      "2019-03-11 09:00:00-04:56\n",
      "2019-03-12 09:00:00-04:56\n",
      "2019-03-13 09:00:00-04:56\n",
      "2019-03-14 09:00:00-04:56\n",
      "2019-03-15 09:00:00-04:56\n",
      "2019-03-18 09:00:00-04:56\n",
      "2019-03-19 09:00:00-04:56\n",
      "2019-03-20 09:00:00-04:56\n",
      "2019-03-21 09:00:00-04:56\n",
      "2019-03-22 09:00:00-04:56\n",
      "2019-03-25 09:00:00-04:56\n",
      "2019-03-26 09:00:00-04:56\n",
      "2019-03-27 09:00:00-04:56\n",
      "2019-03-28 09:00:00-04:56\n",
      "2019-03-29 09:00:00-04:56\n",
      "2019-04-01 09:00:00-04:56\n",
      "2019-04-02 09:00:00-04:56\n",
      "2019-04-03 09:00:00-04:56\n",
      "2019-04-04 09:00:00-04:56\n",
      "2019-04-05 09:00:00-04:56\n",
      "2019-04-08 09:00:00-04:56\n",
      "2019-04-09 09:00:00-04:56\n",
      "2019-04-10 09:00:00-04:56\n",
      "2019-04-11 09:00:00-04:56\n",
      "2019-04-12 09:00:00-04:56\n",
      "2019-04-15 09:00:00-04:56\n",
      "2019-04-16 09:00:00-04:56\n",
      "2019-04-17 09:00:00-04:56\n",
      "2019-04-18 09:00:00-04:56\n",
      "2019-04-22 09:00:00-04:56\n",
      "2019-04-23 09:00:00-04:56\n",
      "2019-04-24 09:00:00-04:56\n",
      "2019-04-25 09:00:00-04:56\n",
      "2019-04-26 09:00:00-04:56\n",
      "2019-04-29 09:00:00-04:56\n",
      "2019-04-30 09:00:00-04:56\n",
      "2019-05-01 09:00:00-04:56\n",
      "2019-05-02 09:00:00-04:56\n",
      "2019-05-03 09:00:00-04:56\n",
      "2019-05-06 09:00:00-04:56\n",
      "2019-05-07 09:00:00-04:56\n",
      "2019-05-08 09:00:00-04:56\n",
      "2019-05-09 09:00:00-04:56\n",
      "2019-05-10 09:00:00-04:56\n",
      "2019-05-13 09:00:00-04:56\n",
      "2019-05-14 09:00:00-04:56\n",
      "2019-05-15 09:00:00-04:56\n",
      "2019-05-16 09:00:00-04:56\n",
      "2019-05-17 09:00:00-04:56\n",
      "2019-05-20 09:00:00-04:56\n",
      "2019-05-21 09:00:00-04:56\n",
      "2019-05-22 09:00:00-04:56\n",
      "2019-05-23 09:00:00-04:56\n",
      "2019-05-24 09:00:00-04:56\n",
      "2019-05-28 09:00:00-04:56\n",
      "2019-05-29 09:00:00-04:56\n",
      "2019-05-30 09:00:00-04:56\n",
      "2019-05-31 09:00:00-04:56\n",
      "2019-06-03 09:00:00-04:56\n",
      "2019-06-04 09:00:00-04:56\n",
      "2019-06-05 09:00:00-04:56\n",
      "2019-06-06 09:00:00-04:56\n",
      "2019-06-07 09:00:00-04:56\n",
      "2019-06-10 09:00:00-04:56\n",
      "2019-06-11 09:00:00-04:56\n",
      "2019-06-12 09:00:00-04:56\n",
      "2019-06-13 09:00:00-04:56\n",
      "2019-06-14 09:00:00-04:56\n",
      "2019-06-17 09:00:00-04:56\n",
      "2019-06-18 09:00:00-04:56\n",
      "2019-06-19 09:00:00-04:56\n",
      "2019-06-20 09:00:00-04:56\n",
      "2019-06-21 09:00:00-04:56\n",
      "2019-06-24 09:00:00-04:56\n",
      "2019-06-25 09:00:00-04:56\n",
      "2019-06-26 09:00:00-04:56\n",
      "2019-06-27 09:00:00-04:56\n",
      "2019-06-28 09:00:00-04:56\n",
      "2019-07-01 09:00:00-04:56\n",
      "2019-07-02 09:00:00-04:56\n",
      "2019-07-03 09:00:00-04:56\n",
      "2019-07-05 09:00:00-04:56\n",
      "2019-07-08 09:00:00-04:56\n",
      "2019-07-09 09:00:00-04:56\n",
      "2019-07-10 09:00:00-04:56\n",
      "2019-07-11 09:00:00-04:56\n",
      "2019-07-12 09:00:00-04:56\n",
      "2019-07-15 09:00:00-04:56\n",
      "2019-07-16 09:00:00-04:56\n",
      "2019-07-17 09:00:00-04:56\n",
      "2019-07-18 09:00:00-04:56\n",
      "2019-07-19 09:00:00-04:56\n",
      "2019-07-22 09:00:00-04:56\n",
      "2019-07-23 09:00:00-04:56\n",
      "2019-07-24 09:00:00-04:56\n",
      "2019-07-25 09:00:00-04:56\n",
      "2019-07-26 09:00:00-04:56\n",
      "2019-07-29 09:00:00-04:56\n",
      "2019-07-30 09:00:00-04:56\n",
      "2019-07-31 09:00:00-04:56\n",
      "2019-08-01 09:00:00-04:56\n",
      "2019-08-02 09:00:00-04:56\n",
      "2019-08-05 09:00:00-04:56\n",
      "2019-08-06 09:00:00-04:56\n",
      "2019-08-07 09:00:00-04:56\n",
      "2019-08-08 09:00:00-04:56\n",
      "2019-08-09 09:00:00-04:56\n",
      "2019-08-12 09:00:00-04:56\n",
      "2019-08-13 09:00:00-04:56\n",
      "2019-08-14 09:00:00-04:56\n",
      "2019-08-15 09:00:00-04:56\n",
      "2019-08-16 09:00:00-04:56\n",
      "2019-08-19 09:00:00-04:56\n",
      "2019-08-20 09:00:00-04:56\n",
      "2019-08-21 09:00:00-04:56\n",
      "2019-08-22 09:00:00-04:56\n",
      "2019-08-23 09:00:00-04:56\n",
      "2019-08-26 09:00:00-04:56\n",
      "2019-08-27 09:00:00-04:56\n",
      "2019-08-28 09:00:00-04:56\n",
      "2019-08-29 09:00:00-04:56\n",
      "2019-08-30 09:00:00-04:56\n",
      "2019-09-03 09:00:00-04:56\n",
      "2019-09-04 09:00:00-04:56\n",
      "2019-09-05 09:00:00-04:56\n",
      "2019-09-06 09:00:00-04:56\n",
      "2019-09-09 09:00:00-04:56\n",
      "2019-09-10 09:00:00-04:56\n",
      "2019-09-11 09:00:00-04:56\n",
      "2019-09-12 09:00:00-04:56\n",
      "2019-09-13 09:00:00-04:56\n",
      "2019-09-16 09:00:00-04:56\n",
      "2019-09-17 09:00:00-04:56\n",
      "2019-09-18 09:00:00-04:56\n",
      "2019-09-19 09:00:00-04:56\n",
      "2019-09-20 09:00:00-04:56\n",
      "2019-09-23 09:00:00-04:56\n",
      "2019-09-24 09:00:00-04:56\n",
      "2019-09-25 09:00:00-04:56\n",
      "2019-09-26 09:00:00-04:56\n",
      "2019-09-27 09:00:00-04:56\n",
      "2019-09-30 09:00:00-04:56\n",
      "2019-10-01 09:00:00-04:56\n",
      "2019-10-02 09:00:00-04:56\n",
      "2019-10-03 09:00:00-04:56\n",
      "2019-10-04 09:00:00-04:56\n",
      "2019-10-07 09:00:00-04:56\n",
      "2019-10-08 09:00:00-04:56\n",
      "2019-10-09 09:00:00-04:56\n",
      "2019-10-10 09:00:00-04:56\n",
      "2019-10-11 09:00:00-04:56\n",
      "2019-10-14 09:00:00-04:56\n",
      "2019-10-15 09:00:00-04:56\n",
      "2019-10-16 09:00:00-04:56\n",
      "2019-10-17 09:00:00-04:56\n",
      "2019-10-18 09:00:00-04:56\n",
      "2019-10-21 09:00:00-04:56\n",
      "2019-10-22 09:00:00-04:56\n",
      "2019-10-23 09:00:00-04:56\n",
      "2019-10-24 09:00:00-04:56\n",
      "2019-10-25 09:00:00-04:56\n",
      "2019-10-28 09:00:00-04:56\n",
      "2019-10-29 09:00:00-04:56\n",
      "2019-10-30 09:00:00-04:56\n",
      "2019-10-31 09:00:00-04:56\n",
      "2019-11-01 09:00:00-04:56\n",
      "2019-11-04 09:00:00-04:56\n",
      "2019-11-05 09:00:00-04:56\n",
      "2019-11-06 09:00:00-04:56\n",
      "2019-11-07 09:00:00-04:56\n",
      "2019-11-08 09:00:00-04:56\n",
      "2019-11-11 09:00:00-04:56\n",
      "2019-11-12 09:00:00-04:56\n",
      "2019-11-13 09:00:00-04:56\n",
      "2019-11-14 09:00:00-04:56\n",
      "2019-11-15 09:00:00-04:56\n",
      "2019-11-18 09:00:00-04:56\n",
      "2019-11-19 09:00:00-04:56\n",
      "2019-11-20 09:00:00-04:56\n",
      "2019-11-21 09:00:00-04:56\n",
      "2019-11-22 09:00:00-04:56\n",
      "2019-11-25 09:00:00-04:56\n",
      "2019-11-26 09:00:00-04:56\n",
      "2019-11-27 09:00:00-04:56\n",
      "2019-11-29 09:00:00-04:56\n",
      "2019-12-02 09:00:00-04:56\n",
      "2019-12-03 09:00:00-04:56\n",
      "2019-12-04 09:00:00-04:56\n",
      "2019-12-05 09:00:00-04:56\n",
      "2019-12-06 09:00:00-04:56\n",
      "2019-12-09 09:00:00-04:56\n",
      "2019-12-10 09:00:00-04:56\n",
      "2019-12-11 09:00:00-04:56\n",
      "2019-12-12 09:00:00-04:56\n",
      "2019-12-13 09:00:00-04:56\n",
      "2019-12-16 09:00:00-04:56\n",
      "2019-12-17 09:00:00-04:56\n",
      "2019-12-18 09:00:00-04:56\n",
      "2019-12-19 09:00:00-04:56\n",
      "2019-12-20 09:00:00-04:56\n",
      "2019-12-23 09:00:00-04:56\n",
      "2019-12-24 09:00:00-04:56\n",
      "2019-12-26 09:00:00-04:56\n",
      "2019-12-27 09:00:00-04:56\n",
      "2019-12-30 09:00:00-04:56\n",
      "2019-12-31 09:00:00-04:56\n",
      "2020-01-02 09:00:00-04:56\n",
      "2020-01-03 09:00:00-04:56\n",
      "2020-01-06 09:00:00-04:56\n",
      "2020-01-07 09:00:00-04:56\n",
      "2020-01-08 09:00:00-04:56\n",
      "2020-01-09 09:00:00-04:56\n",
      "2020-01-10 09:00:00-04:56\n",
      "2020-01-13 09:00:00-04:56\n",
      "2020-01-14 09:00:00-04:56\n",
      "2020-01-15 09:00:00-04:56\n",
      "2020-01-16 09:00:00-04:56\n",
      "2020-01-17 09:00:00-04:56\n",
      "2020-01-21 09:00:00-04:56\n",
      "2020-01-22 09:00:00-04:56\n",
      "2020-01-23 09:00:00-04:56\n",
      "2020-01-24 09:00:00-04:56\n",
      "2020-01-27 09:00:00-04:56\n",
      "2020-01-28 09:00:00-04:56\n",
      "2020-01-29 09:00:00-04:56\n",
      "2020-01-30 09:00:00-04:56\n",
      "2020-01-31 09:00:00-04:56\n",
      "2020-02-03 09:00:00-04:56\n",
      "2020-02-04 09:00:00-04:56\n",
      "2020-02-05 09:00:00-04:56\n",
      "2020-02-06 09:00:00-04:56\n",
      "2020-02-07 09:00:00-04:56\n",
      "2020-02-10 09:00:00-04:56\n",
      "2020-02-11 09:00:00-04:56\n",
      "2020-02-12 09:00:00-04:56\n",
      "2020-02-13 09:00:00-04:56\n",
      "2020-02-14 09:00:00-04:56\n",
      "2020-02-18 09:00:00-04:56\n",
      "2020-02-19 09:00:00-04:56\n",
      "2020-02-20 09:00:00-04:56\n",
      "2020-02-21 09:00:00-04:56\n",
      "2020-02-24 09:00:00-04:56\n",
      "2020-02-25 09:00:00-04:56\n",
      "2020-02-26 09:00:00-04:56\n",
      "2020-02-27 09:00:00-04:56\n",
      "2020-02-28 09:00:00-04:56\n",
      "2020-03-02 09:00:00-04:56\n",
      "2020-03-03 09:00:00-04:56\n",
      "2020-03-04 09:00:00-04:56\n",
      "2020-03-05 09:00:00-04:56\n",
      "2020-03-06 09:00:00-04:56\n",
      "2020-03-09 09:00:00-04:56\n",
      "2020-03-10 09:00:00-04:56\n",
      "2020-03-11 09:00:00-04:56\n",
      "2020-03-12 09:00:00-04:56\n",
      "2020-03-13 09:00:00-04:56\n",
      "2020-03-16 09:00:00-04:56\n",
      "2020-03-17 09:00:00-04:56\n",
      "2020-03-18 09:00:00-04:56\n",
      "2020-03-19 09:00:00-04:56\n",
      "2020-03-20 09:00:00-04:56\n",
      "2020-03-23 09:00:00-04:56\n",
      "2020-03-24 09:00:00-04:56\n",
      "2020-03-25 09:00:00-04:56\n",
      "2020-03-26 09:00:00-04:56\n",
      "2020-03-27 09:00:00-04:56\n",
      "2020-03-30 09:00:00-04:56\n",
      "2020-03-31 09:00:00-04:56\n",
      "2020-04-01 09:00:00-04:56\n",
      "2020-04-02 09:00:00-04:56\n",
      "2020-04-03 09:00:00-04:56\n",
      "2020-04-06 09:00:00-04:56\n",
      "2020-04-07 09:00:00-04:56\n",
      "2020-04-08 09:00:00-04:56\n",
      "2020-04-09 09:00:00-04:56\n",
      "2020-04-13 09:00:00-04:56\n",
      "2020-04-14 09:00:00-04:56\n",
      "2020-04-15 09:00:00-04:56\n",
      "2020-04-16 09:00:00-04:56\n",
      "2020-04-17 09:00:00-04:56\n",
      "2020-04-20 09:00:00-04:56\n",
      "2020-04-21 09:00:00-04:56\n",
      "2020-04-22 09:00:00-04:56\n",
      "2020-04-23 09:00:00-04:56\n",
      "2020-04-24 09:00:00-04:56\n",
      "2020-04-27 09:00:00-04:56\n",
      "2020-04-28 09:00:00-04:56\n",
      "2020-04-29 09:00:00-04:56\n",
      "2020-04-30 09:00:00-04:56\n",
      "2020-05-01 09:00:00-04:56\n",
      "2020-05-04 09:00:00-04:56\n",
      "2020-05-05 09:00:00-04:56\n",
      "2020-05-06 09:00:00-04:56\n",
      "2020-05-07 09:00:00-04:56\n",
      "2020-05-08 09:00:00-04:56\n",
      "2020-05-11 09:00:00-04:56\n",
      "2020-05-12 09:00:00-04:56\n",
      "2020-05-13 09:00:00-04:56\n",
      "2020-05-14 09:00:00-04:56\n",
      "2020-05-15 09:00:00-04:56\n",
      "2020-05-18 09:00:00-04:56\n",
      "2020-05-19 09:00:00-04:56\n",
      "2020-05-20 09:00:00-04:56\n",
      "2020-05-21 09:00:00-04:56\n",
      "2020-05-22 09:00:00-04:56\n",
      "2020-05-26 09:00:00-04:56\n",
      "2020-05-27 09:00:00-04:56\n",
      "2020-05-28 09:00:00-04:56\n",
      "2020-05-29 09:00:00-04:56\n",
      "2020-06-01 09:00:00-04:56\n",
      "2020-06-02 09:00:00-04:56\n",
      "2020-06-03 09:00:00-04:56\n",
      "2020-06-04 09:00:00-04:56\n",
      "2020-06-05 09:00:00-04:56\n",
      "2020-06-08 09:00:00-04:56\n",
      "2020-06-09 09:00:00-04:56\n"
     ]
    }
   ],
   "source": [
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0)\n",
    "# add the eastern timezone so can compare against timezone sensitive data\n",
    "possible_dates = [datetime(datetime.strptime(d, '%Y-%m-%d').year, datetime.strptime(d, '%Y-%m-%d').month, datetime.strptime(d, '%Y-%m-%d').day, 9, 0, 0, 0, est) for d in stock_data['A']['Open'].keys() if datetime.strptime(d, '%Y-%m-%d') > start_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal weighted strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n",
      "Prev days: -1\n",
      "Prev days: 0\n",
      "Prev days: 1\n",
      "Prev days: 2\n",
      "Prev days: 3\n",
      "Prev days: 4\n",
      "Prev days: 5\n",
      "Prev days: 6\n",
      "Prev days: 7\n",
      "Prev days: 8\n",
      "New folder ./data/out-of-sample/2013-05-01-ew-day+8 created\n",
      "Prev days: 9\n",
      "New folder ./data/out-of-sample/2013-05-01-ew-day+9 created\n",
      "Prev days: 10\n",
      "New folder ./data/out-of-sample/2013-05-01-ew-day+10 created\n"
     ]
    }
   ],
   "source": [
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "prev_days = 7\n",
    "best_config_file = './data/models/stemming/word-lists/2013-5-1.csv'\n",
    "sentiment_words = []\n",
    "O = np.array([0,0])\n",
    "with open(best_config_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "O = O[1:]\n",
    "# for i in range(len(sentiment_words)):\n",
    "#     print(sentiment_words[i] + \": \" + str(O[i][0]) + \"//\" + str(O[i][1]))\n",
    "for prev_days in range(-1,11):\n",
    "    print('Prev days: ' + str(prev_days))\n",
    "    # Collect out of sample articles\n",
    "\n",
    "    trial_id = '2013-05-01-ew-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    curr_date = possible_dates[min(prev_days,0)]\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('long value'), str('EARNING LONG'), str('short value'), str('EARNING SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    portfolio_value = 10000\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        # article_date_1 = article_date + dt.timedelta(days=1)\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        # total_earnings_long = 0\n",
    "        # total_earnings_short = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_top = sum([prev_top[t] for t in prev_top])\n",
    "                for tick in prev_top:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        investment = portfolio_value*(1/len(prev_top))\n",
    "                        # investment = portfolio_value*(prev_top[tick]/total_sum_top)\n",
    "                        earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                        investment_long += investment\n",
    "                        # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                        # csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_bot = sum([prev_bot[t] for t in prev_bot])\n",
    "                for tick in prev_bot:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                        investment = portfolio_value*(1/len(prev_bot))\n",
    "                        # investment = portfolio_value*(prev_bot[tick]/total_sum_bot)\n",
    "                        earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                        investment_short += investment\n",
    "                        # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_d.append(oos_bow)\n",
    "            \n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0.5\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                testing_s = sum(oos_bow.get(w,0) for w in sentiment_words)\n",
    "                if (testing_s > 0):\n",
    "                    est_p = fminbound(equation_to_solve, 0, 1, (O,oos_bow, sentiment_words,testing_s,LAM))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            # ensure the portfolio can't buy the same stock for both long and short (because that's SILLY)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items() if val > 0.5}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items() if val < 0.5}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((curr_date).date()), str(investment_long), str(earning_long), str(investment_short), str(earning_short), str(len(prev_top)), str(len(prev_bot)), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value weighted strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n",
      "neutral: 0.028112281337826537//0.03788081623962594\n",
      "upgrade: 0.08477955395321501//0.04041305299806427\n",
      "buy: 0.10101603882844185//0.0594780310378326\n",
      "jack: 0.0003082304464546281//0.0023568229914368724\n",
      "loser: 0.008666302582382482//0.056943556447852525\n",
      "miss: 0.004744626722759458//0.006615791034511048\n",
      "rais: 0.08854163745835934//0.05883520740477709\n",
      "replace: 0.001778222328543531//0.0014911273645555122\n",
      "proceed: 0.003287108729308542//0.00237986508127797\n",
      "nuclear: 0.00043207860476657847//0.0014342035951465642\n",
      "lower: 0.032457915999656264//0.0706361876516049\n",
      "offer: 0.029806144051811184//0.034763231398137875\n",
      "pressure: 0.0003931779221157448//0.00455028696440091\n",
      "bill: 0.002868935768281278//0.0013894763442133659\n",
      "volume: 0.011972706794278903//0.0037549127844452297\n",
      "improving: 0.0009499260719184737//0.0011256903173662727\n",
      "strength: 0.0017213619950899905//0.0021744584240629495\n",
      "fall: 0.010082377133565011//0.020304269566622145\n",
      "overweight: 0.014872517278038873//0.007948402539532653\n",
      "gainer: 0.03504081486760051//0.00696355378821905\n",
      "downgrade: 0.04945004608405729//0.10344254318516766\n",
      "valuation: 0.003011436235510071//0.003654066865390233\n",
      "strong: 0.010367942313143759//0.011387623193723849\n",
      "mover: 0.04093099239290451//0.010090289118023957\n",
      "high: 0.04293441225786055//0.015826675445054575\n",
      "solid: 0.004006311740292073//0.002230609607758601\n",
      "mention: 0.0016414841606966347//0.001263365151121388\n",
      "mix: 0.009068722703462767//0.012066481146403773\n",
      "cut: 0.0034172605916972854//0.012953731744269002\n",
      "resume: 0.0073831373078936825//0.009176982289450021\n",
      "higher: 0.04136116479995132//0.030622743728353267\n",
      "atlantic: 0.0013067386607433214//0.0009867306753593274\n",
      "date: 0.002346019773962963//0.0024866393239452366\n",
      "outperform: 0.03420076989170911//0.02280505065154523\n",
      "prelim: 0.0027763894470841173//0.0031965190257173858\n",
      "worst: 0.0010654932606377128//0.014263141632045489\n",
      "public: 0.0041737188898335964//0.004196621074721198\n",
      "build: 0.0037120919247681915//0.003334019642118421\n",
      "material: 0.005402218135734719//0.0023007659205591755\n",
      "downbeat: 0.003711146295512687//0.0056976112069720645\n",
      "concern: 0.002480725837150721//0.004752873807314765\n",
      "fitch: 0.004125788963429101//0.0041922405213955785\n",
      "attract: 0.0023837124767245275//0.0020874068282173562\n",
      "spike: 0.013376175906542711//0.007111615055713191\n",
      "lift: 0.00329164105811321//0.0027102628484573163\n",
      "underweight: 0.001136008349222452//0.002893437740401096\n",
      "low: 0.008461547885688295//0.028330316101213997\n",
      "placement: 0.001455061939512035//0.0005211735238679834\n",
      "peer: 0.000743886516947397//0.001091837949531669\n",
      "remove: 0.0027102568420225705//0.0044718239961064115\n",
      "feel: 0.0007164394030303529//0.001261367726917486\n",
      "uncertainty: 0.0007773352111593724//0.002266043914499146\n",
      "clearance: 0.0010048896815599982//0.0014240835914293756\n",
      "wont: 0.0010230568843169884//0.0015174599094751819\n",
      "plummet: 0.0005193542689827308//0.0015837270920872576\n",
      "chronic: 0.0002886109465098539//0.0017915190768039856\n",
      "accept: 0.0028828370014741247//0.0018867355155922055\n",
      "shire: 0.005682005140661859//0.0\n",
      "interim: 0.0014512183843035126//0.001997155157977653\n",
      "loss: 0.007810452765257092//0.014624176210275964\n",
      "common: 0.0041912603695770725//0.005775716503954091\n",
      "shelf: 0.0027559095210093407//0.003795605786547786\n",
      "repurchase: 0.0035670349861128886//0.0028164103909301716\n",
      "cite: 0.0012086850506775786//0.0017453104330265724\n",
      "downside: 0.0008648764604100341//0.0017974501952559415\n",
      "fail: 0.00048010934790082967//0.00248289539760741\n",
      "completion: 0.001142590058000988//0.0008079859792289277\n",
      "replacement: 0.0009913491150722186//0.0015391257963985059\n",
      "bond: 0.0021925634418678824//0.006281642260165284\n",
      "navy: 0.0011111791036968208//0.0013520168182474465\n",
      "weight: 0.0011521497893354365//0.0002841496803748373\n",
      "rumor: 0.006563306343937557//0.004098066148807788\n",
      "tumble: 0.003906244644277723//0.0058217367687202655\n",
      "press: 0.000726965799364257//0.0018409334613425435\n",
      "extension: 0.0015787543664187243//0.0017442739479846525\n",
      "standpoint: 0.001273502873185777//0.0005060787055169573\n",
      "hunt: 0.00028046689808354925//0.0015605529716337513\n",
      "weak: 0.003912218676865907//0.009962646318702835\n",
      "rail: 0.0004839514329271968//0.001236612136204935\n",
      "la: 0.002318595710883576//0.0007791936345550503\n",
      "connect: 0.0029794860345765603//0.0005192973599186979\n",
      "commerce: 0.00034503491910915627//0.0015797603369307705\n",
      "threat: 0.0005712792818659333//0.0014892098744706838\n",
      "agency: 0.0019395247125834265//0.001483017387383351\n",
      "art: 0.0001268302973779005//0.0036761462778153943\n",
      "east: 0.0005168147518586976//0.0021606430117431462\n",
      "period: 0.0009074182663256251//0.0013963934616550609\n",
      "joint: 0.0024987237774314478//0.0012880026342948206\n",
      "large: 0.0019118603623929434//0.0019363811560727368\n",
      "stress: 0.0014318002723382956//0.0016003960527640174\n",
      "shop: 0.0016424447210681644//0.0014114223286062753\n",
      "battle: 0.0008940120771844881//0.0013181613403016126\n",
      "p: 0.0018664459500284//0.0005661716782707187\n",
      "transfer: 0.0018560639822314392//0.0010505595733702364\n",
      "innovation: 0.002222932077487151//0.0008353848236540768\n",
      "defend: 0.0006988921990180079//0.0012329662763607087\n",
      "bath: 0.0020495307417227855//0.001360160495391992\n",
      "probe: 0.0010307010842981526//0.0019908371246468016\n",
      "litigation: 0.001799935839955332//0.001564362857892238\n",
      "interact: 0.0012931462332427899//0.0008088468409760784\n",
      "appeal: 0.0017516935794213474//0.0021448661417386465\n",
      "smith: 0.004124056924296799//0.0\n",
      "nephew: 0.0011871435883441797//0.0\n",
      "staple: 0.0020023575598865197//0.00011602661908026969\n",
      "dynamic: 0.002654183390522667//0.0024542169341297214\n",
      "ing: 0.0018857473915748122//0.0\n",
      "candid: 0.0011723563505493969//0.0012829156000310703\n",
      "airway: 0.00045296171953050613//0.0014801872713712872\n",
      "love: 0.0017525843749288058//0.0019376698498333917\n",
      "watcher: 0.0007411140771109546//0.0003559770602004735\n",
      "lone: 0.0005680918999758388//0.000808163085152064\n",
      "pine: 0.0004680257454175857//0.0004494414071754073\n",
      "silicon: 0.002058613446155285//0.0\n",
      "sink: 8.78428712822774e-05//0.002291200752164812\n",
      "roundup: 0.0014069224838256956//0.0015492429882343075\n",
      "debut: 0.00035741967963992175//0.0016206288600778712\n",
      "modest: 0.0003964387726499108//0.002682090412858253\n",
      "jazz: 0.0//0.0022284494863934585\n",
      "publish: 0.001199289494179856//0.0011842423365155258\n",
      "plunge: 0.0010953136397039687//0.0020578120407623588\n",
      "shift: 0.0008150427937829039//0.0014106834854696279\n",
      "storage: 0.00398654291839672//0.0005005718744037895\n",
      "l: 0.0014473146082758124//0.0012802986757532813\n",
      "convict: 0.000495892435834051//0.0006479223153249582\n",
      "retreat: 0.0015312058295437418//0.0014066658182183248\n",
      "commodity: 0.000687559773293874//0.002361506017991235\n",
      "export: 0.00034118800541112085//0.0021275828275830223\n",
      "farmer: 0.001894898358318246//0.0006299679414158298\n",
      "drink: 0.0016844630531160044//0.0004226116278970821\n",
      "impact: 0.00194057199481909//0.0039429521118971075\n",
      "commission: 0.0007748956799098583//0.001239664476055122\n",
      "field: 0.0002561488119641243//0.002285092110267937\n",
      "secondary: 0.0016545349326271552//0.0020760637924303283\n",
      "every: 0.000789313068430659//0.0014143358637661607\n",
      "act: 0.0009206553926115636//0.0011588532630098247\n",
      "salon: 0.0017243327071988501//0.0\n",
      "entertain: 0.002796478231870677//0.0018675019486731686\n",
      "unusual: 0.002055470898076996//0.0007984148036334255\n",
      "cliff: 0.0//0.006703590650451624\n",
      "fourth: 0.000584956238096482//0.001301364865064365\n",
      "disappoint: 0.0007271417086824132//0.0044025709959410915\n",
      "alto: 0.0034092476700313813//0.00021774492859610116\n",
      "outfitter: 0.0056448633812449175//0.0\n",
      "survey: 0.0008219141958857272//0.0010120264455280858\n",
      "myriad: 0.002494817638321971//0.0011069516415714022\n",
      "southern: 0.002779991649636676//0.0010832656308926143\n",
      "journal: 0.001117699024361075//0.0009232598661826667\n",
      "suffer: 0.000769034913392074//0.0020263357127000316\n",
      "brown: 0.0//0.0019773400087791907\n",
      "kindred: 0.0032425903052966856//0.0\n",
      "warn: 0.0006795247935004737//0.003085776357806502\n",
      "tenet: 0.0019798035493824883//0.0\n",
      "within: 0.0011173762519382775//0.0006751521273348931\n",
      "accident: 0.0010428686679022792//0.0013020609335205149\n",
      "southwestern: 0.002036272949261621//0.0\n",
      "robin: 0.0019665366606750312//0.0\n",
      "inch: 0.000746211886529308//0.0004846775188824116\n",
      "closer: 0.0005375612340974817//0.0009153118704201193\n",
      "lodging: 0.0008073171349199684//0.0026282419299557855\n",
      "crash: 0.0007685827447076811//0.0014033489911006757\n",
      "micron: 0.0063894118362433465//0.0009655563393859201\n",
      "choice: 1.623959163178863e-05//0.0018673528337511118\n",
      "tri: 0.0011053232737539818//0.0010628579330773204\n",
      "century: 0.0013207706861964165//0.0005805342145416685\n",
      "transocean: 0.004718927876627654//0.0\n",
      "ackman: 0.0009934361398020266//0.0006908002685573994\n",
      "broker: 0.001028985548972144//0.0014479337989394766\n",
      "collin: 0.0//0.0028030880803554714\n",
      "soar: 0.0025718717869148157//0.0013141142761085333\n",
      "dot: 1.301177298844867e-05//0.0019926313520766325\n",
      "foot: 0.0005077760989392388//0.0016563889531001207\n",
      "locker: 0.0001268580527984564//0.0011667964359697478\n",
      "flu: 0.0013803299773544162//0.001101441674784443\n",
      "vale: 0.0017729959469748638//0.0\n",
      "signet: 0.0026613116267041924//0.0\n",
      "pioneer: 0.0014545752417788407//0.000880294275980321\n",
      "safety: 0.0011211774888301546//0.0025298190090922387\n",
      "charter: 0.00021509674027844796//0.002068149027993267\n",
      "rig: 0.0013036524651517092//0.00023994968276431618\n",
      "sanction: 0.00047761815795720967//0.0015432926211761248\n",
      "weather: 0.0003604172192011524//0.0016573181180024254\n",
      "kohl: 0.0016227881968112956//0.000578959049713614\n",
      "soup: 0.0//0.0027541664822904655\n",
      "lumber: 0.004620740698489848//0.00231683207351246\n",
      "discover: 0.0//0.0028468882454744217\n",
      "pinnacle: 0.0016832924104583325//0.0001449266247278702\n",
      "soon: 0.0007748052947294546//0.0017408737358025905\n",
      "lam: 0.0016117626129398255//1.6325825938980685e-05\n",
      "dominion: 0.0005624623162301127//0.0018430384953950516\n",
      "boston: 0.0//0.007682513429908318\n",
      "monster: 0.0034790471467772794//0.0003378546635971753\n",
      "train: 0.0005120613427263268//0.0015374973762411457\n",
      "fisher: 0.0013184541484977683//0.0001362364115710118\n",
      "thermo: 0.0012525847545986262//0.0\n",
      "governor: 0.0//0.00424326078843613\n",
      "starboard: 0.0008553429414792733//0.0013565322311085675\n",
      "vera: 0.0020288387506391414//0.0\n",
      "Prev days: -1\n",
      "Prev days: 0\n",
      "Prev days: 1\n",
      "Prev days: 2\n",
      "Prev days: 3\n",
      "Prev days: 4\n",
      "Prev days: 5\n",
      "Prev days: 6\n",
      "Prev days: 7\n",
      "Prev days: 8\n",
      "New folder ./data/out-of-sample/2013-05-01-vw-day+8 created\n",
      "Prev days: 9\n",
      "New folder ./data/out-of-sample/2013-05-01-vw-day+9 created\n",
      "Prev days: 10\n",
      "New folder ./data/out-of-sample/2013-05-01-vw-day+10 created\n"
     ]
    }
   ],
   "source": [
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "prev_days = 7\n",
    "best_config_file = './data/models/stemming/word-lists/2013-5-1.csv'\n",
    "sentiment_words = []\n",
    "O = np.array([0,0])\n",
    "with open(best_config_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "O = O[1:]\n",
    "for i in range(len(sentiment_words)):\n",
    "    print(sentiment_words[i] + \": \" + str(O[i][0]) + \"//\" + str(O[i][1]))\n",
    "for prev_days in range(-1,11):\n",
    "    print('Prev days: ' + str(prev_days))\n",
    "    # Collect out of sample articles\n",
    "    trial_id = '2013-05-01-vw-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    curr_date = possible_dates[min(prev_days,0)]\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('long value'), str('EARNING LONG'), str('short value'), str('EARNING SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    portfolio_value = 10000\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        # article_date_1 = article_date + dt.timedelta(days=1)\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        # total_earnings_long = 0\n",
    "        # total_earnings_short = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_top = sum([prev_top[t] for t in prev_top])\n",
    "                for tick in prev_top:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # investment = portfolio_value*(1/len(prev_top))\n",
    "                        investment = portfolio_value*(prev_top[tick]/total_sum_top)\n",
    "                        earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                        investment_long += investment\n",
    "                        # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                        # csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_bot = sum([prev_bot[t] for t in prev_bot])\n",
    "                for tick in prev_bot:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                        # investment = portfolio_value*(1/len(prev_bot))\n",
    "                        investment = portfolio_value*(prev_bot[tick]/total_sum_bot)\n",
    "                        earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                        investment_short += investment\n",
    "                        # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_d.append(oos_bow)\n",
    "            \n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0.5\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                testing_s = sum(oos_bow.get(w,0) for w in sentiment_words)\n",
    "                if (testing_s > 0):\n",
    "                    est_p = fminbound(equation_to_solve, 0, 1, (O,oos_bow, sentiment_words,testing_s,LAM))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            # ensure the portfolio can't buy the same stock for both long and short (because that's SILLY)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items() if val > 0.5}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items() if val < 0.5}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((curr_date).date()), str(investment_long), str(earning_long), str(investment_short), str(earning_short), str(len(prev_top)), str(len(prev_bot)), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w/ bigrams\n",
    "Construct portfolios using the model trained by bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EW\n",
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "prev_days = 7\n",
    "best_config_file = './data/models/stemming/word-lists/2013-5-1.csv'\n",
    "best_config_bigram_file = './data/models/stemming/word-lists/2013-5-1.csv'\n",
    "sentiment_words = []\n",
    "O = np.array([0,0])\n",
    "with open(best_config_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "with open(best_config_bigram_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "O = O[1:]\n",
    "\n",
    "# change this value to calculate from day t + x to t + y\n",
    "for prev_days in range(1,2):\n",
    "    print('Prev days: ' + str(prev_days))\n",
    "    # Collect out of sample articles\n",
    "\n",
    "    trial_id = '2013-05-01-111-+bigram-ew-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    curr_date = possible_dates[min(prev_days,0)]\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('long value'), str('EARNING LONG'), str('short value'), str('EARNING SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    # amount invested\n",
    "    portfolio_value = 10000\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_top = sum([prev_top[t] for t in prev_top])\n",
    "                for tick in prev_top:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        investment = portfolio_value*(1/len(prev_top))\n",
    "                        # investment = portfolio_value*(prev_top[tick]/total_sum_top)\n",
    "                        earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                        investment_long += investment\n",
    "                        # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                        # csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_bot = sum([prev_bot[t] for t in prev_bot])\n",
    "                for tick in prev_bot:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                        investment = portfolio_value*(1/len(prev_bot))\n",
    "                        # investment = portfolio_value*(prev_bot[tick]/total_sum_bot)\n",
    "                        earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                        investment_short += investment\n",
    "                        # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            oos_d_bigram = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_bow.update(text_to_bow_bigram(oos_a['headline']))\n",
    "                oos_d.append(oos_bow)\n",
    "                # oos_bow_bigram = text_to_bow_bigram(oos_a['headline'])\n",
    "                # oos_d.append(oos_bow)\n",
    "\n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0.5\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                testing_s = sum(oos_bow.get(w,0) for w in sentiment_words)\n",
    "                if (testing_s > 0):\n",
    "                    est_p = fminbound(equation_to_solve, 0, 1, (O,oos_bow, sentiment_words,testing_s,LAM))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            # ensure the portfolio can't buy the same stock for both long and short (because that's SILLY)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items() if val > 0.5}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items() if val < 0.5}\n",
    "            print(str(curr_date) + \"//\" + str(bot_50_tickers))\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((curr_date).date()), str(investment_long), str(earning_long), str(investment_short), str(earning_short), str(len(prev_top)), str(len(prev_bot)), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LM and H4 lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form each word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc negative and positive word lists for LM\n",
    "positive_words_lm = {}\n",
    "negative_words_lm = {}\n",
    "with open('../external-csvs/LM-Dictionary-1993-2021.csv', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            if row[8] != str(0):\n",
    "                positive_words_lm[str(row[0]).lower()] = 0\n",
    "            if row[7] != str(0):\n",
    "                negative_words_lm[str(row[0]).lower()] = 0\n",
    "            # sentiment_score[row[0]] = {'neg': float(row[8]), 'pos': float(row[9])}\n",
    "        line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc negative and positive word lists for h4\n",
    "positive_words_h4 = {}\n",
    "negative_words_h4 = {}\n",
    "with open('../external-csvs/HIV-4.csv', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            if row[2] != \"\":\n",
    "                positive_words_h4[str(row[0]).lower()] = 0\n",
    "            if row[3] != \"\":\n",
    "                negative_words_h4[str(row[0]).lower()] = 0\n",
    "            # sentiment_score[row[0]] = {'neg': float(row[8]), 'pos': float(row[9])}\n",
    "        line_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LM portfolio formation\n",
    "TODO: change positive words to positive_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n"
     ]
    }
   ],
   "source": [
    "# Collect out of sample articles\n",
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "for prev_days in range(-1,8):\n",
    "    trial_id = 'LM-EW-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    # sentiment_words = []\n",
    "    # sentiment_score = {}\n",
    "    # with open('../external-csvs/LM-Dictionary-1993-2021.csv', encoding='utf-8') as csv_file:\n",
    "    #     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    #     line_count = 0\n",
    "    #     # FORMAT: line#,headline,date,stock\n",
    "    #     for row in csv_reader:\n",
    "    #         if line_count > 0:\n",
    "    #             sentiment_score[row[0]] = {'neg': float(row[8]), 'pos': float(row[9])}\n",
    "    #         line_count += 1\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('EARNING LONG'), str('INVESTMENT LONG'), str('EARNING SHORT'), str('INVESTMENT SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TURNOVER %'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        # article_date_1 = article_date + dt.timedelta(days=1)\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        # calculate tfidf for each day\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        # total_earnings_long = 0\n",
    "        # total_earnings_short = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        portfolio_value = 10000\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                    csvwriter = csv.writer(csv_file)\n",
    "                    for tick in prev_top:\n",
    "                        testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                        if not testing_value == 'E':\n",
    "                            investment = portfolio_value*(1/len(prev_top))\n",
    "                            earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                            investment_long += investment\n",
    "                            # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                            csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                            if (prev_top[tick] - testing_value <= 0):\n",
    "                                long_correct += 1\n",
    "                with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                    csvwriter = csv.writer(csv_file)\n",
    "                    for tick in prev_bot:\n",
    "                        testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                        if not testing_value == 'E':\n",
    "                            csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                            investment = portfolio_value*(1/len(prev_bot))\n",
    "                            earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                            investment_short += investment\n",
    "                            # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "                            if (prev_bot[tick] - testing_value > 0):\n",
    "                                short_correct += 1\n",
    "                    #else what? Assume not correct? Assume correct? I have chosen to assume incorrect\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_d.append(oos_bow)\n",
    "            for w in positive_words:\n",
    "                # tf = \n",
    "                idf = len(oos_d) / (1+(len([a for a in oos_d if w in a])))\n",
    "                if idf > len(oos_d):\n",
    "                    print(str(w) + \" // \" + str(idf))\n",
    "                positive_words[w] = math.log(idf)\n",
    "            # print(oos_d[0])\n",
    "            # print('coverage' in oos_d[0])\n",
    "            # print([a for a in oos_d if 'miss' in a])\n",
    "            for w in negative_words:\n",
    "                # tf = \n",
    "                idf = len(oos_d) / (1+(len([a for a in oos_d if w in a])))\n",
    "                if idf > len(oos_d):\n",
    "                    print(str(w) + \" // \" + str(idf))\n",
    "                negative_words[w] = math.log(idf)\n",
    "            \n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                for w in oos_bow:\n",
    "                    # if w in positive_words or w in negative_words:\n",
    "                    est_p += (positive_words.get(w,0)*math.log(1+oos_bow.get(w,0)) - negative_words.get(w,0)*math.log(1+oos_bow.get(w,0)))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items()}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items()}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "            if (len(prev_top) > 0):\n",
    "                diff_stocks /= (len(prev_bot) + len(prev_top))\n",
    "            else:\n",
    "                diff_stocks = 0\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((prev_date).date()), str(earning_long), str(investment_long), str(earning_short), str(investment_short), str(len(prev_top)), str(len(prev_bot)), str(diff_stocks), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))\n",
    "        curr_date = curr_date + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4 portfolio formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n",
      "New folder ./data/out-of-sample/H4-EW-day+-1 created\n",
      "New folder ./data/out-of-sample/H4-EW-day+0 created\n",
      "New folder ./data/out-of-sample/H4-EW-day+1 created\n",
      "New folder ./data/out-of-sample/H4-EW-day+2 created\n",
      "New folder ./data/out-of-sample/H4-EW-day+3 created\n",
      "New folder ./data/out-of-sample/H4-EW-day+4 created\n",
      "New folder ./data/out-of-sample/H4-EW-day+5 created\n",
      "New folder ./data/out-of-sample/H4-EW-day+6 created\n",
      "New folder ./data/out-of-sample/H4-EW-day+7 created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collect out of sample articles\n",
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "for prev_days in range(-1,8):\n",
    "    trial_id = 'H4-EW-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    # sentiment_words = []\n",
    "    # sentiment_score = {}\n",
    "    # with open('../external-csvs/LM-Dictionary-1993-2021.csv', encoding='utf-8') as csv_file:\n",
    "    #     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    #     line_count = 0\n",
    "    #     # FORMAT: line#,headline,date,stock\n",
    "    #     for row in csv_reader:\n",
    "    #         if line_count > 0:\n",
    "    #             sentiment_score[row[0]] = {'neg': float(row[8]), 'pos': float(row[9])}\n",
    "    #         line_count += 1\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('EARNING LONG'), str('INVESTMENT LONG'), str('EARNING SHORT'), str('INVESTMENT SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TURNOVER %'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        # article_date_1 = article_date + dt.timedelta(days=1)\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        # calculate tfidf for each day\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        # total_earnings_long = 0\n",
    "        # total_earnings_short = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        portfolio_value = 10000\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                    csvwriter = csv.writer(csv_file)\n",
    "                    for tick in prev_top:\n",
    "                        testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                        if not testing_value == 'E':\n",
    "                            investment = portfolio_value*(1/len(prev_top))\n",
    "                            earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                            investment_long += investment\n",
    "                            # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                            csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                            if (prev_top[tick] - testing_value <= 0):\n",
    "                                long_correct += 1\n",
    "                with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                    csvwriter = csv.writer(csv_file)\n",
    "                    for tick in prev_bot:\n",
    "                        testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                        if not testing_value == 'E':\n",
    "                            csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                            investment = portfolio_value*(1/len(prev_bot))\n",
    "                            earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                            investment_short += investment\n",
    "                            # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "                            if (prev_bot[tick] - testing_value > 0):\n",
    "                                short_correct += 1\n",
    "                    #else what? Assume not correct? Assume correct? I have chosen to assume incorrect\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_d.append(oos_bow)\n",
    "            for w in positive_words_h4:\n",
    "                # tf = \n",
    "                idf = len(oos_d) / (1+(len([a for a in oos_d if w in a])))\n",
    "                if idf > len(oos_d):\n",
    "                    print(str(w) + \" // \" + str(idf))\n",
    "                positive_words_h4[w] = math.log(idf)\n",
    "            # print(oos_d[0])\n",
    "            # print('coverage' in oos_d[0])\n",
    "            # print([a for a in oos_d if 'miss' in a])\n",
    "            for w in negative_words_h4:\n",
    "                # tf = \n",
    "                idf = len(oos_d) / (1+(len([a for a in oos_d if w in a])))\n",
    "                if idf > len(oos_d):\n",
    "                    print(str(w) + \" // \" + str(idf))\n",
    "                negative_words_h4[w] = math.log(idf)\n",
    "            \n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                for w in oos_bow:\n",
    "                    # if w in positive_words or w in negative_words:\n",
    "                    est_p += (positive_words_h4.get(w,0)*math.log(1+oos_bow.get(w,0)) - negative_words_h4.get(w,0)*math.log(1+oos_bow.get(w,0)))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items()}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items()}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "            if (len(prev_top) > 0):\n",
    "                diff_stocks /= (len(prev_bot) + len(prev_top))\n",
    "            else:\n",
    "                diff_stocks = 0\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((prev_date).date()), str(earning_long), str(investment_long), str(earning_short), str(investment_short), str(len(prev_top)), str(len(prev_bot)), str(diff_stocks), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))\n",
    "        curr_date = curr_date + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate daily turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHORT\n",
      "0.8870249518359968\n",
      "long\n",
      "0.918329076742926\n"
     ]
    }
   ],
   "source": [
    "trial_id = '2013-05-01-ew-day+1'\n",
    "\n",
    "# pseudocode\n",
    "# sum(weight_t+1 - (weight_t(1+value_t+1)/1+sum(weights_t*values_t+1)))/2T\n",
    "\n",
    "portfolio_path_short = './data/out-of-sample/' + trial_id + '/portfolios/short'\n",
    "portfolio_path_long = './data/out-of-sample/' + trial_id + '/portfolios/long'\n",
    "# recreate list of stock information\n",
    "pathlist = Path(portfolio_path_short).rglob('*.csv')\n",
    "i = 0\n",
    "lst_dates = []\n",
    "weights_long = {}\n",
    "vals_long = {}\n",
    "for path in pathlist:\n",
    "    date = datetime.strptime(os.path.basename(str(path))[:-4], '%Y-%m-%d').date()\n",
    "    lst_dates.append(date)\n",
    "    #long\n",
    "    line_count = 0\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        # line_count = sum(1 for row in csv_reader1)\n",
    "        # # FORMAT: line#,headline,date,stock\n",
    "        # line_count = len(list(csv_reader))\n",
    "        # print(line_count)\n",
    "        # print(list(csv_reader))\n",
    "        weights_date = []\n",
    "        for row in csv_reader:\n",
    "            if weights_long.get(row[0], 'E') == 'E':\n",
    "                weights_long[row[0]] = {}\n",
    "                vals_long[row[0]] = {}\n",
    "            vals_long[row[0]][date] = float(row[2])\n",
    "            weights_date.append(float(row[1]))\n",
    "            line_count += 1\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            # weights_long[row[0]][date] = weights_date[line_count]/sum(weights_date)\n",
    "            weights_long[row[0]][date] = 1/len(weights_date)\n",
    "            line_count += 1\n",
    "pathlist = Path(portfolio_path_long).rglob('*.csv')\n",
    "i = 0\n",
    "lst_dates = []\n",
    "weights_short = {}\n",
    "vals_short = {}\n",
    "for path in pathlist:\n",
    "    date = datetime.strptime(os.path.basename(str(path))[:-4], '%Y-%m-%d').date()\n",
    "    lst_dates.append(date)\n",
    "    #long\n",
    "    line_count = 0\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        # line_count = sum(1 for row in csv_reader1)\n",
    "        # # FORMAT: line#,headline,date,stock\n",
    "        # line_count = len(list(csv_reader))\n",
    "        # print(line_count)\n",
    "        # print(list(csv_reader))\n",
    "        weights_date = []\n",
    "        for row in csv_reader:\n",
    "            if weights_short.get(row[0], 'E') == 'E':\n",
    "                weights_short[row[0]] = {}\n",
    "                vals_short[row[0]] = {}\n",
    "            vals_short[row[0]][date] = float(row[2])\n",
    "            weights_date.append(float(row[1]))\n",
    "            line_count += 1\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            # weights_short[row[0]][date] = weights_date[line_count]/sum(weights_date)\n",
    "            weights_short[row[0]][date] = 1/len(weights_date)\n",
    "            line_count+=1\n",
    "\n",
    "# print(vals_long)\n",
    "# print(weights_long)\n",
    "turnover = 0\n",
    "print('SHORT')\n",
    "for t in range(len(lst_dates)-1):\n",
    "    calc_sum = 0\n",
    "    for j in weights_long:\n",
    "        calc_sum += weights_long[j].get(lst_dates[t], 0)*vals_long[j].get(lst_dates[t+1], 0)\n",
    "    for i in weights_long:\n",
    "        turnover += abs(weights_long[i].get(lst_dates[t+1], 0) - (weights_long[i].get(lst_dates[t],0)*(1+vals_long[i].get(lst_dates[t+1], 0)))/(1+calc_sum))\n",
    "turnover /= 2*len(lst_dates)\n",
    "print(turnover)\n",
    "turnover = 0\n",
    "print('long')\n",
    "for t in range(len(lst_dates)-1):\n",
    "    calc_sum = 0\n",
    "    for j in weights_short:\n",
    "        calc_sum += weights_short[j].get(lst_dates[t], 0)*vals_short[j].get(lst_dates[t+1], 0)\n",
    "    for i in weights_short:\n",
    "        turnover += abs(weights_short[i].get(lst_dates[t+1], 0) - (weights_short[i].get(lst_dates[t],0)*(1+vals_short[i].get(lst_dates[t+1], 0)))/(1+calc_sum))\n",
    "turnover /= 2*len(lst_dates)\n",
    "print(turnover)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print table of words with similarities in H4 and LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_words(data, n=2, order=False, reverse=True):\n",
    "    \"\"\"Get top n words by tone. \n",
    "\n",
    "    Returns a dictionary or an `OrderedDict` if `order` is true.\n",
    "    \"\"\" \n",
    "    top = sorted(data.items(), key=lambda x: x[1][''], reverse=reverse)[:n]\n",
    "    if order:\n",
    "        return OrderedDict(top)\n",
    "    return dict(top)\n",
    "\n",
    "pathlist = Path('./data/models/stemming/word-lists/').rglob('*.csv')\n",
    "word_list_tone = {}\n",
    "word_list_count = {}\n",
    "for path in pathlist:\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        line = 0\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if line > 0:\n",
    "                if (row[0] in word_list_tone):\n",
    "                    word_list_tone[row[0]] += (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] += 1\n",
    "                else:\n",
    "                    word_list_tone[row[0]] = (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] = 1\n",
    "            line += 1\n",
    "\n",
    "word_list_count = {k: v for k, v in sorted(word_list_count.items(), reverse = True, key=lambda item: item[1])}\n",
    "word_list_tone = {k: v for k, v in sorted(word_list_tone.items(), key=lambda item: item[1])}\n",
    "for w in word_list_tone:\n",
    "    word_list_tone[w] /= 19\n",
    "# print(word_list_count)\n",
    "\n",
    "high_word_counts    = [w for w in word_list_count if word_list_count[w] > 14]\n",
    "# for w in high_word_counts:\n",
    "#     print(w, word_list_tone[w], word_list_count[w])\n",
    "pos_word_list_count = []\n",
    "neg_word_list_count = []\n",
    "count_id = 20\n",
    "while (len(pos_word_list_count) < 50):\n",
    "    new_words = {w : word_list_tone[w] for w in word_list_count if (word_list_count[w] == count_id and word_list_tone[w] > 0)}\n",
    "    new_words = dict(sorted(new_words.items(), key=lambda item: item[1], reverse = True))\n",
    "    for w in new_words:\n",
    "        pos_word_list_count.append(w)\n",
    "    count_id -= 1\n",
    "pos_word_list_count = pos_word_list_count[:50]\n",
    "count_id = 20\n",
    "while (len(neg_word_list_count) < 50):\n",
    "    new_words = {w : word_list_tone[w] for w in word_list_count if (word_list_count[w] == count_id and word_list_tone[w] < 0)}\n",
    "    new_words = dict(sorted(new_words.items(), key=lambda item: item[1], reverse = False))\n",
    "    for w in new_words:\n",
    "        neg_word_list_count.append(w)\n",
    "    count_id -= 1\n",
    "neg_word_list_count = neg_word_list_count[:50]\n",
    "# pos_word_list_count = [w for w in word_list_count if word_list_tone[w] > 0][:50]\n",
    "# neg_word_list_count = [w for w in word_list_count if word_list_tone[w] < 0][:50]\n",
    "for i in range(50):\n",
    "# for w in pos_word_list_count:\n",
    "    pos_w = pos_word_list_count[i]\n",
    "    neg_w = neg_word_list_count[i]\n",
    "    in_lm_pos = 0\n",
    "    in_h4_pos = 0\n",
    "    if pos_w in positive_words_lm:\n",
    "        in_lm_pos = 1\n",
    "    if pos_w in positive_words_h4:\n",
    "        in_h4_pos = 1\n",
    "# for w in neg_word_list_count:\n",
    "    in_lm_neg = 0\n",
    "    in_h4_neg = 0\n",
    "    if neg_w in negative_words_lm:\n",
    "        in_lm_neg = 1\n",
    "    if neg_w in negative_words_h4:\n",
    "        in_h4_neg = 1\n",
    "    print(pos_w + \" & \" + str(round(word_list_tone[pos_w],6)) + \" & \" + str(word_list_count[pos_w]) + \" & \" + str(in_lm_pos) + \" & \" + str(in_h4_pos) + \" & \" + neg_w + \" & \" + str(round(word_list_tone[neg_w],6)) + \" & \" + str(word_list_count[neg_w]) + \" & \" + str(in_lm_neg) + \" & \" + str(in_h4_neg) + \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing profit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input ff data\n",
    "ff_5 = {}\n",
    "with open('../external-csvs/F-F_Research_Data_5_Factors_2x3_daily_CSV/F-F_Research_Data_5_Factors_2x3_daily.CSV', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            ff_date = datetime.strptime(row[0],'%Y%m%d').date()\n",
    "            if ff_date >= datetime(2019,1,1).date():\n",
    "                # print(ff_date)\n",
    "                ff_5[ff_date] = [float(row[1]), float(row[2]), float(row[3]), float(row[4]), float(row[5]), float(row[6])]\n",
    "        line_count += 1\n",
    "\n",
    "test_date = datetime(2019,1,4).date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======EW======\n",
      "EW LS & 0.19 & 2.61 & & 0.73 & 1.46\\% & 1.23 & 1.62\\% \\\\\n",
      "EW L & 0.7 & 10.09 & & 8.39 & 26.6\\% & 8.55 & 27.32\\% \\\\\n",
      "EW S & -0.57 & -7.47 & & -8.36 & 24.29\\% & -8.02 & 24.87\\% \\\\\n",
      "VW LS & -0.14 & -0.6 & & -2.71 & 4.45\\% & -2.87 & 4.47\\% \\\\\n",
      "VW L & -0.28 & -2.51 & & -5.29 & 22.04\\% & -5.89 & 23.52\\% \\\\\n",
      "VW S & 0.09 & 1.91 & & 1.88 & 29.57\\% & 2.32 & 30.65\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def gen_alpha_table(path,prefix):\n",
    "    long_profit = {}\n",
    "    short_profit = {}\n",
    "    ls_profit = {}\n",
    "    long_rf_profit = []\n",
    "    short_rf_profit = []\n",
    "    ls_rf_profit = []\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count > 0:\n",
    "                # DATE,long value,EARNING LONG,short value,EARNING SHORT,NUMBER LONG,NUMBER SHORT,TOTAL FIRMS WITH ARTS,HEADLINES WITH SENTIMENT WORDS\n",
    "                if float(row[1]) > 0:\n",
    "                    long_profit[row[0]] = (float(row[2])/float(row[1])  - 1)\n",
    "                    short_profit[row[0]] = (float(row[3])/float(row[4])  - 1)\n",
    "                    ls_profit[row[0]] = long_profit[row[0]] + short_profit[row[0]]\n",
    "            line_count += 1\n",
    "        \n",
    "    long_avg = 0\n",
    "    short_avg = 0\n",
    "    ls_avg = 0\n",
    "    len_d = 0\n",
    "    mkt_rf = []\n",
    "    smb = []\n",
    "    hml = []\n",
    "    rmw = []\n",
    "    cma = []\n",
    "    rf = []\n",
    "    for d in long_profit:\n",
    "        long_avg += long_profit[d]\n",
    "        short_avg += short_profit[d]\n",
    "        ls_avg += ls_profit[d]\n",
    "\n",
    "        prof_date = datetime.strptime(d, '%Y-%m-%d').date()\n",
    "\n",
    "        if prof_date in ff_5:\n",
    "            #Mkt-RF,SMB,HML,RMW,CMA,RF\n",
    "            mkt_rf.append(ff_5[prof_date][0])\n",
    "            smb.append(ff_5[prof_date][1])\n",
    "            hml.append(ff_5[prof_date][2])\n",
    "            rmw.append(ff_5[prof_date][3])\n",
    "            cma.append(ff_5[prof_date][4])\n",
    "            rf.append(ff_5[prof_date][5])\n",
    "            ls_rf_profit.append((float(ls_profit[d])*100 - float(ff_5[prof_date][5])))\n",
    "            long_rf_profit.append((float(long_profit[d])*100 - float(ff_5[prof_date][5])))\n",
    "            short_rf_profit.append((float(short_profit[d])*100 - float(ff_5[prof_date][5])))\n",
    "            len_d += 1\n",
    "\n",
    "    ff_3_data = [mkt_rf, smb, hml]\n",
    "    ff_5_data = [mkt_rf, smb, hml, rmw, cma]\n",
    "\n",
    "    ff_3_data = np.array(ff_3_data).transpose()\n",
    "    ff_5_data = np.array(ff_5_data).transpose()\n",
    "    # print(ff_3_data)\n",
    "\n",
    "    ff_3_sm = sm.add_constant(ff_3_data)\n",
    "    ff_5_sm = sm.add_constant(ff_5_data)\n",
    "    model_3 = sm.OLS(ls_rf_profit, ff_3_sm)\n",
    "    results_3 = model_3.fit()\n",
    "\n",
    "    model_5 = sm.OLS(ls_rf_profit, ff_5_sm)\n",
    "    results_5 = model_5.fit()\n",
    "\n",
    "    ls_avg /= len_d\n",
    "    sharpe_ls = ((np.average(ls_rf_profit)/np.std(ls_rf_profit))) * math.sqrt(252)\n",
    "    print(prefix + \" LS & \" + str(round(sharpe_ls,2)) + \" & \" + str(round(ls_avg*10000,2)) + ' & & ' + str(round(results_3.params[0]*100,2)) + ' & ' + str(round(results_3.rsquared*100,2)) + '\\\\% & ' + str(round(results_5.params[0]*100,2)) + ' & ' + str(round(results_5.rsquared*100,2)) + '\\\\% \\\\\\\\')\n",
    "    # print(results.model())\n",
    "\n",
    "    long_avg /= len_d\n",
    "\n",
    "    model_3 = sm.OLS(long_rf_profit, ff_3_sm)\n",
    "    results_3 = model_3.fit()\n",
    "\n",
    "    model_5 = sm.OLS(long_rf_profit, ff_5_sm)\n",
    "    results_5 = model_5.fit()\n",
    "\n",
    "    sharpe_long = ((np.average(long_rf_profit)/np.std(long_rf_profit))) * math.sqrt(252)\n",
    "    print(prefix + \" L & \" + str(round(sharpe_long,2)) + \" & \" + str(round(long_avg*10000,2)) + ' & & ' + str(round(results_3.params[0]*100,2)) + ' & ' + str(round(results_3.rsquared*100,2)) + '\\\\% & ' + str(round(results_5.params[0]*100,2)) + ' & ' + str(round(results_5.rsquared*100,2)) + '\\\\% \\\\\\\\')\n",
    "\n",
    "    short_avg /= len_d\n",
    "    model_3 = sm.OLS(short_rf_profit, ff_3_sm)\n",
    "    results_3 = model_3.fit()\n",
    "\n",
    "    model_5 = sm.OLS(short_rf_profit, ff_5_sm)\n",
    "    results_5 = model_5.fit()\n",
    "\n",
    "    sharpe_short = ((np.average(short_rf_profit)/np.std(short_rf_profit))) * math.sqrt(252)\n",
    "    print(prefix + \" S & \" + str(round(sharpe_short,2)) + \" & \" + str(round(short_avg*10000,2)) + ' & & ' + str(round(results_3.params[0]*100,2)) + ' & ' + str(round(results_3.rsquared*100,2)) + '\\\\% & ' + str(round(results_5.params[0]*100,2)) + ' & ' + str(round(results_5.rsquared*100,2)) + '\\\\% \\\\\\\\')\n",
    "\n",
    "# output average returns of LS, L, S of equal and value weighted\n",
    "# equal weighted\n",
    "trial_date  = '2013-05-01'\n",
    "suffix      = '-111-ew-day+1' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "path = target_directory + '/estimations.csv'\n",
    "\n",
    "\n",
    "print('======EW======')\n",
    "gen_alpha_table(path, 'EW')\n",
    "trial_date  = '2013-05-01'\n",
    "suffix      = '-111-vw-day+1' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "path = target_directory + '/estimations.csv'\n",
    "gen_alpha_table(path, 'VW')\n",
    "# print('====LONG SHORT====')\n",
    "# # print(reg_m(l_raw, ff_3_data).summary())\n",
    "# print(results.params)\n",
    "# # print(results.model())\n",
    "# print(results.rsquared)\n",
    "# print('ls avg: ' + str(ls_avg*10000))\n",
    "# print('long avg: ' + str(long_avg*10000))\n",
    "# print('short avg: ' + str(short_avg*10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-day ahead cum log graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [[\"null\",0,0,0,0,0,0]]\n",
    "trial_date  = '2013-05-01'\n",
    "suffix      = '-111-ew-day+1' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "ew_path = target_directory + '/estimations.csv'\n",
    "\n",
    "suffix      = '-111-vw-day+1' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "vw_path = target_directory + '/estimations.csv'\n",
    "\n",
    "with open(str(ew_path), encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    prev_row = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and float(row[1]) > 0:\n",
    "            #do something\n",
    "            long_profit = math.log(float(row[2])/float(row[1]))\n",
    "            short_profit = math.log(float(row[3])/float(row[4]))\n",
    "            days.append([row[0], long_profit + days[prev_row][1], short_profit + days[prev_row][2], long_profit + short_profit + days[prev_row][3]])\n",
    "            prev_row += 1\n",
    "        line_count += 1\n",
    "\n",
    "with open(str(vw_path), encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    prev_row = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and float(row[1]) > 0:\n",
    "            #do something\n",
    "            long_profit = math.log(float(row[2])/float(row[1]))\n",
    "            short_profit = math.log(float(row[3])/float(row[4]))\n",
    "            days[prev_row + 1].extend([long_profit + days[prev_row][4], short_profit + days[prev_row][5], short_profit + long_profit + days[prev_row][6]])\n",
    "            prev_row += 1\n",
    "        line_count += 1\n",
    "\n",
    "with open('../Report/data/one-day-ahead.csv', 'w',newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"date\",\"EW-L\",\"EW-S\",\"EW-LS\",\"VW-L\",\"VW-S\",\"VW-LS\"])\n",
    "    for d in days:\n",
    "        if d[0] != 'null':\n",
    "            csvwriter.writerow([d[0], d[1]*100, d[2]*100, d[3]*100, d[4]*100, d[5]*100, d[6]*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EW LS & 15.53 & 258.76 & & 259.78 & 4.92\\% & 259.21 & 7.6\\% \\\\\n",
      "EW L & 7.23 & 138.28 & & 140.4 & 7.78\\% & 140.33 & 9.12\\% \\\\\n",
      "EW S & 7.84 & 120.48 & & 118.68 & 29.62\\% & 118.18 & 29.78\\% \\\\\n",
      "VW LS & 12.72 & 164.11 & & 164.44 & 3.4\\% & 163.49 & 6.23\\% \\\\\n",
      "VW L & 5.26 & 76.57 & & 76.11 & 14.2\\% & 76.35 & 15.76\\% \\\\\n",
      "VW S & 6.58 & 87.55 & & 87.63 & 28.88\\% & 86.44 & 29.39\\% \\\\\n",
      "EW LS & 10.0 & 113.75 & & 110.15 & 4.11\\% & 110.42 & 4.25\\% \\\\\n",
      "EW L & 2.5 & 34.18 & & 31.59 & 23.47\\% & 32.09 & 24.37\\% \\\\\n",
      "EW S & 4.83 & 79.57 & & 77.87 & 23.08\\% & 77.64 & 23.36\\% \\\\\n",
      "VW LS & 9.61 & 95.39 & & 93.13 & 4.79\\% & 93.42 & 4.93\\% \\\\\n",
      "VW L & 2.78 & 30.1 & & 27.12 & 30.44\\% & 27.53 & 31.03\\% \\\\\n",
      "VW S & 4.77 & 65.29 & & 65.31 & 24.58\\% & 65.2 & 25.13\\% \\\\\n",
      "EW LS & 0.19 & 2.61 & & 0.73 & 1.46\\% & 1.23 & 1.62\\% \\\\\n",
      "EW L & 0.7 & 10.09 & & 8.39 & 26.6\\% & 8.55 & 27.32\\% \\\\\n",
      "EW S & -0.57 & -7.47 & & -8.36 & 24.29\\% & -8.02 & 24.87\\% \\\\\n",
      "VW LS & -0.14 & -0.6 & & -2.71 & 4.45\\% & -2.87 & 4.47\\% \\\\\n",
      "VW L & -0.28 & -2.51 & & -5.29 & 22.04\\% & -5.89 & 23.52\\% \\\\\n",
      "VW S & 0.09 & 1.91 & & 1.88 & 29.57\\% & 2.32 & 30.65\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "## day -1 to day +1 table\n",
    "\n",
    "for day_no in range(-1, 2):\n",
    "    trial_date  = '2013-05-01'\n",
    "    suffix      = '-111-ew-day+' + str(day_no)\n",
    "    target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "    path = target_directory + '/estimations.csv'\n",
    "    gen_alpha_table(path, 'EW')\n",
    "\n",
    "    trial_date  = '2013-05-01'\n",
    "    suffix      = '-111-vw-day+'  + str(day_no)\n",
    "    target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "    path = target_directory + '/estimations.csv'\n",
    "    gen_alpha_table(path, 'VW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed assimilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../Report/data/speed-assimilation.csv', 'w',newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"day\",\"avg-LS\",\"avg-L\",\"avg-S\"])\n",
    "\n",
    "for day_no in range(1, 11):\n",
    "    trial_date  = '2013-05-01'\n",
    "    suffix      = '-111-ew-day+' + str(day_no)\n",
    "    target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "    path = target_directory + '/estimations.csv'\n",
    "    long_profit = {}\n",
    "    short_profit = {}\n",
    "    ls_profit = {}\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count > 0:\n",
    "                # DATE,long value,EARNING LONG,short value,EARNING SHORT,NUMBER LONG,NUMBER SHORT,TOTAL FIRMS WITH ARTS,HEADLINES WITH SENTIMENT WORDS\n",
    "                if float(row[1]) > 0:\n",
    "                    long_profit[row[0]] = (float(row[2])/float(row[1])  - 1)\n",
    "                    short_profit[row[0]] = (float(row[3])/float(row[4])  - 1)\n",
    "                    ls_profit[row[0]] = long_profit[row[0]] + short_profit[row[0]]\n",
    "            line_count += 1\n",
    "        \n",
    "    long_avg = 0\n",
    "    short_avg = 0\n",
    "    ls_avg = 0\n",
    "    len_d = 0\n",
    "    for d in long_profit:\n",
    "        long_avg += long_profit[d]\n",
    "        short_avg += short_profit[d]\n",
    "        ls_avg += ls_profit[d]\n",
    "\n",
    "    with open('../Report/data/speed-assimilation.csv', 'a',newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([str(day_no),str(ls_avg*10000/len(ls_profit)), str(long_avg*10000/len(long_profit)), str(short_avg*10000/len(short_profit))])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
