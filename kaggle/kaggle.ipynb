{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sklearn\n",
    "from scipy.optimize import fminbound\n",
    "from sklearn import preprocessing\n",
    "# import scikit-learn\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# from textblob import TextBlob as tb\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "est = pytz.timezone('US/Eastern')\n",
    "import datetime as dt\n",
    "import nltk\n",
    "import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pandas import DataFrame\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import os\n",
    "import yfinance as yf\n",
    "import json\n",
    "import sys\n",
    "# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from itertools import islice\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "#mulitthreading imports\n",
    "import logging\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the big list of functions for the actual SESTM computation. We'll use this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "#Hyper params -- ROLLING WINDOW IN FUTUERe\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "en_words = set(nltk.corpus.words.words())\n",
    "\n",
    "# the complete sestm function list\n",
    "def calc_returns(article):\n",
    "    # ABSOLUTE VALUE\n",
    "    # returns = float(article['mrkt_info']['close']) - float(article['mrkt_info']['open'])\n",
    "    # PERCENTAGE VALUE\n",
    "    returns = float(article['mrkt_info']['close'])/float(article['mrkt_info']['open']) - 1\n",
    "    sgn_a = -1\n",
    "    if (returns > 0): # add -1 if returns are 0 or less, 1 otherwise\n",
    "        sgn_a = 1\n",
    "    return (returns, sgn_a)\n",
    "\n",
    "def text_to_bow(text):\n",
    "    readable_text = text.lower()\n",
    "    # print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "    # substitute non alphabet chars (new lines become spaces)\n",
    "    readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "    readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "    # sub multiple spaces with one space\n",
    "    readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "    # tokenise text\n",
    "    words = nltk.wordpunct_tokenize(readable_text)\n",
    "    bow_art = {}\n",
    "    # lemmatised_words = []\n",
    "    if len(words) > 0:\n",
    "        # lemmatise, remove non-english, and remove stopwords\n",
    "        for w in words:\n",
    "            lemmatized_word = lemmatizer.lemmatize(w)\n",
    "            rootword = stemmer.stem(lemmatized_word)\n",
    "            if rootword not in en_words and lemmatized_word in en_words:\n",
    "                rootword = lemmatized_word\n",
    "            # rootword = w\n",
    "            if rootword not in STOP_WORDS and rootword in en_words:\n",
    "                # lemmatised_words.append(rootword)\n",
    "                if rootword in bow_art:\n",
    "                    bow_art[rootword] += 1\n",
    "                else:\n",
    "                    bow_art[rootword] = 1\n",
    "        # convert to bag of words\n",
    "        # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # for l in lemmatised_words:\n",
    "        #     if l in bow_art:\n",
    "        #         bow_art[l] += 1\n",
    "        #     else:\n",
    "        #         bow_art[l] = 1\n",
    "    \n",
    "    return bow_art\n",
    "\n",
    "def text_to_bow_raw(text):\n",
    "    readable_text = text.lower()\n",
    "    # print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "    # substitute non alphabet chars (new lines become spaces)\n",
    "    readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "    readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "    # sub multiple spaces with one space\n",
    "    readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "    # tokenise text\n",
    "    words = nltk.wordpunct_tokenize(readable_text)\n",
    "    bow_art = {}\n",
    "    # lemmatised_words = []\n",
    "    if len(words) > 0:\n",
    "        # lemmatise, remove non-english, and remove stopwords\n",
    "        for w in words:\n",
    "            rootword = w\n",
    "            if rootword not in STOP_WORDS:\n",
    "                # lemmatised_words.append(rootword)\n",
    "                if rootword in bow_art:\n",
    "                    bow_art[rootword] += 1\n",
    "                else:\n",
    "                    bow_art[rootword] = 1\n",
    "        # convert to bag of words\n",
    "        # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # for l in lemmatised_words:\n",
    "        #     if l in bow_art:\n",
    "        #         bow_art[l] += 1\n",
    "        #     else:\n",
    "        #         bow_art[l] = 1\n",
    "    \n",
    "    return bow_art\n",
    "\n",
    "def text_to_bow_bigram(text):\n",
    "    readable_text = text.lower()\n",
    "    # print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "    # substitute non alphabet chars (new lines become spaces)\n",
    "    readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "    readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "    # sub multiple spaces with one space\n",
    "    readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "    # tokenise text\n",
    "    words = nltk.wordpunct_tokenize(readable_text)\n",
    "    bow_art = {}\n",
    "    # lemmatised_words = []\n",
    "    if len(words) > 0:\n",
    "        # lemmatise, remove non-english, and remove stopwords\n",
    "        prev_word = ''\n",
    "        for w in words:\n",
    "            lemmatized_word = lemmatizer.lemmatize(w)\n",
    "            rootword = stemmer.stem(lemmatized_word)\n",
    "            if rootword not in en_words and lemmatized_word in en_words:\n",
    "                rootword = lemmatized_word\n",
    "            # rootword = w\n",
    "            if rootword not in STOP_WORDS and rootword in en_words:\n",
    "                # lemmatised_words.append(rootword)\n",
    "                if prev_word != '':\n",
    "                    new_bigram = prev_word + \" \" + rootword\n",
    "                    if new_bigram in bow_art:\n",
    "                        bow_art[new_bigram] += 1\n",
    "                    else:\n",
    "                        bow_art[new_bigram] = 1\n",
    "                prev_word = rootword\n",
    "        # convert to bag of words\n",
    "        # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "        # for l in lemmatised_words:\n",
    "        #     if l in bow_art:\n",
    "        #         bow_art[l] += 1\n",
    "        #     else:\n",
    "        #         bow_art[l] = 1\n",
    "    \n",
    "    return bow_art\n",
    "\n",
    "def calc_f(d, sgn):\n",
    "    pos_j = {}  #j occuring in positive article\n",
    "    total_j = {}#j occuring in any article\n",
    "    f = {}      #fraction of positive occurrences\n",
    "    for i in range(len(d)):\n",
    "        for w in d[i]:\n",
    "            # pos_sent = sgn[i]\n",
    "            pos_sent = 0\n",
    "            if (sgn[i] == 1): pos_sent = 1\n",
    "            if w in total_j:\n",
    "                total_j[w] += d[i][w]\n",
    "                pos_j[w] += d[i][w]*pos_sent\n",
    "            else:\n",
    "                total_j[w] = d[i][w]\n",
    "                pos_j[w] = d[i][w]*pos_sent\n",
    "            f[w] = pos_j[w]/total_j[w]\n",
    "    return (pos_j, total_j, f)\n",
    "\n",
    "def gen_sent_word_list(total_j,sgn,f):\n",
    "    pi = sum(sgn_i > 0 for sgn_i in sgn)/len(sgn)\n",
    "    print(pi)\n",
    "    sentiment_words = [] # S\n",
    "    neutral_words = []   # N\n",
    "    for i in total_j:\n",
    "        if ((f[i] >= pi + ALPHA_PLUS or f[i] <= pi - ALPHA_MINUS) and total_j[i] >= KAPPA and len(i) > 1):\n",
    "            sentiment_words.append(i)\n",
    "        else:\n",
    "            neutral_words.append(i)\n",
    "    return(sentiment_words, neutral_words)\n",
    "\n",
    "# Calculates p_i\n",
    "def calc_p(y):\n",
    "    p = [0] * len(y)\n",
    "    for i, x in enumerate(sorted(range(len(y)), key=lambda y_lam: y[y_lam])):\n",
    "        p[x] = float((i+1)/(len(y)))\n",
    "    return p\n",
    "\n",
    "# Calculates s_i\n",
    "def calc_s(sentiment_words, d):\n",
    "    s = []                                          # ith element corresponds to total count of sentiment charged words for document i\n",
    "    d_s = []                                        # ith element corresponds to list of word counts for each of the sentiment charged words for document i\n",
    "    for doc in d:\n",
    "        s.append(sum(doc.get(val,0) for val in sentiment_words))\n",
    "        d_s.append([doc.get(val,0) for val in sentiment_words])\n",
    "    return (s, d_s)\n",
    "\n",
    "# Calculates h_i\n",
    "def calc_h(sentiment_words, d, s, d_s):\n",
    "    h = np.zeros((len(d), len(sentiment_words)))    # ith element corresponds to |S|x1 vector of word frequencies divided by total sentiment words in doc i\n",
    "\n",
    "    for i in range(len(d)):\n",
    "        # subvector of sentiment words in d_i\n",
    "        if (s[i] == 0) :\n",
    "            h[i] = np.zeros(len(sentiment_words)).transpose()\n",
    "        else:\n",
    "            h[i] = np.array([(j/s[i]) for j in d_s[i]]).transpose()\n",
    "    return h\n",
    "\n",
    "# Calculates O\n",
    "def calc_o(p,h):\n",
    "    p_inv = [(1-val) for val in p]\n",
    "    W = np.column_stack((p, p_inv))\n",
    "    W = W.transpose()\n",
    "    ww = np.matmul(W,W.transpose())\n",
    "    w2 = np.matmul(W.transpose(), inv(ww))\n",
    "    O = np.matmul(h.transpose(),w2)\n",
    "    O[O < 0] = 0 # remove negative entries of O\n",
    "    O = O.transpose()\n",
    "    # Normalise O columns to have l1 norm\n",
    "    O[0] = O[0]/np.linalg.norm(O[0], ord=1)\n",
    "    O[1] = O[1]/np.linalg.norm(O[1], ord=1)\n",
    "    # O = sklearn.preprocessing.normalize(O,norm='l1')\n",
    "    O = O.transpose()\n",
    "    return O\n",
    "\n",
    "# lam = 3 is what i normally use\n",
    "def equation_to_solve(p_solve, O, new_bow, sentiment_words, new_s, lam):\n",
    "    # i = 0\n",
    "    # equation = 0\n",
    "\n",
    "    equation = sum([new_bow.get(sentiment_words[i],0)* math.log(p_solve*float(O[i][0]) + (1-p_solve)*float(O[i][1])) for i in range(len(sentiment_words))])/new_s + lam*(p_solve*(1-p_solve))\n",
    "\n",
    "    # for j in sentiment_words:\n",
    "    #     # a = (new_bow.get(j,0) * math.log(new_p*O[i][0] + (1-new_p)*O[i][1]))\n",
    "    #     d_j = new_bow.get(j,0)\n",
    "    #     in_log = p_solve*O[i][0] + (1-p_solve)*O[i][1]\n",
    "    #     if not in_log == 0:\n",
    "    #         equation += d_j * math.log(p_solve*O[i][0] + (1-p_solve)*O[i][1])\n",
    "\n",
    "    #     i += 1\n",
    "    #     # i += 1/new_s + lam * (new_p*(1-new_p))\n",
    "\n",
    "    # if new_s == 0:\n",
    "    #     new_s = 1\n",
    "    # equation /= new_s\n",
    "    # equation += lam*(p_solve*(1-p_solve))\n",
    "\n",
    "    equation *= -1 #flip equation for argmin\n",
    "    return equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here if articles have not been processed\n",
    "Now we will import all of the headlines from kaggle and pull the required stock information to compile a json list of articles like we have normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1400470 lines generating 1397891 usable headlines\n"
     ]
    }
   ],
   "source": [
    "#loop through list of files\n",
    "article_list = []\n",
    "file_name = './archive/analyst_ratings_processed.csv'\n",
    "with open(file_name, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and len(row) == 4:\n",
    "            new_art = {\n",
    "                'headline': row[1],\n",
    "                'date': row[2],\n",
    "                'ticker': row[3]\n",
    "            }\n",
    "            article_list.append(new_art)\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines generating {len(article_list)} usable headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a function to list how many articles we have per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates = [a['date'] for a in article_list]\n",
    "print(f'Min date {min(list_dates)} and max date {max(list_dates)}')\n",
    "for year in range(2009,2021):\n",
    "    year_count = len([a for a in article_list if (a['date'] <= str(year+1) + '-01-01' and a['date'] >= str(year) + '-01-01')])\n",
    "    print(f'No. articles in {year}: {year_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to pull stock information about the tickers. It should already be downlaoded in `processed-data`, but if not, this will generate that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tickers = [a['ticker'] for a in article_list]\n",
    "list_tickers = list(dict.fromkeys(list_tickers))\n",
    "stock_data = {}\n",
    "failed_stocks = []\n",
    "end_date = max(list_dates) + dt.timedelta(days=5)\n",
    "start_date = min(list_dates) - dt.timedelta(days=5)\n",
    "print('pulling stocks...')\n",
    "# data = yf.download(tickers = list_tickers, end=str(end_date.date()), start=str(start_date.date()), progress=True)\n",
    "curr_index = 0\n",
    "TOTAL_TICKERS = len(list_tickers)\n",
    "for t in list_tickers:\n",
    "    arts_ticker = [datetime.strptime(a['date'],'%Y-%m-%d %H:%M:%S%z') for a in article_list if a['ticker'] == t]\n",
    "    # print(type(arts_ticker[0]))\n",
    "    end_date = max(arts_ticker) + dt.timedelta(days=5)\n",
    "    start_date = min(arts_ticker) - dt.timedelta(days=5)\n",
    "    try:\n",
    "        data = yf.download(tickers = t, end=str(end_date.date()), start=str(start_date.date()), progress=False, show_errors=False)\n",
    "        if len(data > 0):\n",
    "            stock_data[t] = data\n",
    "        else:\n",
    "            failed_stocks.append(t)\n",
    "    except:\n",
    "        failed_stocks.append(t)\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_TICKERS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d (%d)\" % ('='*int(20*j), 100*j, curr_index, TOTAL_TICKERS, len(failed_stocks)))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1\n",
    "print(\"Failed stocks = \" + str(failed_stocks))\n",
    "# for a in article_list:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates the aforementioned file so you dont spend 2 hours downloading every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump stock data (probably dont do this tho lol, it takes up a fair bit of space i won't lie)\n",
    "# print(failed_stocks)\n",
    "# stock_list_data = [s.to_json() for s in stock_list_data]\n",
    "# dict_stock = dict(zip(stock_list_tickers, stock_list_data))\n",
    "for s in stock_data:\n",
    "    with open('./processed-data/' + s + '.json', 'w') as json_file:\n",
    "        json.dump(stock_data[s].to_json(), json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stock are private, so we are unable to pull stock information about these tickers. This segment removes any of the articles with these tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list of articles with associated market info...\n"
     ]
    }
   ],
   "source": [
    "print('Generating list of articles with associated market info...')\n",
    "failed_stocks = ['AAN', 'AAV', 'AAVL', 'ABAC', 'ABCW', 'ABDC', 'ABGB', 'ABTL', 'ABX', 'ABY', 'ACAS', 'ACAT', 'ACCU', 'ACE', 'ACG', 'ACHN', 'ACMP', 'ACPW', 'ACSF', 'ACT', 'ACTS', 'ACXM', 'ADAT', 'ADEP', 'ADGE', 'ADHD', 'ADK', 'ADMS', 'ADNC', 'ADRA', 'ADVS', 'AEC', 'AEGN', 'AEGR', 'AEPI', 'AETI', 'AF', 'AFA', 'AFC', 'AFFX', 'AFH', 'AFOP', 'AGC', 'AGII', 'AGN', 'AGNCB', 'AGOL', 'AGU', 'AHC', 'AHP', 'AI', 'AIB', 'AIRM', 'AIXG', 'AKAO', 'AKER', 'AKG', 'AKP', 'AKRX', 'AKS', 'ALDR', 'ALDW', 'ALJ', 'ALLB', 'ALQA', 'ALSK', 'ALTV', 'ALU', 'ALXA', 'ALXN', 'AMAG', 'AMBR', 'AMCC', 'AMCO', 'AMDA', 'AMFW', 'AMIC', 'AMID', 'AMPS', 'AMRB', 'AMRE', 'AMRI', 'AMSG', 'AMTG', 'AMZG', 'ANAC', 'ANAD', 'ANCI', 'AND', 'ANH', 'ANW', 'AOI', 'AOL', 'APAGF', 'APC', 'APF', 'API', 'APL', 'APOL', 'APP', 'APPY', 'APRI', 'APSA', 'AQQ', 'AQXP', 'ARCI', 'ARCX', 'ARDM', 'AREX', 'ARGS', 'ARIA', 'ARIS', 'ARMH', 'ARO', 'ARPI', 'ARQL', 'ARRS', 'ARRY', 'ARTX', 'ASBI', 'ASCMA', 'ASFI', 'ASMI', 'ASNA', 'ASPX', 'AST', 'AT', 'ATE', 'ATHN', 'ATK', 'ATL', 'ATLS', 'ATML', 'ATNY', 'ATRM', 'ATTU', 'ATU', 'ATV', 'ATW', 'AUMA', 'AUMAU', 'AUQ', 'AUXL', 'AV', 'AVG', 'AVH', 'AVHI', 'AVIV', 'AVL', 'AVNR', 'AVOL', 'AVP', 'AVX', 'AXE', 'AXJS', 'AXLL', 'AXN', 'AXPW', 'AXX', 'AYR', 'AZIA', 'BAA', 'BABS', 'BABY', 'BAF', 'BAGR', 'BALT', 'BAMM', 'BAS', 'BASI', 'BBCN', 'BBF', 'BBG', 'BBK', 'BBLU', 'BBNK', 'BBRC', 'BBRY', 'BBT', 'BBX', 'BCA', 'BCOM', 'BCR', 'BDBD', 'BDCV', 'BDE', 'BDGE', 'BEAT', 'BEE', 'BEL', 'BF', 'BFR', 'BFY', 'BGCA', 'BGG', 'BHBK', 'BHI', 'BHL', 'BID', 'BIK', 'BIN', 'BIND', 'BIOA', 'BIOD', 'BIOS', 'BIRT', 'BITA', 'BKJ', 'BKK', 'BKMU', 'BKS', 'BKYF', 'BLOX', 'BLT', 'BLVD', 'BLVDU', 'BMR', 'BMTC', 'BNCL', 'BNCN', 'BOBE', 'BOCH', 'BOFI', 'BONA', 'BONE', 'BONT', 'BORN', 'BOTA', 'BOXC', 'BPFH', 'BPFHW', 'BPI', 'BPL', 'BPOPN', 'BQH', 'BRAF', 'BRAQ', 'BRAZ', 'BRCD', 'BRCM', 'BRDR', 'BREW', 'BRK', 'BRKS', 'BRLI', 'BRSS', 'BRXX', 'BSCG', 'BSD', 'BSDM', 'BSE', 'BSFT', 'BSI', 'BSTC', 'BT', 'BTE', 'BTUI', 'BUNL', 'BUNT', 'BVA', 'BVSN', 'BVX', 'BWC', 'BWINA', 'BWINB', 'BWLD', 'BWS', 'BXE', 'BXS', 'BZC', 'BZM', 'CAB', 'CACGU', 'CACQ', 'CADC', 'CADT', 'CAFE', 'CAK', 'CAM', 'CAP', 'CAPN', 'CARB', 'CARO', 'CART', 'CAS', 'CASM', 'CATM', 'CAW', 'CBAK', 'CBB', 'CBDE', 'CBF', 'CBG', 'CBIN', 'CBK', 'CBLI', 'CBM', 'CBMG', 'CBMX', 'CBNJ', 'CBPO', 'CBPX', 'CBR', 'CBRX', 'CBS', 'CBSHP', 'CBST', 'CCC', 'CCCL', 'CCCR', 'CCE', 'CCG', 'CCSC', 'CCV', 'CCX', 'CCXE', 'CDC', 'CDI', 'CECO', 'CEL', 'CELGZ', 'CEMP', 'CERE', 'CERU', 'CETV', 'CFD', 'CFN', 'CFNL', 'CFP', 'CFRXW', 'CFRXZ', 'CGG', 'CGI', 'CGIX', 'CH', 'CHA', 'CHEV', 'CHFC', 'CHK', 'CHKE', 'CHL', 'CHLN', 'CHMT', 'CHOC', 'CHOP', 'CHSP', 'CHU', 'CHXF', 'CHYR', 'CIE', 'CIFC', 'CIMT', 'CISG', 'CIU', 'CJES', 'CKEC', 'CKH', 'CKP', 'CKSW', 'CLAC', 'CLC', 'CLCT', 'CLD', 'CLDN', 'CLGX', 'CLI', 'CLMS', 'CLNT', 'CLNY', 'CLRX', 'CLTX', 'CLUB', 'CLY', 'CMCSK', 'CMD', 'CMFN', 'CMGE', 'CMLP', 'CMN', 'CMSB', 'CNBKA', 'CNCO', 'CNDA', 'CNDO', 'CNIT', 'CNNX', 'CNTF', 'CNV', 'CNW', 'CNYD', 'COB', 'COBK', 'COCO', 'CODE', 'COH', 'COOL', 'COR', 'CORE', 'COSI', 'COT', 'COV', 'COVR', 'COVS', 'CPAH', 'CPGI', 'CPHD', 'CPHR', 'CPL', 'CPN', 'CPST', 'CPTA', 'CRAY', 'CRBQ', 'CRC', 'CRCM', 'CRD', 'CRDC', 'CRDS', 'CRDT', 'CRED', 'CREE', 'CRME', 'CRR', 'CRRC', 'CRRS', 'CRV', 'CRWN', 'CRZO', 'CSC', 'CSFL', 'CSG', 'CSH', 'CSJ', 'CSOD', 'CSRE', 'CSS', 'CST', 'CSUN', 'CTCT', 'CTF', 'CTL', 'CTNN', 'CTRL', 'CTRX', 'CTV', 'CTWS', 'CU', 'CUB', 'CUI', 'CUNB', 'CUO', 'CUR', 'CVA', 'CVC', 'CVD', 'CVOL', 'CVSL', 'CVTI', 'CWEI', 'CXA', 'CXO', 'CXP', 'CY', 'CYBX', 'CYN', 'CYNI', 'CYOU', 'CYT', 'CYTX', 'CZFC', 'CZZ', 'DAEG', 'DAKP', 'DANG', 'DARA', 'DATA', 'DATE', 'DBBR', 'DBMX', 'DBU', 'DBUK', 'DCA', 'DCIX', 'DCM', 'DCT', 'DDC', 'DDR', 'DEG', 'DEJ', 'DEPO', 'DEST', 'DF', 'DFRG', 'DFT', 'DGAS', 'DGI', 'DGSE', 'DHRM', 'DIVI', 'DKT', 'DLBL', 'DLPH', 'DM', 'DMD', 'DMND', 'DNB', 'DNBF', 'DNKN', 'DNO', 'DNR', 'DO', 'DOM', 'DOVR', 'DPLO', 'DPM', 'DPRX', 'DPW', 'DRAD', 'DRAM', 'DRC', 'DRII', 'DRL', 'DRNA', 'DRWI', 'DRYS', 'DSCI', 'DSCO', 'DSE', 'DSKX', 'DSKY', 'DSUM', 'DTLK', 'DTSI', 'DTV', 'DUC', 'DV', 'DVCR', 'DVD', 'DW', 'DWA', 'DWRE', 'DWTI', 'DXB', 'DXJF', 'DXJR', 'DXKW', 'DXM', 'DXPS', 'DYAX', 'DYN', 'EAC', 'EBIO', 'EBSB', 'ECA', 'ECR', 'ECT', 'ECTE', 'EDE', 'EDR', 'EDS', 'EE', 'EEHB', 'EEI', 'EEME', 'EEML', 'EFF', 'EFII', 'EFUT', 'EGAS', 'EGI', 'EGLT', 'EGOV', 'EGRW', 'EGT', 'EHIC', 'EIGI', 'EIV', 'EJ', 'ELGX', 'ELLI', 'ELNK', 'ELOS', 'ELRC', 'ELX', 'EMBB', 'EMCD', 'EMCI', 'EMCR', 'EMDI', 'EMES', 'EMEY', 'EMQ', 'EMSA', 'EMXX', 'ENBL', 'ENFC', 'ENGN', 'ENH', 'ENI', 'ENL', 'ENOC', 'ENRJ', 'ENT', 'ENVI', 'ENY', 'ENZY', 'EOC', 'EOPN', 'EOX', 'EPAX', 'EPE', 'EPIQ', 'EPRS', 'EQM', 'EQY', 'ERA', 'ERB', 'ERO', 'EROS', 'ERS', 'ESBF', 'ESCR', 'ESL', 'ESR', 'ESSX', 'ESV', 'ESYS', 'ETAK', 'ETE', 'ETF', 'ETFC', 'ETH', 'ETM', 'ETRM', 'EU', 'EV', 'EVAL', 'EVAR', 'EVBS', 'EVDY', 'EVEP', 'EVJ', 'EVLV', 'EVRY', 'EWCS', 'EWHS', 'EWSS', 'EXA', 'EXAM', 'EXAR', 'EXFO', 'EXL', 'EXLP', 'EXXI', 'FAC', 'FAV', 'FBNK', 'FBSS', 'FCAU', 'FCE', 'FCF', 'FCH', 'FCHI', 'FCLF', 'FCS', 'FCSC', 'FDEF', 'FDI', 'FDML', 'FDO', 'FEIC', 'FELP', 'FES', 'FEYE', 'FFG', 'FGP', 'FHCO', 'FHY', 'FI', 'FIG', 'FISH', 'FLIR', 'FLML', 'FLTX', 'FLXN', 'FLY', 'FMD', 'FMER', 'FNBC', 'FNFG', 'FNFV', 'FNJN', 'FNSR', 'FOMX', 'FPO', 'FPRX', 'FRAN', 'FRED', 'FREE', 'FRM', 'FRP', 'FRS', 'FRSH', 'FSAM', 'FSBK', 'FSC', 'FSGI', 'FSIC', 'FSL', 'FSNN', 'FSRV', 'FSYS', 'FTD', 'FTR', 'FTT', 'FUEL', 'FULL', 'FUR', 'FWM', 'FWV', 'FXCB', 'FXCM', 'GAI', 'GAINO', 'GALTU', 'GARS', 'GBSN', 'GCA', 'GCAP', 'GCH', 'GCVRZ', 'GDAY', 'GDEF', 'GDF', 'GDP', 'GEVA', 'GFA', 'GFIG', 'GFNCP', 'GFY', 'GG', 'GGAC', 'GGE', 'GGM', 'GGOV', 'GGP', 'GHDX', 'GHI', 'GIG', 'GIMO', 'GK', 'GKNT', 'GLDC', 'GLDX', 'GLOG', 'GLPW', 'GLUU', 'GMCR', 'GMFS', 'GMK', 'GMLP', 'GMO', 'GMT', 'GMZ', 'GNC', 'GNI', 'GNMK', 'GNVC', 'GOMO', 'GOODO', 'GOODP', 'GOV', 'GPIC', 'GPM', 'GPOR', 'GPX', 'GRAM', 'GRH', 'GRIF', 'GRN', 'GRO', 'GRT', 'GSB', 'GSH', 'GSI', 'GSOL', 'GST', 'GSVC', 'GTAA', 'GTI', 'GTIV', 'GTT', 'GTU', 'GTWN', 'GTXI', 'GUID', 'GUR', 'GURX', 'GWL', 'GWPH', 'GWR', 'GY', 'GYEN', 'GZT', 'HABT', 'HAR', 'HAWKB', 'HBHC', 'HBK', 'HBNK', 'HBOS', 'HCAC', 'HCAP', 'HCBK', 'HCHC', 'HCLP', 'HCN', 'HCOM', 'HCP', 'HDRA', 'HDRAU', 'HDS', 'HDY', 'HEB', 'HELI', 'HEOP', 'HF', 'HFBC', 'HFFC', 'HGG', 'HGI', 'HGR', 'HGT', 'HH', 'HIIQ', 'HILL', 'HILO', 'HK', 'HKOR', 'HKTV', 'HLS', 'HLSS', 'HME', 'HMPR', 'HMSY', 'HNH', 'HNR', 'HNSN', 'HNT', 'HOS', 'HOTRW', 'HPJ', 'HPT', 'HPTX', 'HPY', 'HRC', 'HRS', 'HRT', 'HSEA', 'HSGX', 'HSNI', 'HSOL', 'HSP', 'HTCH', 'HTF', 'HTR', 'HTS', 'HTWO', 'HTWR', 'HTZ', 'HUB', 'HVB', 'HW', 'HWAY', 'HWCC', 'HYGS', 'HYH', 'IACI', 'IBCA', 'IBCC', 'IBKC', 'IBLN', 'ICA', 'ICB', 'ICEL', 'ICI', 'ICN', 'ICON', 'IDHB', 'IDI', 'IDSY', 'IDTI', 'IDXJ', 'IEC', 'IFMI', 'IFON', 'IFT', 'IG', 'IGLD', 'IGTE', 'IGU', 'IID', 'IILG', 'IJNK', 'IKAN', 'IKGH', 'IKNX', 'IL', 'IM', 'IMDZ', 'IMI', 'IMMU', 'IMN', 'IMNP', 'IMPR', 'IMRS', 'IMS', 'IMUC', 'INAP', 'INB', 'INCR', 'IND', 'INF', 'INFA', 'ININ', 'INP', 'INPH', 'INS', 'INSY', 'INTL', 'INVN', 'INWK', 'INXN', 'INXX', 'INY', 'IOC', 'IOIL', 'IOT', 'IPCI', 'IPCM', 'IPD', 'IPF', 'IPHS', 'IPK', 'IPU', 'IPW', 'IQNT', 'IRC', 'IRDMB', 'IRDMZ', 'IRE', 'IRET', 'IRF', 'IRR', 'ISCA', 'ISF', 'ISH', 'ISIL', 'ISIS', 'ISLE', 'ISNS', 'ISRL', 'ISSI', 'IST', 'ITC', 'ITF', 'ITG', 'ITLT', 'ITLY', 'ITR', 'IVAN', 'IVOP', 'IXYS', 'JAH', 'JASN', 'JASO', 'JAXB', 'JCOM', 'JCP', 'JDD', 'JDSU', 'JEC', 'JFC', 'JGBD', 'JGBL', 'JGBS', 'JGBT', 'JGW', 'JIVE', 'JJA', 'JJM', 'JJN', 'JJP', 'JJT', 'JJU', 'JMEI', 'JMLP', 'JMP', 'JNS', 'JO', 'JOEZ', 'JONE', 'JOY', 'JPEP', 'JPP', 'JRN', 'JSC', 'JST', 'JTA', 'JTD', 'JTP', 'JUNR', 'JW', 'JYN', 'KATE', 'KBIO', 'KBSF', 'KBWC', 'KBWI', 'KCAP', 'KCC', 'KCG', 'KEF', 'KEG', 'KEM', 'KFH', 'KFI', 'KFX', 'KHI', 'KIN', 'KITE', 'KKD', 'KME', 'KNL', 'KNM', 'KONA', 'KONE', 'KOOL', 'KORS', 'KROO', 'KRU', 'KST', 'KSU', 'KTEC', 'KUTV', 'KWT', 'KYO', 'KYTH', 'KZ', 'LABC', 'LABL', 'LACO', 'LAS', 'LBF', 'LBIX', 'LBMH', 'LBY', 'LDL', 'LDR', 'LDRH', 'LEI', 'LEVY', 'LEVYU', 'LG', 'LGCY', 'LGF', 'LINE', 'LION', 'LIOX', 'LIQD', 'LLDM', 'LLEM', 'LLEX', 'LLSC', 'LLTC', 'LM', 'LMCA', 'LMCB', 'LMCK', 'LMIA', 'LMLP', 'LMNX', 'LMOS', 'LMRK', 'LNBB', 'LNCO', 'LNKD', 'LOCK', 'LOCM', 'LOGM', 'LOJN', 'LONG', 'LOOK', 'LORL', 'LPHI', 'LPT', 'LPTN', 'LRAD', 'LRE', 'LSC', 'LSG', 'LTM', 'LTS', 'LTXB', 'LUX', 'LVLT', 'LVNTA', 'LWC', 'LXFT', 'LXK', 'MAGS', 'MAMS', 'MBFI', 'MBLX', 'MBLY', 'MBRG', 'MBTF', 'MBVT', 'MCC', 'MCF', 'MCGC', 'MCOX', 'MCP', 'MCRL', 'MCUR', 'MCV', 'MCZ', 'MDAS', 'MDCA', 'MDCO', 'MDD', 'MDGN', 'MDLY', 'MDP', 'MDSO', 'MDSY', 'MDVN', 'MDVXU', 'MDW', 'MEA', 'MEET', 'MEG', 'MELA', 'MELR', 'MEN', 'MENT', 'MEP', 'MERU', 'MES', 'METR', 'MFI', 'MFLX', 'MFNC', 'MFRI', 'MFRM', 'MFSF', 'MFT', 'MGCD', 'MGH', 'MGLN', 'MGN', 'MGT', 'MHE', 'MHFI', 'MHGC', 'MHR', 'MIE', 'MIFI', 'MIK', 'MIL', 'MILL', 'MINI', 'MJN', 'MKTO', 'MLHR', 'MLNK', 'MLNX', 'MLPJ', 'MLPL', 'MM', 'MMAC', 'MNE', 'MNGA', 'MNI', 'MNK', 'MNRK', 'MNTA', 'MOBI', 'MOC', 'MOCO', 'MOG', 'MOKO', 'MOLG', 'MON', 'MONY', 'MORE', 'MPEL', 'MPET', 'MPO', 'MRD', 'MRH', 'MRKT', 'MRVC', 'MSBF', 'MSF', 'MSG', 'MSLI', 'MSO', 'MSON', 'MSP', 'MSTX', 'MTK', 'MTS', 'MTSC', 'MTSL', 'MTSN', 'MTT', 'MTU', 'MUH', 'MUS', 'MVC', 'MVG', 'MVNR', 'MW', 'MWE', 'MWIV', 'MWV', 'MXIM', 'MXWL', 'MY', 'MYCC', 'MYF', 'MYL', 'MYOS', 'MZF', 'NADL', 'NAME', 'NANO', 'NAO', 'NATL', 'NAV', 'NBBC', 'NBG', 'NBL', 'NBS', 'NBTF', 'NCB', 'NCFT', 'NCI', 'NCIT', 'NCQ', 'NDRO', 'NE', 'NEOT', 'NETE', 'NEWM', 'NEWS', 'NFEC', 'NGHC', 'NGHCP', 'NGLS', 'NHF', 'NHTB', 'NJ', 'NJV', 'NKA', 'NKY', 'NLNK', 'NMBL', 'NMO', 'NMRX', 'NMY', 'NNA', 'NNC', 'NOR', 'NORD', 'NPBC', 'NPD', 'NPP', 'NPSP', 'NRCIA', 'NRF', 'NRX', 'NSAM', 'NSH', 'NSPH', 'NSR', 'NTI', 'NTK', 'NTL', 'NTLS', 'NTN', 'NTRSP', 'NTT', 'NTX', 'NU', 'NUTR', 'NVDQ', 'NVGN', 'NVSL', 'NVX', 'NWHM', 'NWY', 'NXQ', 'NXR', 'NXTDW', 'NXTM', 'NYLD', 'NYMTP', 'NYNY', 'NYV', 'OAK', 'OAKS', 'OB', 'OCIR', 'OCLS', 'OCR', 'OCRX', 'OGXI', 'OHAI', 'OHGI', 'OHRP', 'OIBR', 'OILT', 'OKS', 'OKSB', 'OLO', 'OMAM', 'OME', 'OMED', 'OMG', 'OMN', 'ONEF', 'ONFC', 'ONNN', 'ONP', 'ONTY', 'ONVI', 'OPB', 'OPHT', 'OPWR', 'OPXA', 'ORB', 'ORBC', 'ORBK', 'OREX', 'ORIT', 'ORM', 'ORPN', 'OSGB', 'OSHC', 'OSIR', 'OSM', 'OSN', 'OTEL', 'OTIV', 'OUTR', 'OVTI', 'OWW', 'OXFD', 'OXLCO', 'OZM', 'OZRK', 'PACD', 'PAF', 'PAGG', 'PAH', 'PAL', 'PARN', 'PAY', 'PBCP', 'PBIB', 'PBM', 'PBMD', 'PBY', 'PCI', 'PCL', 'PCLN', 'PCMI', 'PCO', 'PCP', 'PCYC', 'PDII', 'PDLI', 'PE', 'PEGI', 'PEIX', 'PENX', 'PEOP', 'PER', 'PERF', 'PERM', 'PES', 'PETM', 'PETX', 'PFBI', 'PFK', 'PFNX', 'PFPT', 'PGI', 'PGM', 'PGN', 'PGNX', 'PHF', 'PHII', 'PHIIK', 'PHMD', 'PICO', 'PIH', 'PIP', 'PIR', 'PJC', 'PKD', 'PKO', 'PKY', 'PLCM', 'PLKI', 'PLMT', 'PLND', 'PLNR', 'PLPM', 'PLT', 'PLTM', 'PMBC', 'PMC', 'PMCS', 'PMFG', 'PNRA', 'PNTR', 'PNX', 'POL', 'POM', 'POPE', 'POT', 'POWR', 'POZN', 'PPHM', 'PPHMP', 'PPO', 'PPP', 'PPR', 'PPS', 'PQ', 'PRAH', 'PRAN', 'PRB', 'PRCP', 'PRE', 'PRGN', 'PRGX', 'PRLS', 'PRSC', 'PRTO', 'PRXI', 'PRXL', 'PRY', 'PSAU', 'PSBH', 'PSDV', 'PSEM', 'PSG', 'PSTB', 'PSTR', 'PSUN', 'PTBI', 'PTIE', 'PTLA', 'PTM', 'PTP', 'PTRY', 'PTX', 'PULB', 'PVA', 'PVTB', 'PWE', 'PWRD', 'PWX', 'PXMC', 'PXR', 'PXSC', 'PZI', 'Q', 'QADA', 'QCAN', 'QDEM', 'QDXU', 'QEH', 'QEM', 'QEP', 'QEPM', 'QGBR', 'QIHU', 'QKOR', 'QLGC', 'QLIK', 'QLTB', 'QLTC', 'QLTI', 'QLTY', 'QSII', 'QTM', 'QTS', 'QTWN', 'QTWW', 'QUNR', 'QVCA', 'QVCB', 'QXUS', 'RAI', 'RALY', 'RAS', 'RATE', 'RAVN', 'RAX', 'RBC', 'RBL', 'RBPAA', 'RBS', 'RBY', 'RCAP', 'RCPI', 'RCPT', 'RDC', 'RDEN', 'RDS', 'RECN', 'REE', 'REMY', 'REN', 'RENT', 'RESI', 'REXI', 'REXX', 'RFT', 'RGDO', 'RGDX', 'RGSE', 'RHP', 'RHS', 'RHT', 'RIC', 'RICE', 'RIF', 'RIGP', 'RIO', 'RIOM', 'RIT', 'RIVR', 'RJET', 'RJF', 'RKT', 'RKUS', 'RLD', 'RLH', 'RLOC', 'RLOG', 'RLYP', 'RNA', 'RNDY', 'RNET', 'RNF', 'RNN', 'ROC', 'ROIAK', 'ROIQ', 'ROIQU', 'ROIQW', 'ROKA', 'RORO', 'ROSE', 'ROVI', 'ROX', 'RP', 'RPAI', 'RPRX', 'RPRXW', 'RPRXZ', 'RPTP', 'RPX', 'RRST', 'RSE', 'RSH', 'RSO', 'RST', 'RSTI', 'RT', 'RTEC', 'RTGN', 'RTI', 'RTIX', 'RTK', 'RTR', 'RTRX', 'RUBI', 'RUK', 'RVBD', 'RVLT', 'RVM', 'RWC', 'RWV', 'RWXL', 'RXDX', 'RXII', 'RYL', 'S', 'SAAS', 'SAEX', 'SAJA', 'SALT', 'SAPE', 'SARA', 'SBBX', 'SBGL', 'SBRAP', 'SBSA', 'SBY', 'SCAI', 'SCHB', 'SCHF', 'SCHL', 'SCHP', 'SCLN', 'SCMP', 'SCOK', 'SCPB', 'SCSC', 'SCSS', 'SCTY', 'SCU', 'SCVL', 'SCZ', 'SD', 'SDLP', 'SDR', 'SDRL', 'SDT', 'SE', 'SEIC', 'SEMG', 'SEMI', 'SERV', 'SEV', 'SFB', 'SFG', 'SFL', 'SFLA', 'SFLY', 'SFN', 'SFNC', 'SFS', 'SFXE', 'SGAR', 'SGB', 'SGBK', 'SGF', 'SGG', 'SGM', 'SGNL', 'SGNT', 'SGOC', 'SGY', 'SGYP', 'SGYPU', 'SGYPW', 'SHLD', 'SHLO', 'SHM', 'SHOR', 'SHOS', 'SIAL', 'SIBC', 'SIFI', 'SIGM', 'SIMG', 'SINA', 'SINO', 'SIRO', 'SKBI', 'SKH', 'SKIS', 'SKUL', 'SLCT', 'SLH', 'SLI', 'SLTC', 'SLW', 'SLXP', 'SMACU', 'SMI', 'SMK', 'SMRT', 'SMTP', 'SMTX', 'SN', 'SNAK', 'SNBC', 'SNC', 'SNDK', 'SNH', 'SNOW', 'SNR', 'SNSS', 'SNTA', 'SORL', 'SPA', 'SPAN', 'SPAR', 'SPEX', 'SPF', 'SPKE', 'SPLS', 'SPNC', 'SPP', 'SPPR', 'SPPRO', 'SPRT', 'SPU', 'SPW', 'SQBG', 'SQBK', 'SQI', 'SQNM', 'SRSC', 'SSE', 'SSFN', 'SSH', 'SSI', 'SSLT', 'SSN', 'SSNI', 'SSRG', 'SSRI', 'SSS', 'SSW', 'STAY', 'STCK', 'STEM', 'STI', 'STJ', 'STML', 'STMP', 'STNR', 'STO', 'STR', 'STRN', 'STRZA', 'STRZB', 'STS', 'SUBK', 'SUNE', 'SUSQ', 'SUTR', 'SVLC', 'SWHC', 'SWY', 'SXCP', 'SXE', 'SYA', 'SYKE', 'SYMC', 'SYMX', 'SYNC', 'SYRG', 'SYRX', 'SYT', 'SYUT', 'SYX', 'SZYM', 'TAHO', 'TAOM', 'TAS', 'TASR', 'TAT', 'TAX', 'TAXI', 'TBAR', 'TBIO', 'TCAP', 'TCBIP', 'TCCA', 'TCHI', 'TCK', 'TCO', 'TCP', 'TCPI', 'TCRD', 'TDA', 'TDD', 'TDI', 'TE', 'TEAR', 'TECD', 'TECU', 'TEG', 'TERP', 'TESO', 'TEU', 'TFM', 'TFSCU', 'TGC', 'TGD', 'TGE', 'THOR', 'THRX', 'THTI', 'TI', 'TICC', 'TIF', 'TIK', 'TIME', 'TINY', 'TISA', 'TIVO', 'TKAI', 'TKMR', 'TLF', 'TLI', 'TLL', 'TLLP', 'TLM', 'TLMR', 'TLO', 'TLP', 'TLR', 'TMH', 'TMK', 'TNAV', 'TNDQ', 'TNGO', 'TOF', 'TOO', 'TOT', 'TOWR', 'TPI', 'TPLM', 'TPRE', 'TPUB', 'TRAK', 'TRCB', 'TRCH', 'TRCO', 'TRF', 'TRGT', 'TRIL', 'TRIV', 'TRK', 'TRLA', 'TRMR', 'TRND', 'TROV', 'TROVU', 'TROVW', 'TRR', 'TRTL', 'TRW', 'TRXC', 'TSL', 'TSLF', 'TSO', 'TSRA', 'TSRE', 'TSS', 'TST', 'TSU', 'TSYS', 'TTF', 'TTFS', 'TTHI', 'TTPH', 'TTS', 'TUBE', 'TUES', 'TVIZ', 'TWC', 'TWMC', 'TXTR', 'TYC', 'TYPE', 'UACL', 'UAM', 'UBC', 'UBIC', 'UBNK', 'UBSH', 'UCD', 'UCFC', 'UCP', 'UDF', 'UFS', 'UHN', 'UIL', 'ULTI', 'ULTR', 'UMX', 'UN', 'UNIS', 'UNT', 'UNXL', 'UPIP', 'UPL', 'URZ', 'USAG', 'USAT', 'USBI', 'USCR', 'USG', 'USMD', 'USMI', 'USTR', 'UTEK', 'UTIW', 'UWTI', 'VA', 'VAL', 'VAR', 'VCO', 'VDSI', 'VGGL', 'VIAB', 'VIAS', 'VICL', 'VIEW', 'VII', 'VIMC', 'VIP', 'VISI', 'VISN', 'VLCCF', 'VLTC', 'VMEM', 'VNTV', 'VOLC', 'VRD', 'VRML', 'VRNG', 'VRTB', 'VRTU', 'VSAR', 'VSCI', 'VSCP', 'VSI', 'VSLR', 'VSR', 'VTAE', 'VTG', 'VTL', 'VTSS', 'VTTI', 'VVUS', 'VYFC', 'WAC', 'WAGE', 'WAIR', 'WAVX', 'WBAI', 'WBC', 'WBIH', 'WBMD', 'WCG', 'WDR', 'WDTI', 'WEBK', 'WEET', 'WFBI', 'WFD', 'WFM', 'WFT', 'WG', 'WGA', 'WGBS', 'WGP', 'WHX', 'WHZ', 'WIBC', 'WIFI', 'WILN', 'WIN', 'WITE', 'WLB', 'WLH', 'WLRHU', 'WLT', 'WMAR', 'WMGI', 'WMLP', 'WNR', 'WNRL', 'WPCS', 'WPG', 'WPPGY', 'WPT', 'WPX', 'WR', 'WRES', 'WSH', 'WSTC', 'WTR', 'WTSL', 'WUBA', 'WWAV', 'WYN', 'XBKS', 'XCO', 'XEC', 'XGTI', 'XGTIW', 'XIV', 'XL', 'XLRN', 'XLS', 'XNY', 'XON', 'XONE', 'XOOM', 'XOVR', 'XRA', 'XRS', 'XUE', 'XXV', 'YAO', 'YDKN', 'YDLE', 'YGE', 'YHOO', 'YOD', 'YOKU', 'YRCW', 'YUMA', 'YUME', 'YZC', 'ZA', 'ZAGG', 'ZAYO', 'ZBB', 'ZFC', 'ZFGN', 'ZINC', 'ZIONW', 'ZIXI', 'ZLTQ', 'ZMH', 'ZN', 'ZPIN', 'ZQK', 'ZSPH', 'ZU', 'ZX']\n",
    "article_list_updated = [a for a in article_list if not a['ticker'] in failed_stocks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment pulls the stock data from the file rather than YAHOO FINANCE API. It's much much much quicker so if you have the file, run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate list of stock information\n",
    "ticker_path = './processed-data/'\n",
    "pathlist = Path(ticker_path).rglob('*.json')\n",
    "stock_data = {}\n",
    "i = 0\n",
    "for path in pathlist:\n",
    "    with open(str(path)) as json_file:\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"pulling file no %d\" % (i))\n",
    "        sys.stdout.flush()#\n",
    "        i += 1\n",
    "        data = json.load(json_file)\n",
    "        data = json.loads(data)\n",
    "        datetime_dict = {}\n",
    "        for line in data:\n",
    "            datetime_line = {}\n",
    "            if line == 'Close':\n",
    "                for k in data[line]:\n",
    "                    # print(str(datetime.fromtimestamp(float(k)/1000.0)))\n",
    "                    datetime_line[datetime.fromtimestamp(float(k)/1000.0).date()] = data[line][k]\n",
    "                datetime_dict[line] = datetime_line \n",
    "        stock_data[os.path.basename(str(path))[:-5]] = DataFrame.from_dict(datetime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the stock data all pulled, we now need to assign these to the appropriate article. It's important to note that there is not stock market information available for every single day, so if there is a day missing, we just take the next available day with information and call this day $t$. Doing this for each article is incredibly time consuming and takes around 30 hours, so instead, we create a lookup table here. Inside, it has the adjusted stock data required using the stock ticker and the date the article was released as the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'day_t_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joshf\\Documents\\thesis\\code\\kaggle\\kaggle.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(day_t_data\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'day_t_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(day_t_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100% 4151 out of 4152"
     ]
    }
   ],
   "source": [
    "day_t_data = {}\n",
    "curr_index = 0\n",
    "TOTAL_ARTS = len(stock_data)\n",
    "for t in stock_data:\n",
    "    # print(t)\n",
    "    ticker_date_data = {}\n",
    "    start = min(stock_data[t].index)\n",
    "    end = max(stock_data[t].index)\n",
    "    curr_date = start\n",
    "    while curr_date < end:\n",
    "        day_t = curr_date\n",
    "        while not day_t in stock_data[t].index and day_t <= end:\n",
    "            day_t += dt.timedelta(days=1)\n",
    "        from_stock = day_t - dt.timedelta(days=2)\n",
    "        to_stock = day_t + dt.timedelta(days=1)\n",
    "        while not from_stock in stock_data[t].index and from_stock >= start:\n",
    "            from_stock -= dt.timedelta(days=1)\n",
    "        while not to_stock in stock_data[t].index and to_stock <= end:\n",
    "            to_stock += dt.timedelta(days=1)\n",
    "        if(from_stock > start and to_stock < end):\n",
    "            ticker_date_data[curr_date] = {'day_t': day_t, 'from_stock': stock_data[t]['Close'][from_stock], 'to_stock': stock_data[t]['Close'][to_stock]}\n",
    "        curr_date += dt.timedelta(days=1)\n",
    "    day_t_data[t] = ticker_date_data\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_ARTS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d\" % ('='*int(20*j), 100*j, curr_index, TOTAL_ARTS))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the tools at our disposal, we now need to compile the updated list of articles. This segment puts them in a file for later use, so we don't even need to do any of the things previously ever again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list_mrkt = []\n",
    "print(len(article_list))\n",
    "print(len(article_list_updated))\n",
    "curr_index = 0\n",
    "TOTAL_ARTS = len(article_list_updated)\n",
    "print('Done, assigning stock data to articles...')\n",
    "with open('archive/analyst_ratings_mrkt.csv', 'w',newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['headline','date','ticker','open','close'])\n",
    "    for a in article_list_updated:\n",
    "        article_date = datetime.strptime(a['date'], '%Y-%m-%d %H:%M:%S%z')\n",
    "        print(day_t)\n",
    "        if (article_date.hour > 15):\n",
    "            day_t = day_t + dt.timedelta(days=1)\n",
    "        if a['ticker'] in list(day_t_data.keys()):\n",
    "            if (day_t in day_t_data[a['ticker']]):\n",
    "                day_t = day_t_data[a['ticker']][day_t]['day_t']\n",
    "                from_stock = day_t_data[a['ticker']][day_t]['from_stock']\n",
    "                to_stock = day_t_data[a['ticker']][day_t]['to_stock']\n",
    "                new_art = {\n",
    "                    'headline': a['headline'],\n",
    "                    'mrkt_info': {\n",
    "                        'open': from_stock,\n",
    "                        'close': to_stock\n",
    "                    },\n",
    "                    'date': a['date'],\n",
    "                    'ticker': a['ticker']\n",
    "                }\n",
    "                csvwriter.writerow([a['headline'], a['date'], a['ticker'], str(from_stock), str(to_stock)])\n",
    "                # article_list_mrkt.append(new_art)\n",
    "    # else:\n",
    "    #     print(\"ticker \" + t + \" date \" + str(day_t) + \" not in range\")\n",
    "    # sys.stdout.write('\\r')\n",
    "    # j = (curr_index + 1) / TOTAL_ARTS\n",
    "    # sys.stdout.write(\"%d out of %d articles processed\" % (curr_index, TOTAL_ARTS))\n",
    "    # sys.stdout.flush()\n",
    "    # curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# START HERE IF `archive/analyst_ratings_mrkt.csv` EXISTS\n",
    "Now that we have everything at our disposal, we can actually begin testing some things. Let's first read the file into the notebook so we can begin doing some computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1027533 lines generating 1027533 usable headlines\n"
     ]
    }
   ],
   "source": [
    "# import market data articles\n",
    "#loop through list of files\n",
    "article_list = []\n",
    "file_name = './archive/analyst_ratings_mrkt.csv'\n",
    "with open(file_name, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            new_art = {\n",
    "                'headline': row[0],\n",
    "                'date': datetime.strptime(row[1], '%Y-%m-%d %H:%M:%S%z'),\n",
    "                'mrkt_info': {\n",
    "                    'open': row[3],\n",
    "                    'close': row[4]\n",
    "                },\n",
    "                'ticker': row[2]\n",
    "            }\n",
    "            article_list.append(new_art)\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count-1} lines generating {len(article_list)} usable headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The paper 'predicting stock returns with text data' used a rolling window method to train and validate window. They train models of the matrix and minimise a loss function. The tuning parameters are $(\\alpha_+, \\alpha_-, \\kappa, \\lambda)$. In each case, a limited number of choices were used.\n",
    "- For the two $\\alpha$ params, they are set such that the number of words in each group (both positive and negative) is either 25, 50, or 100.\n",
    "- For $\\kappa$, five choices are considered, at: 86%, 88%, 90%, 92% and 94% quantiles of the count distribution each year\n",
    "- For $\\lambda$, three choices: 1, 5, and 10\n",
    "- This totals 45 configurations\n",
    "\n",
    "The loss function they used is the $\\ell^1$-norm of the differences between estimated article sentiment scores and the corresponding standardised return ranks for all events in the validation sample.\n",
    "\n",
    "They had data all the way from 1989, but we don't have that luxury, with articles spanning 2009-2020. For this reason, we will use slightly different window sizes. We will attempt to estimate and validate the model around 20 times, meaning the whole window will be a three years total total. The training sample will be two years and the validation the following year. We then move the window along by four months, giving us 19 models total. Importantly, the articles from 2019-05-01 are not used in either window, so are out of sample articles that could be used for testing models later on.\n",
    "\n",
    "**IMPORTANT NOTES**\n",
    "- It is entirely possible that the news articles in a certain month will carry certain sentiment, so investigation may be necessary into random 8 months of the year.\n",
    "- 2009 and 2010 are comparatively smaller datasets, so I will combine these two years to create a single 'year' `(!!to implement. I am currently ignoring 2009)`\n",
    "- 2020 has a similar number of headlines as the other years, but only has headlines up to June. The training and validation samples in this year will just be half that of other years..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi thread version -- Unigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting articles by date...\n",
      "New training window: 2010-01-01 00:00:00+00:00\n",
      "New validation window: 2012-01-01 00:00:00+00:00\n",
      "Preprocessing data...\n",
      "Beginning trials\n",
      "15 of 15 trials complete (last trial train: 12s, validation: 722s) [a=100,k=94,kval=218]Saving best config...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/word-lists/2010-1-1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joshf\\Documents\\thesis\\code\\kaggle\\kaggle.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=160'>161</a>\u001b[0m     csvwriter \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(csv_file)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=161'>162</a>\u001b[0m     csvwriter\u001b[39m.\u001b[39mwriterow([\u001b[39mstr\u001b[39m(curr_year) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(curr_month) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39malpha_plus\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39malpha_minus\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39mkappa\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39mlam\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mstr\u001b[39m(best_config[\u001b[39m'\u001b[39m\u001b[39mnorm_err\u001b[39m\u001b[39m'\u001b[39m])])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=162'>163</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mdata/word-lists/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(curr_year) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(curr_month) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m-1\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m, newline\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m csv_file:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=163'>164</a>\u001b[0m     \u001b[39m#write header\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=164'>165</a>\u001b[0m     csvwriter \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(csv_file)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joshf/Documents/thesis/code/kaggle/kaggle.ipynb#ch0000023?line=165'>166</a>\u001b[0m     csvwriter\u001b[39m.\u001b[39mwriterow([\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mO+ value\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mO- value\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/word-lists/2010-1-1.csv'"
     ]
    }
   ],
   "source": [
    "def validate_window(index, val_d, val_p, sentiment_words, LAM, trials):\n",
    "    error_arr = np.array(0)\n",
    "    # print(str(index) + \" computing lambda of \" + str(LAM))\n",
    "    for val_index in range(len(val_d)):\n",
    "        est_p = 0.5\n",
    "        val_bow = val_d[val_index]\n",
    "\n",
    "        testing_s = sum(val_bow.get(w,0) for w in sentiment_words)\n",
    "        if (testing_s > 0):\n",
    "            est_p = fminbound(equation_to_solve, 0, 1, (O,val_bow, sentiment_words,testing_s,LAM))\n",
    "        error_arr = np.append(error_arr, est_p - val_p[val_index])\n",
    "    normalised_error = np.linalg.norm(error_arr, 1)\n",
    "    lam_trial = {\n",
    "        'alpha': alpha,\n",
    "        'alpha_plus': ALPHA_PLUS,\n",
    "        'alpha_minus': ALPHA_MINUS,\n",
    "        'kappa': KAPPA,\n",
    "        'lam': LAM,\n",
    "        'o': O,\n",
    "        'sentiment_words': sentiment_words,\n",
    "        'norm_err': normalised_error\n",
    "    }\n",
    "    trials.append(lam_trial)\n",
    "\n",
    "# reset global variables\n",
    "sgn = []\n",
    "y = []\n",
    "dates = []\n",
    "global_bow = {}\n",
    "d = []\n",
    "kappa_configs   = [86, 88, 90, 92, 94]\n",
    "alpha_configs   = [25,50,100]\n",
    "lambda_configs  = [1,5,10]\n",
    "model_id = 'stem-timezone'\n",
    "# kappa_configs   = [86]\n",
    "# alpha_configs   = [25]\n",
    "# lambda_configs  = [1]\n",
    "curr_year = 2010\n",
    "curr_month = 1\n",
    "opening_date = datetime(curr_year,curr_month,1,0,0,0,0)\n",
    "list_dates = [a['date'] for a in article_list]\n",
    "# end_date = datetime.strptime(max(list_dates), '%Y-%m-%d %H:%M:%S%z')\n",
    "end_date = max(list_dates)\n",
    "\n",
    "# reset saved config files\n",
    "destination_directory = './data/models/' + model_id\n",
    "if not os.path.exists(destination_directory):\n",
    "    os.mkdir(destination_directory)\n",
    "    os.mkdir(os.path.join(destination_directory,'word-lists'))\n",
    "with open(destination_directory + '/configurations.csv', 'w', newline='') as csv_file:\n",
    "    csvwriter = csv.writer(csv_file)\n",
    "    csvwriter.writerow([('date'), ('no. sentiment words'), ('alpha plus'), ('alpha_minus'), ('kappa'), ('lambda'), ('normalised error')])\n",
    "\n",
    "\n",
    "while(datetime(curr_year+3, curr_month, 1, 0,0,0,0).date() < end_date.date()):\n",
    "    #SELECT ARTICLES\n",
    "    val_date_start = datetime(curr_year+2,curr_month,1,0,0,0,0)\n",
    "    val_date_end = datetime(curr_year+3,curr_month,1,0,0,0,0)\n",
    "    curr_date = datetime(curr_year, curr_month, 1,0,0,0,0,est)\n",
    "    val_date = datetime(curr_year + 2, curr_month, 1,0,0,0,0,est)\n",
    "    val_end = datetime(curr_year + 3, curr_month, 1,0,0,0,0,est)\n",
    "    print('Selecting articles by date...')\n",
    "    print(\"New training window: \" + str(curr_date))\n",
    "    print(\"New validation window: \" + str(val_date))\n",
    "    training_arts   = [a for a in article_list if (a['date'] >= curr_date and a['date'] < val_date)]\n",
    "    validation_arts = [a for a in article_list if (a['date'] >= val_date and a['date'] < val_end)]\n",
    "    # print(training_arts)\n",
    "\n",
    "    #PRE-PROCESS\n",
    "    print('Preprocessing data...')\n",
    "    train_d = []\n",
    "    train_sgn = []\n",
    "    train_y = []\n",
    "    val_d = []\n",
    "    val_sgn =[]\n",
    "    val_y = []\n",
    "    for train_a in training_arts:\n",
    "        train_bow = text_to_bow(train_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(train_a)\n",
    "        train_d.append(train_bow)\n",
    "        train_sgn.append(sgn_a)\n",
    "        train_y.append(returns)\n",
    "    for val_a in validation_arts:\n",
    "        val_bow = text_to_bow(val_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(val_a)\n",
    "        val_d.append(val_bow)\n",
    "        val_y.append(returns)\n",
    "    \n",
    "    # fraction of positively tagged training articles\n",
    "    train_pi = sum(sgn_i > 0 for sgn_i in train_sgn)/len(train_sgn)\n",
    "\n",
    "    # start training\n",
    "    print('Beginning trials')\n",
    "    trials = []\n",
    "    curr_trial = 0\n",
    "    # pre calculations (things not affected by the changes we make)\n",
    "    (pos_j, total_j, f) = calc_f(train_d, train_sgn)\n",
    "    p                   = calc_p(train_y)\n",
    "    val_p               = calc_p(val_y)\n",
    "    #PARAM GRID\n",
    "    for alpha in alpha_configs:\n",
    "        for KAPPA in kappa_configs:\n",
    "            #TRAINING\n",
    "            train_time_0 = time.time()\n",
    "            kappa_percentile = np.percentile(np.array(list(total_j.values())),KAPPA) # return the nth percentile of all appearances for KAPPA\n",
    "\n",
    "            #calculate alpha vals\n",
    "            ALPHA_PLUS  = train_pi/2\n",
    "            ALPHA_MINUS = train_pi/2\n",
    "            delta_plus  = train_pi/4\n",
    "            delta_minus  = train_pi/4\n",
    "            delta_limit = 0.001\n",
    "            while(delta_plus > delta_limit):\n",
    "                no_pos_words = len([w for w in total_j if f[w] >= train_pi + ALPHA_PLUS and total_j[w] >= kappa_percentile])\n",
    "                if no_pos_words == alpha:\n",
    "                    delta_plus = 0\n",
    "                elif (no_pos_words > alpha):\n",
    "                    ALPHA_PLUS += delta_plus\n",
    "                    delta_plus /= 2\n",
    "                else:\n",
    "                    ALPHA_PLUS -= delta_plus\n",
    "                    delta_plus /= 2\n",
    "            while(delta_minus > delta_limit):\n",
    "                no_neg_words = len([w for w in total_j if f[w] <= train_pi - ALPHA_MINUS and total_j[w] >= kappa_percentile])\n",
    "                if no_neg_words == alpha:\n",
    "                    delta_minus = 0\n",
    "                elif (no_neg_words > alpha):\n",
    "                    ALPHA_MINUS += delta_minus\n",
    "                    delta_minus /= 2\n",
    "                else:\n",
    "                    ALPHA_MINUS -= delta_minus\n",
    "                    delta_minus /= 2\n",
    "            sentiment_words = [w for w in total_j if ((f[w] >= train_pi + ALPHA_PLUS or f[w] <= train_pi - ALPHA_MINUS) and total_j[w] >= kappa_percentile)]\n",
    "\n",
    "            (s, d_s)    = calc_s(sentiment_words, train_d)\n",
    "            h           = calc_h(sentiment_words, train_d, s, d_s)\n",
    "            O           = calc_o(p,h)\n",
    "            train_time_1 = time.time()\n",
    "            # for LAM in lambda_configs:\n",
    "\n",
    "            #VALIDATING\n",
    "            # start multithreading here\n",
    "            t0 = time.time()\n",
    "            threads = []\n",
    "            for index in range(3):\n",
    "                logging.info(\"Main    : create and start thread %d.\", index)\n",
    "                x = threading.Thread(target=validate_window, args=(index,val_d, val_p, sentiment_words,lambda_configs[index],trials))\n",
    "                threads.append(x)\n",
    "                x.start()\n",
    "\n",
    "            for index, thread in enumerate(threads):\n",
    "                logging.info(\"Main    : before joining thread %d.\", index)\n",
    "                thread.join()\n",
    "                logging.info(\"Main    : thread %d done\", index)\n",
    "            curr_trial += 1\n",
    "            t1 = time.time()\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"%d of %d trials complete (last trial train: %ds, validation: %ds) [a=%d,k=%d,kval=%d]\" % (curr_trial, 15, train_time_1-train_time_0, t1-t0, alpha, KAPPA, kappa_percentile))\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    #RECORD BEST CONFIG\n",
    "    print(\"Saving best config...\")\n",
    "    best_config = min(trials, key=lambda x:x['norm_err'])\n",
    "    with open(destination_directory + '/configurations.csv', 'a', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str(curr_year) + '-' + str(curr_month) + '-1', str(best_config['alpha']), str(best_config['alpha_plus']), str(best_config['alpha_minus']), str(best_config['kappa']), str(best_config['lam']), str(best_config['norm_err'])])\n",
    "    with open(destination_directory + '/word-lists/' + str(curr_year) + '-' + str(curr_month) + '-1' + '.csv', 'w', newline='') as csv_file:\n",
    "        #write header\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow(['word', 'O+ value', 'O- value'])\n",
    "    with open(destination_directory + '/word-lists/' + str(curr_year) + '-' + str(curr_month) + '-1' + '.csv', 'a', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        for ind in range(len(best_config['sentiment_words'])):\n",
    "            csvwriter.writerow([str(best_config['sentiment_words'][ind]), str(best_config['o'][ind][0]), str(best_config['o'][ind][1])])\n",
    "\n",
    "    #record start date, sentiment words, O, and params probably\n",
    "    #i think maybe start date and params in a different file to sentiment words and O.\n",
    "\n",
    "    #MOVE WINDOW\n",
    "    curr_month += 4\n",
    "    if (curr_month > 12):\n",
    "        curr_month = 1\n",
    "        curr_year += 1\n",
    "    # opening_date += relativedelta(months=4)\n",
    "\n",
    "# #2020 SPECIAL CASE\n",
    "\n",
    "\n",
    "# TOTAL_ARTS = len(article_list)\n",
    "# curr_index = 0\n",
    "# curr_thousand = 0\n",
    "# for a in article_list:\n",
    "#     raw_html = a['headline']\n",
    "#     if(raw_html):\n",
    "#         bow_art = text_to_bow(raw_html)\n",
    "#         (returns, sgn_a) = calc_returns(a)\n",
    "#         dates.append(a['date'])\n",
    "#         d.append(bow_art)\n",
    "#         sgn.append(sgn_a)\n",
    "#         y.append(returns)\n",
    "#     if curr_index >= curr_thousand:\n",
    "#         curr_thousand += 10000\n",
    "#         sys.stdout.write('\\r')\n",
    "#         j = (curr_index + 1) / TOTAL_ARTS\n",
    "#         sys.stdout.write(\"%d out of %d articles processed\" % (curr_index, TOTAL_ARTS))\n",
    "#         sys.stdout.flush()\n",
    "#     curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Thread -- Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_window(index, val_d, val_p, sentiment_words, LAM, trials):\n",
    "    error_arr = np.array(0)\n",
    "    # print(str(index) + \" computing lambda of \" + str(LAM))\n",
    "    for val_index in range(len(val_d)):\n",
    "        est_p = 0.5\n",
    "        val_bow = val_d[val_index]\n",
    "\n",
    "        testing_s = sum(val_bow.get(w,0) for w in sentiment_words)\n",
    "        if (testing_s > 0):\n",
    "            est_p = fminbound(equation_to_solve, 0, 1, (O,val_bow, sentiment_words,testing_s,LAM))\n",
    "        error_arr = np.append(error_arr, est_p - val_p[val_index])\n",
    "    normalised_error = np.linalg.norm(error_arr, 1)\n",
    "    lam_trial = {\n",
    "        'alpha': alpha,\n",
    "        'alpha_plus': ALPHA_PLUS,\n",
    "        'alpha_minus': ALPHA_MINUS,\n",
    "        'kappa': KAPPA,\n",
    "        'lam': LAM,\n",
    "        'o': O,\n",
    "        'sentiment_words': sentiment_words,\n",
    "        'norm_err': normalised_error\n",
    "    }\n",
    "    trials.append(lam_trial)\n",
    "\n",
    "# reset global variables\n",
    "sgn = []\n",
    "y = []\n",
    "dates = []\n",
    "global_bow = {}\n",
    "d = []\n",
    "kappa_configs   = [86, 88, 90, 92, 94]\n",
    "alpha_configs   = [25,50,100]\n",
    "lambda_configs  = [1,5,10]\n",
    "KAPPA_BIGRAM = 90\n",
    "# kappa_configs   = [86]\n",
    "# alpha_configs   = [25]\n",
    "# lambda_configs  = [1]\n",
    "curr_year = 2010\n",
    "curr_month = 1\n",
    "opening_date = datetime(curr_year,curr_month,1,0,0,0,0)\n",
    "list_dates = [a['date'] for a in article_list]\n",
    "# end_date = datetime.strptime(max(list_dates), '%Y-%m-%d %H:%M:%S%z')\n",
    "end_date = max(list_dates)\n",
    "\n",
    "destination_directory = './data/models/' + model_id\n",
    "if not os.path.exists(destination_directory):\n",
    "    os.mkdir(destination_directory)\n",
    "    os.mkdir(os.path.join(destination_directory,'word-lists'))\n",
    "\n",
    "# reset saved config files\n",
    "with open(destination_directory + '/configurations.csv', 'w', newline='') as csv_file:\n",
    "    csvwriter = csv.writer(csv_file)\n",
    "    csvwriter.writerow([('date'), ('no. sentiment words'), ('alpha plus'), ('alpha_minus'), ('kappa'), ('lambda'), ('normalised error')])\n",
    "\n",
    "\n",
    "while(datetime(curr_year+3, curr_month, 1, 0,0,0,0).date() < end_date.date()):\n",
    "    #SELECT ARTICLES\n",
    "    val_date_start = datetime(curr_year+2,curr_month,1,0,0,0,0)\n",
    "    val_date_end = datetime(curr_year+3,curr_month,1,0,0,0,0)\n",
    "    curr_date = datetime(curr_year, curr_month, 1,0,0,0,0,est)\n",
    "    val_date = datetime(curr_year + 2, curr_month, 1,0,0,0,0,est)\n",
    "    val_end = datetime(curr_year + 3, curr_month, 1,0,0,0,0,est)\n",
    "    print('Selecting articles by date...')\n",
    "    print(\"New training window: \" + str(curr_date))\n",
    "    print(\"New validation window: \" + str(val_date))\n",
    "    training_arts   = [a for a in article_list if (a['date'] >= curr_date and a['date'] < val_date)]\n",
    "    validation_arts = [a for a in article_list if (a['date'] >= val_date and a['date'] < val_end)]\n",
    "    # print(training_arts)\n",
    "\n",
    "    #PRE-PROCESS\n",
    "    print('Preprocessing data...')\n",
    "    train_d = []\n",
    "    train_d_bigram = []\n",
    "    train_sgn = []\n",
    "    train_y = []\n",
    "    val_d = []\n",
    "    val_sgn =[]\n",
    "    val_y = []\n",
    "    for train_a in training_arts:\n",
    "        train_bow = text_to_bow(train_a['headline'])\n",
    "        train_bow_bigram = text_to_bow_bigram(train_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(train_a)\n",
    "        train_d.append(train_bow)\n",
    "        train_d_bigram.append(train_bow_bigram)\n",
    "        train_sgn.append(sgn_a)\n",
    "        train_y.append(returns)\n",
    "    for val_a in validation_arts:\n",
    "        val_bow = text_to_bow_bigram(val_a['headline'])\n",
    "        (returns, sgn_a) = calc_returns(val_a)\n",
    "        val_d.append(val_bow)\n",
    "        val_y.append(returns)\n",
    "    \n",
    "    # fraction of positively tagged training articles\n",
    "    train_pi = sum(sgn_i > 0 for sgn_i in train_sgn)/len(train_sgn)\n",
    "\n",
    "    # start training\n",
    "    print('Beginning trials')\n",
    "    trials = []\n",
    "    curr_trial = 0\n",
    "    # pre calculations (things not affected by the changes we make)\n",
    "    (pos_j, total_j, f) = calc_f(train_d, train_sgn)\n",
    "    (pos_j_bigram, total_j_bigram, f_bigram) = calc_f(train_d_bigram, train_sgn)\n",
    "    p                   = calc_p(train_y)\n",
    "    val_p               = calc_p(val_y)\n",
    "    #PARAM GRID\n",
    "    kappa_percentile_bigram = np.percentile(np.array(list(total_j.values())),KAPPA_BIGRAM) # return the nth percentile of all appearances for KAPPA\n",
    "    bigrams_to_remove = []\n",
    "    mutual_info = {}\n",
    "    for w in total_j_bigram:\n",
    "        component_words = w.split()\n",
    "        if not (total_j[component_words[0]] >= kappa_percentile_bigram and total_j[component_words[1]] >= kappa_percentile_bigram):\n",
    "            bigrams_to_remove.append(w)\n",
    "        else:\n",
    "            mutual_info[w] = total_j_bigram[w] / (total_j[component_words[0]] * total_j[component_words[1]])\n",
    "    mutual_info_percentile = np.percentile(np.array(list(mutual_info.values())),95) # return the nth percentile of all appearances for KAPPA\n",
    "    bigrams_to_remove.extend([w for w in mutual_info if mutual_info[w] <= mutual_info_percentile])\n",
    "    for b in bigrams_to_remove:\n",
    "        pos_j_bigram.pop(b)\n",
    "        total_j_bigram.pop(b)\n",
    "        f_bigram.pop(b)\n",
    "\n",
    "    for alpha in alpha_configs:\n",
    "        for KAPPA in kappa_configs:\n",
    "            #TRAINING\n",
    "            train_time_0 = time.time()\n",
    "            kappa_percentile = np.percentile(np.array(list(total_j_bigram.values())),KAPPA) # return the nth percentile of all appearances for KAPPA\n",
    "\n",
    "            #calculate alpha vals (NOW WITH QUICKER SEARCHING)\n",
    "            ALPHA_PLUS  = train_pi/2\n",
    "            ALPHA_MINUS = train_pi/2\n",
    "            delta_plus  = train_pi/4\n",
    "            delta_minus  = train_pi/4\n",
    "            delta_limit = 0.0000001\n",
    "\n",
    "            while(delta_plus > delta_limit):\n",
    "                no_pos_words = len([w for w in total_j_bigram if f_bigram[w] >= train_pi + ALPHA_PLUS and total_j_bigram[w] >= kappa_percentile])\n",
    "                if no_pos_words == alpha:\n",
    "                    delta_plus = 0\n",
    "                elif (no_pos_words > alpha):\n",
    "                    ALPHA_PLUS += delta_plus\n",
    "                    delta_plus /= 2\n",
    "                else:\n",
    "                    ALPHA_PLUS -= delta_plus\n",
    "                    delta_plus /= 2\n",
    "            while(delta_minus > delta_limit):\n",
    "                no_neg_words = len([w for w in total_j_bigram if f_bigram[w] <= train_pi - ALPHA_MINUS and total_j_bigram[w] >= kappa_percentile])\n",
    "                if no_neg_words == alpha:\n",
    "                    delta_minus = 0\n",
    "                elif (no_neg_words > alpha):\n",
    "                    ALPHA_MINUS += delta_minus\n",
    "                    delta_minus /= 2\n",
    "                else:\n",
    "                    ALPHA_MINUS -= delta_minus\n",
    "                    delta_minus /= 2\n",
    "            print('no pos '  + str(no_pos_words))\n",
    "            print('no neg ' + str(no_neg_words))\n",
    "            print('Alpha + // - ', ALPHA_PLUS, ALPHA_MINUS)\n",
    "            # sentiment_words = [w for w in total_j_bigram]\n",
    "            sentiment_words = [w for w in total_j_bigram if ((f_bigram[w] >= train_pi + ALPHA_PLUS or f_bigram[w] <= train_pi - ALPHA_MINUS) and total_j_bigram[w] >= kappa_percentile)]\n",
    "            print(len(sentiment_words))\n",
    "\n",
    "            (s, d_s)    = calc_s(sentiment_words, train_d_bigram)\n",
    "            h           = calc_h(sentiment_words, train_d_bigram, s, d_s)\n",
    "            O           = calc_o(p,h)\n",
    "            train_time_1 = time.time()\n",
    "            # for LAM in lambda_configs:\n",
    "\n",
    "            #VALIDATING\n",
    "            # start multithreading here\n",
    "            t0 = time.time()\n",
    "            threads = []\n",
    "            for index in range(3):\n",
    "                logging.info(\"Main    : create and start thread %d.\", index)\n",
    "                x = threading.Thread(target=validate_window, args=(index,val_d, val_p, sentiment_words,lambda_configs[index],trials))\n",
    "                threads.append(x)\n",
    "                x.start()\n",
    "\n",
    "            for index, thread in enumerate(threads):\n",
    "                logging.info(\"Main    : before joining thread %d.\", index)\n",
    "                thread.join()\n",
    "                logging.info(\"Main    : thread %d done\", index)\n",
    "            curr_trial += 1\n",
    "            t1 = time.time()\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"%d of %d trials complete (last trial train: %ds, validation: %ds) [a=%d,k=%d,kval=%d]\" % (curr_trial, 15, train_time_1-train_time_0, t1-t0, alpha, KAPPA, kappa_percentile))\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    #RECORD BEST CONFIG\n",
    "    print(\"Saving best config...\")\n",
    "    best_config = min(trials, key=lambda x:x['norm_err'])\n",
    "    with open(destination_directory + '/configurations.csv', 'a', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str(curr_year) + '-' + str(curr_month) + '-1', str(best_config['alpha']), str(best_config['alpha_plus']), str(best_config['alpha_minus']), str(best_config['kappa']), str(best_config['lam']), str(best_config['norm_err'])])\n",
    "    with open(destination_directory + '/word-lists/' + str(curr_year) + '-' + str(curr_month) + '-1' + '.csv', 'w', newline='') as csv_file:\n",
    "        #write header\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow(['word', 'O+ value', 'O- value'])\n",
    "    with open(destination_directory + '/word-lists/' + str(curr_year) + '-' + str(curr_month) + '-1' + '.csv', 'a', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        for ind in range(len(best_config['sentiment_words'])):\n",
    "            csvwriter.writerow([str(best_config['sentiment_words'][ind]), str(best_config['o'][ind][0]), str(best_config['o'][ind][1])])\n",
    "\n",
    "    #record start date, sentiment words, O, and params probably\n",
    "    #i think maybe start date and params in a different file to sentiment words and O.\n",
    "\n",
    "    #MOVE WINDOW\n",
    "    curr_month += 4\n",
    "    if (curr_month > 12):\n",
    "        curr_month = 1\n",
    "        curr_year += 1\n",
    "    # opening_date += relativedelta(months=4)\n",
    "\n",
    "# #2020 SPECIAL CASE\n",
    "\n",
    "\n",
    "# TOTAL_ARTS = len(article_list)\n",
    "# curr_index = 0\n",
    "# curr_thousand = 0\n",
    "# for a in article_list:\n",
    "#     raw_html = a['headline']\n",
    "#     if(raw_html):\n",
    "#         bow_art = text_to_bow(raw_html)\n",
    "#         (returns, sgn_a) = calc_returns(a)\n",
    "#         dates.append(a['date'])\n",
    "#         d.append(bow_art)\n",
    "#         sgn.append(sgn_a)\n",
    "#         y.append(returns)\n",
    "#     if curr_index >= curr_thousand:\n",
    "#         curr_thousand += 10000\n",
    "#         sys.stdout.write('\\r')\n",
    "#         j = (curr_index + 1) / TOTAL_ARTS\n",
    "#         sys.stdout.write(\"%d out of %d articles processed\" % (curr_index, TOTAL_ARTS))\n",
    "#         sys.stdout.flush()\n",
    "#     curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the data\n",
    "\n",
    "With the data all collected, we can do a little visualisation and see what outputs we actually got. First, let's observe the metadata of the dataset by seeing the distribution of articles by year, and then month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise year\n",
    "\n",
    "list_dates = [a['date'] for a in article_list]\n",
    "years = []\n",
    "year_counts = []\n",
    "for year in range(2009,2021):\n",
    "    curr_year = datetime(year, 1, 1,0,0,0,0,est)\n",
    "    curr_year_1 = datetime(year+1, 1, 1,0,0,0,0,est)\n",
    "    year_counts.append(len([a for a in article_list if (a['date'] <= curr_year_1  and a['date'] >= curr_year)]))\n",
    "    years.append(year)\n",
    "\n",
    "month_counts = []\n",
    "months = []\n",
    "for month in range(1, 13):\n",
    "    month_counts.append(len([a for a in article_list if a['date'].month == month]))\n",
    "    months.append(month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.bar(years, year_counts)\n",
    "plt.title(\"Article count by year\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.bar(months, month_counts)\n",
    "plt.title(\"Article count by month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 1\n",
      "done 2\n",
      "done 3\n",
      "done 4\n",
      "done 5\n",
      "done 6\n",
      "done 7\n",
      "done 8\n",
      "done 9\n",
      "done 10\n",
      "done 11\n",
      "done 12\n",
      "done 13\n",
      "done 14\n",
      "done 15\n",
      "done 16\n",
      "done 17\n",
      "done 18\n",
      "done 19\n",
      "done 20\n",
      "done 21\n",
      "done 22\n",
      "2010-1-1 & 100 & 0.0168 & 0.0218 & 90 & 5 & 20416.73 & 0.24739\\\\\n",
      "2010-5-1 & 50 & 0.1123 & 0.0898 & 88 & 5 & 20716.47 & 0.24931\\\\\n",
      "2010-9-1 & 25 & 0.1358 & 0.1013 & 92 & 5 & 20677.31 & 0.24977\\\\\n",
      "2011-1-1 & 25 & 0.1175 & 0.0567 & 94 & 5 & 20238.53 & 0.24916\\\\\n",
      "2011-5-1 & 100 & 0.0263 & 0.0243 & 92 & 5 & 19341.18 & 0.2494\\\\\n",
      "2011-9-1 & 100 & 0.0697 & 0.0461 & 88 & 5 & 19354.6 & 0.2478\\\\\n",
      "2012-1-1 & 100 & 0.0001 & 0.0 & 94 & 5 & 20568.7 & 0.24858\\\\\n",
      "2012-5-1 & 100 & 0.0572 & 0.0579 & 88 & 10 & 21840.62 & 0.24962\\\\\n",
      "2012-9-1 & 25 & 0.1472 & 0.1714 & 86 & 10 & 22286.96 & 0.24978\\\\\n",
      "2013-1-1 & 25 & 0.1386 & 0.1638 & 88 & 5 & 21719.86 & 0.249\\\\\n",
      "2013-5-1 & 25 & 0.1419 & 0.1619 & 88 & 5 & 22357.35 & 0.24843\\\\\n",
      "2013-9-1 & 100 & 0.0206 & 0.0123 & 94 & 5 & 23398.95 & 0.24869\\\\\n",
      "2014-1-1 & 100 & 0.0183 & 0.0142 & 94 & 5 & 24854.8 & 0.24853\\\\\n",
      "2014-5-1 & 50 & 0.0999 & 0.1199 & 88 & 5 & 24305.78 & 0.24901\\\\\n",
      "2014-9-1 & 100 & 0.0274 & 0.0281 & 94 & 5 & 23519.78 & 0.24886\\\\\n",
      "2015-1-1 & 100 & 0.0238 & 0.0239 & 94 & 5 & 22768.1 & 0.24769\\\\\n",
      "2015-5-1 & 100 & 0.021 & 0.0293 & 94 & 5 & 23613.92 & 0.24843\\\\\n",
      "2015-9-1 & 100 & 0.0684 & 0.0618 & 90 & 5 & 26419.55 & 0.24827\\\\\n",
      "2016-1-1 & 100 & 0.0823 & 0.0772 & 88 & 5 & 29914.03 & 0.24788\\\\\n",
      "2016-5-1 & 100 & 0.0304 & 0.0223 & 94 & 5 & 30439.66 & 0.24828\\\\\n",
      "2016-9-1 & 50 & 0.0959 & 0.0775 & 92 & 5 & 31058.81 & 0.24764\\\\\n",
      "2017-1-1 & 50 & 0.1019 & 0.0848 & 92 & 5 & 31695.28 & 0.24773\\\\\n",
      "2017-5-1 & 100 & 0.1023 & 0.0698 & 88 & 5 & 40238.82 & 0.24423\\\\\n"
     ]
    }
   ],
   "source": [
    "curr_year = 2010\n",
    "curr_month = 1\n",
    "\n",
    "config_data = []\n",
    "path = './data/models/bigrams-stem/configurations.csv'\n",
    "with open(str(path), encoding='utf-8') as csv_file:\n",
    "    #date,no. sentiment words,alpha plus,alpha_minus,kappa,lambda,normalised error\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_no = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        if line_no > 0:\n",
    "            config_data.append([row[0],row[1],round(float(row[2]),4),round(float(row[3]),4),row[4],row[5],round(float(row[6]),2),row[6]])\n",
    "        line_no +=  1\n",
    "index = 0\n",
    "\n",
    "while(index < len(config_data)):\n",
    "    #SELECT ARTICLES\n",
    "    val_date_start = datetime(curr_year+2,curr_month,1,0,0,0,0)\n",
    "    val_date_end = datetime(curr_year+3,curr_month,1,0,0,0,0)\n",
    "    curr_date = datetime(curr_year, curr_month, 1,0,0,0,0,est)\n",
    "    val_date = datetime(curr_year + 2, curr_month, 1,0,0,0,0,est)\n",
    "    val_end = datetime(curr_year + 3, curr_month, 1,0,0,0,0,est)\n",
    "    # training_arts   = [a for a in article_list if (a['date'] >= curr_date and a['date'] < val_date)]\n",
    "    validation_arts = [a for a in article_list if (a['date'] >= val_date and a['date'] < val_end)]\n",
    "    config_data[index][7]  = round(float(config_data[index][7])/len(validation_arts),5)\n",
    "\n",
    "    curr_month += 4\n",
    "    if (curr_month > 12):\n",
    "        curr_month = 1\n",
    "        curr_year += 1\n",
    "    print('done', str(index))\n",
    "    index += 1\n",
    "\n",
    "for row in config_data:\n",
    "    print(str(row[0]) + ' & '  + str(row[1]) + ' & '  + str(row[2]) + ' & '  + str(row[3]) + ' & '  + str(row[4]) + ' & '  +str(row[5]) + ' & '  +str(row[6]) + ' & '  +str(row[7]) + '\\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the actual word data we pulled. We'll load it in wordcloud form, with the top 50 words of each sentiment represented by the average tone over all 19 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1119613676641045\n"
     ]
    }
   ],
   "source": [
    "sum_words = 0\n",
    "for a in article_list:\n",
    "    a_bow = text_to_bow(a['headline'])\n",
    "    for b in a_bow:\n",
    "        sum_words += a_bow[b]\n",
    "\n",
    "print(sum_words/len(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week high & 0.023588889579003413 & 19\n",
      "volume mover & 0.01890087984255979 & 19\n",
      "spike higher & 0.011535128630118099 & 16\n",
      "repurchase program & 5.975856692408093e-05 & 13\n",
      "micron technology & 0.0018273803321212933 & 13\n",
      "tender offer & 0.000493698139556141 & 12\n",
      "move higher & 0.0034025812702412703 & 12\n",
      "urban outfitter & 0.002972514273394266 & 11\n",
      "standpoint research & 0.00043239878324577533 & 11\n",
      "time warner & 0.0036646831603115823 & 11\n",
      "western union & 0.001969718841898676 & 11\n",
      "top gainer & 0.014600369545534633 & 11\n",
      "alert call & 0.0016063174700931939 & 10\n",
      "jobless claim & 0.0004886878146253961 & 10\n",
      "dish network & 0.0002073777158751797 & 10\n",
      "marvel technology & 0.0009050305170128138 & 10\n",
      "compass point & 1.0555471895301076e-05 & 9\n",
      "option activity & 9.245489784405593e-05 & 9\n",
      "office depot & 0.0004378670771010597 & 9\n",
      "alto network & 0.00260164229405387 & 9\n",
      "western digit & 0.003228910816026079 & 9\n",
      "special dividend & 0.0003852789097379849 & 9\n",
      "spike high & 0.0014629030012784283 & 8\n",
      "insider buy & 0.00016341653108030343 & 8\n",
      "health system & 0.00016498130447294154 & 8\n",
      "steel dynamic & 0.0023196834919034038 & 8\n",
      "higher heavy & 0.0024864421790584674 & 8\n",
      "natural gas & 0.0003956458893032696 & 8\n",
      "session high & 0.001830724641638416 & 8\n",
      "decline comment & 0.00019190080908583086 & 8\n",
      "quarter dividend & 0.0004301683425905702 & 8\n",
      "initiate coverage & 0.0035019586819353367 & 7\n",
      "midday gainer & 0.00992836446554962 & 7\n",
      "top initiation & 0.0005060361997133979 & 7\n",
      "f show & 0.00046614653113369797 & 7\n",
      "hear unconfirm & 0.00012461205508809315 & 7\n",
      "rais dividend & 0.0007026783776395602 & 7\n",
      "unconfirm chatter & 0.00044302433187505395 & 7\n",
      "money pick & 1.3880245805155028e-05 & 7\n",
      "increase dividend & 0.0005781926097538079 & 7\n",
      "oil gas & 0.0003329780535776144 & 7\n",
      "notable call & 0.0005647017470579695 & 7\n",
      "call option & 0.0004782212654480257 & 7\n",
      "session low & 0.000315045712577606 & 7\n",
      "white petroleum & 0.0022859559127359785 & 7\n",
      "increase quarter & 8.104614049351177e-06 & 7\n",
      "monster beverage & 0.0003123131672504 & 7\n",
      "ahead economic & 3.578923789652388e-05 & 6\n",
      "option alert & 0.0004099439469902586 & 6\n",
      "mar call & 0.00012694463225868018 & 6\n",
      "public offer & -0.007175861842321899 & 19\n",
      "offer common & -0.003422489542476463 & 15\n",
      "resume trade & -0.0032180039044344215 & 14\n",
      "week low & -0.019853563800769376 & 14\n",
      "secondary offer & -0.002503348558672391 & 14\n",
      "sector perform & -0.0023237899779340885 & 13\n",
      "bed bath & -0.001655572884448479 & 13\n",
      "bath beyond & -0.0016488911459325566 & 13\n",
      "general dynamic & -0.0012251503536982403 & 13\n",
      "worst perform & -0.014930116164956203 & 12\n",
      "mix security & -0.0005359364201091856 & 12\n",
      "security shelf & -0.00022838687175500783 & 12\n",
      "boston scientific & -0.003506368607290168 & 11\n",
      "halt news & -0.00030240772386090186 & 10\n",
      "miss estimate & -0.0008898589100110276 & 10\n",
      "finish line & -0.0017322013350707511 & 10\n",
      "community health & -8.641839520083187e-05 & 10\n",
      "cliff natural & -0.0014491907754091249 & 10\n",
      "natural resource & -0.0006972361978509145 & 10\n",
      "first solar & -0.009474547857088012 & 10\n",
      "hold remove & -0.0026996687212615805 & 10\n",
      "news pend & -0.0001992763366625323 & 9\n",
      "dick sport & -0.0005476614974852211 & 9\n",
      "may compare & -0.0003722644244198684 & 9\n",
      "general mill & -0.0012919840485775063 & 9\n",
      "auto part & -0.0017316090374883532 & 9\n",
      "midafternoon market & -0.00013678588887924882 & 8\n",
      "crude oil & -0.0003697647229148961 & 8\n",
      "dollar general & -0.0018020619430197692 & 8\n",
      "issue weak & -0.0004937059868059226 & 8\n",
      "electron art & -0.0011097151170676224 & 8\n",
      "dollar tree & -0.0009418344438011783 & 8\n",
      "award million & -0.0005185915760406786 & 8\n",
      "share repurchase & -0.0004600336209298127 & 8\n",
      "juniper network & -0.0015755250701055422 & 8\n",
      "perform remove & -0.0029464417856645267 & 8\n",
      "midmorn market & -0.0004591318442015353 & 7\n",
      "term disclosed & -0.0005003399602810339 & 7\n",
      "stern age & -0.00021437129499889503 & 7\n",
      "shelf offer & -0.00015898661415484632 & 7\n",
      "realty trust & -0.0006689777030959241 & 7\n",
      "award contract & -0.0003531359167069949 & 7\n",
      "acquisition news & -0.00024829931603555146 & 7\n",
      "profit miss & -3.0035886238730585e-05 & 7\n",
      "new york & -0.0002205530342829452 & 7\n",
      "sport good & -0.0007839041848430713 & 7\n",
      "hit week & -0.00045721544004352876 & 7\n",
      "retail sale & -0.00020853337523922088 & 7\n",
      "eagle outfitter & -0.003284600438155382 & 7\n",
      "worth look & -0.0007338767372143398 & 7\n",
      "Top 100 sentiment positive words\n",
      "[('week high', 0.023588889579003413), ('volume mover', 0.01890087984255979), ('top gainer', 0.014600369545534633), ('spike higher', 0.011535128630118099), ('midday gainer', 0.00992836446554962), ('time warner', 0.0036646831603115823), ('initiate coverage', 0.0035019586819353367), ('move higher', 0.0034025812702412703)]\n",
      "Top 100 sentiment negative words\n",
      "[('week low', -0.019853563800769376), ('worst perform', -0.014930116164956203), ('midday loser', -0.009619434304041335), ('first solar', -0.009474547857088012), ('public offer', -0.007175861842321899), ('top loser', -0.005267554005063458), ('boston scientific', -0.003506368607290168), ('offer common', -0.003422489542476463)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_top_words(data, n=2, order=False, reverse=True):\n",
    "    \"\"\"Get top n words by tone. \n",
    "\n",
    "    Returns a dictionary or an `OrderedDict` if `order` is true.\n",
    "    \"\"\" \n",
    "    top = sorted(data.items(), key=lambda x: x[1][''], reverse=reverse)[:n]\n",
    "    if order:\n",
    "        return OrderedDict(top)\n",
    "    return dict(top)\n",
    "\n",
    "pathlist = Path('./data/models/bigrams-stem/word-lists/').rglob('*.csv')\n",
    "word_list_tone = {}\n",
    "word_list_count = {}\n",
    "for path in pathlist:\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        line = 0\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if line > 0:\n",
    "                if (row[0] in word_list_tone):\n",
    "                    word_list_tone[row[0]] += (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] += 1\n",
    "                else:\n",
    "                    word_list_tone[row[0]] = (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] = 1\n",
    "            line += 1\n",
    "\n",
    "word_list_count = {k: v for k, v in sorted(word_list_count.items(), reverse = True, key=lambda item: item[1])}\n",
    "word_list_tone = {k: v for k, v in sorted(word_list_tone.items(), key=lambda item: item[1])}\n",
    "for w in word_list_tone:\n",
    "    word_list_tone[w] /= 19\n",
    "# print(word_list_count)\n",
    "\n",
    "high_word_counts    = [w for w in word_list_count if word_list_count[w] > 14]\n",
    "# for w in high_word_counts:\n",
    "#     print(w, word_list_tone[w], word_list_count[w])\n",
    "pos_word_list_count = [w for w in word_list_count if word_list_tone[w] > 0][:50]\n",
    "neg_word_list_count = [w for w in word_list_count if word_list_tone[w] < 0][:50]\n",
    "for w in pos_word_list_count:\n",
    "    # in_lm = 0\n",
    "    # in_h4 = 0\n",
    "    # if w in positive_words_lm:\n",
    "    #     in_lm = 1\n",
    "    # if w in positive_words_h4:\n",
    "    #     in_h4 = 1\n",
    "    print(w + \" & \" + str(word_list_tone[w]) + \" & \" + str(word_list_count[w]))\n",
    "for w in neg_word_list_count:\n",
    "    print(w + \" & \" + str(word_list_tone[w]) + \" & \" + str(word_list_count[w]))\n",
    "\n",
    "print(\"Top 100 sentiment positive words\")\n",
    "# pos_words = dict(sorted(word_list_tone.items(), key = itemgetter(1), reverse = True)[:100])\n",
    "pos_words = dict(sorted(word_list_tone.items(), key = itemgetter(1), reverse = True)[:100])\n",
    "print(sorted(word_list_tone.items(), key = itemgetter(1), reverse = True)[:8])\n",
    "pos_words = {w: pos_words[w] for w in pos_words if pos_words[w] > 0}\n",
    "# wordcloud = WordCloud(width=800, height=400,colormap='summer', collocations=True,prefer_horizontal=1, background_color='black')\n",
    "# wordcloud.generate_from_frequencies(frequencies = pos_words)\n",
    "# plt.figure(figsize=(15,12))\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Top 100 sentiment negative words\")\n",
    "#pos_words = dict(sorted(word_list_tone.items(), key = itemgetter(1), reverse = True)[:100])\n",
    "neg_words = dict(sorted(word_list_tone.items(), key = itemgetter(1), reverse = False)[:100])\n",
    "print(sorted(word_list_tone.items(), key = itemgetter(1), reverse = False)[:8])\n",
    "neg_words = {w: neg_words[w] for w in neg_words if neg_words[w] < 0}\n",
    "for w in neg_words:\n",
    "    neg_words[w] = abs(neg_words[w])\n",
    "# wordcloud = WordCloud(background_color='white', colormap='forest',prefer_horizontal=1)\n",
    "# wordcloud = WordCloud(width=800, height=400,colormap='autumn', collocations=True,prefer_horizontal=1, background_color='black', max_words=100)\n",
    "# wordcloud.generate_from_frequencies(frequencies = neg_words)\n",
    "# plt.figure(figsize=(15,12))\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of sample Modelling\n",
    "Ok so we have trained the model and visualised the data. Let's now build some portfolios from the out of sample data and compare the success of the model against some other popular dictionaries.\n",
    "\n",
    "We will construct the sentiment score for these articles by aggregating counts of words listed in their positive sentiment dictionary (weighted tf-idf by LM)  and subtracting weighted counts of words in their negative dictionaries. For our SESTM, we average scores from multiple articles for the same firm in the same day.\n",
    "\n",
    "What I think they mean (and this might be wrong) is that they will take the best 50 sentiment stocks and long them, and vice versa for the 50 words sentiment stocks for that say (short them). At the start of the next trading day, they 'liquidate' the stocks, and evaluate whether they made money or not, and then this is stored. This is done for both lexicon methods.\n",
    "\n",
    "There are a few strategies that will be used to evaluate the methods:\n",
    "- Sharpe ratio, which compares the success over the volatility of the stocks\n",
    "- Potential Granger causality to prove that the headlines ARE actually the cause and it's not a fluke\n",
    "\n",
    "## Fama-French models\n",
    "The Fama-French three-factor model is a statistical model that describes stock returns. They explain phenomena using a small number of underlying causes or factors, and looks like this:\n",
    "$$R_a = R_f + \\beta_1(R_m - R_f)+ \\beta_2 \\cdot SMB + \\beta_3 \\cdot HML + \\alpha$$\n",
    "where:\n",
    "- $R_a$: expected return on asset\n",
    "- $R_f$: Risk free rate\n",
    "- $\\beta_{1,2,3}$: Factor coefficient\n",
    "- $R_m - R_f$: Market risk premium\n",
    "- $SMB$: (small minus big): Excess returns of small cap over large cap\n",
    "- $HML$: (high minus low): Excess returns of value stocks over growth stocks\n",
    "- $\\alpha$: intercept\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following if `kaggle/processed-data-raw/` doesn't contain a list of json files with stock data for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect out of sample articles\n",
    "oos_start_year = 2018\n",
    "oos_start_month = 5\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,pytz.est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "\n",
    "# collect stock info\n",
    "list_tickers = [a['ticker'] for a in oos_arts]\n",
    "list_tickers = list(dict.fromkeys(list_tickers))\n",
    "stock_data = {}\n",
    "failed_stocks = []\n",
    "print('pulling stocks...')\n",
    "# data = yf.download(tickers = list_tickers, end=str(end_date.date()), start=str(start_date.date()), progress=True)\n",
    "curr_index = 0\n",
    "TOTAL_TICKERS = len(list_tickers)\n",
    "for t in list_tickers:\n",
    "    arts_ticker = [a['date'] for a in article_list if a['ticker'] == t]\n",
    "    # print(type(arts_ticker[0]))\n",
    "    end_date_stock_data = max(arts_ticker) + dt.timedelta(days=5)\n",
    "    start_date_stock_data = min(arts_ticker) - dt.timedelta(days=5)\n",
    "    try:\n",
    "        data = yf.download(tickers = t, end=str(end_date_stock_data.date()), start=str(start_date_stock_data.date()), progress=False, show_errors=False)\n",
    "        if len(data > 0):\n",
    "            stock_data[t] = data\n",
    "        else:\n",
    "            failed_stocks.append(t)\n",
    "    except:\n",
    "        failed_stocks.append(t)\n",
    "    sys.stdout.write('\\r')\n",
    "    j = (curr_index + 1) / TOTAL_TICKERS\n",
    "    sys.stdout.write(\"[%-20s] %d%% %d out of %d (%d)\" % ('='*int(20*j), 100*j, curr_index, TOTAL_TICKERS, len(failed_stocks)))\n",
    "    sys.stdout.flush()\n",
    "    curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save stocks to a file (so you dont have to do that again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save stocks to file\n",
    "for s in stock_data:\n",
    "    with open('./processed-data-raw/' + s + '.json', 'w') as json_file:\n",
    "        json.dump(stock_data[s].to_json(), json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling file no 3363"
     ]
    }
   ],
   "source": [
    "# load stocks from file\n",
    "\n",
    "ticker_path = './processed-data-raw/'\n",
    "# recreate list of stock information\n",
    "pathlist = Path(ticker_path).rglob('*.json')\n",
    "stock_data = {}\n",
    "i = 0\n",
    "for path in pathlist:\n",
    "    with open(str(path)) as json_file:\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"pulling file no %d\" % (i))\n",
    "        sys.stdout.flush()#\n",
    "        i += 1\n",
    "        data = json.load(json_file)\n",
    "        data = json.loads(data)\n",
    "        datetime_dict = {}\n",
    "        for line in data:\n",
    "            datetime_line = {}\n",
    "            for k in data[line]:\n",
    "                # print(str(datetime.fromtimestamp(float(k)/1000.0)))\n",
    "                datetime_line[str(datetime.fromtimestamp(float(k)/1000.0).date())] = data[line][k]\n",
    "            datetime_dict[line] = datetime_line \n",
    "        stock_data[os.path.basename(str(path))[:-5]] = DataFrame.from_dict(datetime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to generate a list of possible dates (market days, essentially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0)\n",
    "# add the eastern timezone so can compare against timezone sensitive data\n",
    "possible_dates = [datetime(datetime.strptime(d, '%Y-%m-%d').year, datetime.strptime(d, '%Y-%m-%d').month, datetime.strptime(d, '%Y-%m-%d').day, 9, 0, 0, 0, est) for d in stock_data['A']['Open'].keys() if datetime.strptime(d, '%Y-%m-%d') > start_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal weighted strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n",
      "Prev days: -1\n",
      "Prev days: 0\n",
      "Prev days: 1\n",
      "Prev days: 2\n",
      "Prev days: 3\n",
      "Prev days: 4\n",
      "Prev days: 5\n",
      "Prev days: 6\n",
      "Prev days: 7\n",
      "Prev days: 8\n",
      "New folder ./data/out-of-sample/2013-05-01-ew-day+8 created\n",
      "Prev days: 9\n",
      "New folder ./data/out-of-sample/2013-05-01-ew-day+9 created\n",
      "Prev days: 10\n",
      "New folder ./data/out-of-sample/2013-05-01-ew-day+10 created\n"
     ]
    }
   ],
   "source": [
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "prev_days = 7\n",
    "best_config_file = './data/models/stemming/word-lists/2013-5-1.csv'\n",
    "sentiment_words = []\n",
    "O = np.array([0,0])\n",
    "with open(best_config_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "O = O[1:]\n",
    "# for i in range(len(sentiment_words)):\n",
    "#     print(sentiment_words[i] + \": \" + str(O[i][0]) + \"//\" + str(O[i][1]))\n",
    "for prev_days in range(-1,11):\n",
    "    print('Prev days: ' + str(prev_days))\n",
    "    # Collect out of sample articles\n",
    "\n",
    "    trial_id = '2013-05-01-ew-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    curr_date = possible_dates[min(prev_days,0)]\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('long value'), str('EARNING LONG'), str('short value'), str('EARNING SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    portfolio_value = 10000\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        # article_date_1 = article_date + dt.timedelta(days=1)\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        # total_earnings_long = 0\n",
    "        # total_earnings_short = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_top = sum([prev_top[t] for t in prev_top])\n",
    "                for tick in prev_top:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        investment = portfolio_value*(1/len(prev_top))\n",
    "                        # investment = portfolio_value*(prev_top[tick]/total_sum_top)\n",
    "                        earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                        investment_long += investment\n",
    "                        # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                        # csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_bot = sum([prev_bot[t] for t in prev_bot])\n",
    "                for tick in prev_bot:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                        investment = portfolio_value*(1/len(prev_bot))\n",
    "                        # investment = portfolio_value*(prev_bot[tick]/total_sum_bot)\n",
    "                        earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                        investment_short += investment\n",
    "                        # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_d.append(oos_bow)\n",
    "            \n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0.5\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                testing_s = sum(oos_bow.get(w,0) for w in sentiment_words)\n",
    "                if (testing_s > 0):\n",
    "                    est_p = fminbound(equation_to_solve, 0, 1, (O,oos_bow, sentiment_words,testing_s,LAM))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            # ensure the portfolio can't buy the same stock for both long and short (because that's SILLY)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items() if val > 0.5}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items() if val < 0.5}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((curr_date).date()), str(investment_long), str(earning_long), str(investment_short), str(earning_short), str(len(prev_top)), str(len(prev_bot)), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value weighted strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n",
      "neutral: 0.028112281337826537//0.03788081623962594\n",
      "upgrade: 0.08477955395321501//0.04041305299806427\n",
      "buy: 0.10101603882844185//0.0594780310378326\n",
      "jack: 0.0003082304464546281//0.0023568229914368724\n",
      "loser: 0.008666302582382482//0.056943556447852525\n",
      "miss: 0.004744626722759458//0.006615791034511048\n",
      "rais: 0.08854163745835934//0.05883520740477709\n",
      "replace: 0.001778222328543531//0.0014911273645555122\n",
      "proceed: 0.003287108729308542//0.00237986508127797\n",
      "nuclear: 0.00043207860476657847//0.0014342035951465642\n",
      "lower: 0.032457915999656264//0.0706361876516049\n",
      "offer: 0.029806144051811184//0.034763231398137875\n",
      "pressure: 0.0003931779221157448//0.00455028696440091\n",
      "bill: 0.002868935768281278//0.0013894763442133659\n",
      "volume: 0.011972706794278903//0.0037549127844452297\n",
      "improving: 0.0009499260719184737//0.0011256903173662727\n",
      "strength: 0.0017213619950899905//0.0021744584240629495\n",
      "fall: 0.010082377133565011//0.020304269566622145\n",
      "overweight: 0.014872517278038873//0.007948402539532653\n",
      "gainer: 0.03504081486760051//0.00696355378821905\n",
      "downgrade: 0.04945004608405729//0.10344254318516766\n",
      "valuation: 0.003011436235510071//0.003654066865390233\n",
      "strong: 0.010367942313143759//0.011387623193723849\n",
      "mover: 0.04093099239290451//0.010090289118023957\n",
      "high: 0.04293441225786055//0.015826675445054575\n",
      "solid: 0.004006311740292073//0.002230609607758601\n",
      "mention: 0.0016414841606966347//0.001263365151121388\n",
      "mix: 0.009068722703462767//0.012066481146403773\n",
      "cut: 0.0034172605916972854//0.012953731744269002\n",
      "resume: 0.0073831373078936825//0.009176982289450021\n",
      "higher: 0.04136116479995132//0.030622743728353267\n",
      "atlantic: 0.0013067386607433214//0.0009867306753593274\n",
      "date: 0.002346019773962963//0.0024866393239452366\n",
      "outperform: 0.03420076989170911//0.02280505065154523\n",
      "prelim: 0.0027763894470841173//0.0031965190257173858\n",
      "worst: 0.0010654932606377128//0.014263141632045489\n",
      "public: 0.0041737188898335964//0.004196621074721198\n",
      "build: 0.0037120919247681915//0.003334019642118421\n",
      "material: 0.005402218135734719//0.0023007659205591755\n",
      "downbeat: 0.003711146295512687//0.0056976112069720645\n",
      "concern: 0.002480725837150721//0.004752873807314765\n",
      "fitch: 0.004125788963429101//0.0041922405213955785\n",
      "attract: 0.0023837124767245275//0.0020874068282173562\n",
      "spike: 0.013376175906542711//0.007111615055713191\n",
      "lift: 0.00329164105811321//0.0027102628484573163\n",
      "underweight: 0.001136008349222452//0.002893437740401096\n",
      "low: 0.008461547885688295//0.028330316101213997\n",
      "placement: 0.001455061939512035//0.0005211735238679834\n",
      "peer: 0.000743886516947397//0.001091837949531669\n",
      "remove: 0.0027102568420225705//0.0044718239961064115\n",
      "feel: 0.0007164394030303529//0.001261367726917486\n",
      "uncertainty: 0.0007773352111593724//0.002266043914499146\n",
      "clearance: 0.0010048896815599982//0.0014240835914293756\n",
      "wont: 0.0010230568843169884//0.0015174599094751819\n",
      "plummet: 0.0005193542689827308//0.0015837270920872576\n",
      "chronic: 0.0002886109465098539//0.0017915190768039856\n",
      "accept: 0.0028828370014741247//0.0018867355155922055\n",
      "shire: 0.005682005140661859//0.0\n",
      "interim: 0.0014512183843035126//0.001997155157977653\n",
      "loss: 0.007810452765257092//0.014624176210275964\n",
      "common: 0.0041912603695770725//0.005775716503954091\n",
      "shelf: 0.0027559095210093407//0.003795605786547786\n",
      "repurchase: 0.0035670349861128886//0.0028164103909301716\n",
      "cite: 0.0012086850506775786//0.0017453104330265724\n",
      "downside: 0.0008648764604100341//0.0017974501952559415\n",
      "fail: 0.00048010934790082967//0.00248289539760741\n",
      "completion: 0.001142590058000988//0.0008079859792289277\n",
      "replacement: 0.0009913491150722186//0.0015391257963985059\n",
      "bond: 0.0021925634418678824//0.006281642260165284\n",
      "navy: 0.0011111791036968208//0.0013520168182474465\n",
      "weight: 0.0011521497893354365//0.0002841496803748373\n",
      "rumor: 0.006563306343937557//0.004098066148807788\n",
      "tumble: 0.003906244644277723//0.0058217367687202655\n",
      "press: 0.000726965799364257//0.0018409334613425435\n",
      "extension: 0.0015787543664187243//0.0017442739479846525\n",
      "standpoint: 0.001273502873185777//0.0005060787055169573\n",
      "hunt: 0.00028046689808354925//0.0015605529716337513\n",
      "weak: 0.003912218676865907//0.009962646318702835\n",
      "rail: 0.0004839514329271968//0.001236612136204935\n",
      "la: 0.002318595710883576//0.0007791936345550503\n",
      "connect: 0.0029794860345765603//0.0005192973599186979\n",
      "commerce: 0.00034503491910915627//0.0015797603369307705\n",
      "threat: 0.0005712792818659333//0.0014892098744706838\n",
      "agency: 0.0019395247125834265//0.001483017387383351\n",
      "art: 0.0001268302973779005//0.0036761462778153943\n",
      "east: 0.0005168147518586976//0.0021606430117431462\n",
      "period: 0.0009074182663256251//0.0013963934616550609\n",
      "joint: 0.0024987237774314478//0.0012880026342948206\n",
      "large: 0.0019118603623929434//0.0019363811560727368\n",
      "stress: 0.0014318002723382956//0.0016003960527640174\n",
      "shop: 0.0016424447210681644//0.0014114223286062753\n",
      "battle: 0.0008940120771844881//0.0013181613403016126\n",
      "p: 0.0018664459500284//0.0005661716782707187\n",
      "transfer: 0.0018560639822314392//0.0010505595733702364\n",
      "innovation: 0.002222932077487151//0.0008353848236540768\n",
      "defend: 0.0006988921990180079//0.0012329662763607087\n",
      "bath: 0.0020495307417227855//0.001360160495391992\n",
      "probe: 0.0010307010842981526//0.0019908371246468016\n",
      "litigation: 0.001799935839955332//0.001564362857892238\n",
      "interact: 0.0012931462332427899//0.0008088468409760784\n",
      "appeal: 0.0017516935794213474//0.0021448661417386465\n",
      "smith: 0.004124056924296799//0.0\n",
      "nephew: 0.0011871435883441797//0.0\n",
      "staple: 0.0020023575598865197//0.00011602661908026969\n",
      "dynamic: 0.002654183390522667//0.0024542169341297214\n",
      "ing: 0.0018857473915748122//0.0\n",
      "candid: 0.0011723563505493969//0.0012829156000310703\n",
      "airway: 0.00045296171953050613//0.0014801872713712872\n",
      "love: 0.0017525843749288058//0.0019376698498333917\n",
      "watcher: 0.0007411140771109546//0.0003559770602004735\n",
      "lone: 0.0005680918999758388//0.000808163085152064\n",
      "pine: 0.0004680257454175857//0.0004494414071754073\n",
      "silicon: 0.002058613446155285//0.0\n",
      "sink: 8.78428712822774e-05//0.002291200752164812\n",
      "roundup: 0.0014069224838256956//0.0015492429882343075\n",
      "debut: 0.00035741967963992175//0.0016206288600778712\n",
      "modest: 0.0003964387726499108//0.002682090412858253\n",
      "jazz: 0.0//0.0022284494863934585\n",
      "publish: 0.001199289494179856//0.0011842423365155258\n",
      "plunge: 0.0010953136397039687//0.0020578120407623588\n",
      "shift: 0.0008150427937829039//0.0014106834854696279\n",
      "storage: 0.00398654291839672//0.0005005718744037895\n",
      "l: 0.0014473146082758124//0.0012802986757532813\n",
      "convict: 0.000495892435834051//0.0006479223153249582\n",
      "retreat: 0.0015312058295437418//0.0014066658182183248\n",
      "commodity: 0.000687559773293874//0.002361506017991235\n",
      "export: 0.00034118800541112085//0.0021275828275830223\n",
      "farmer: 0.001894898358318246//0.0006299679414158298\n",
      "drink: 0.0016844630531160044//0.0004226116278970821\n",
      "impact: 0.00194057199481909//0.0039429521118971075\n",
      "commission: 0.0007748956799098583//0.001239664476055122\n",
      "field: 0.0002561488119641243//0.002285092110267937\n",
      "secondary: 0.0016545349326271552//0.0020760637924303283\n",
      "every: 0.000789313068430659//0.0014143358637661607\n",
      "act: 0.0009206553926115636//0.0011588532630098247\n",
      "salon: 0.0017243327071988501//0.0\n",
      "entertain: 0.002796478231870677//0.0018675019486731686\n",
      "unusual: 0.002055470898076996//0.0007984148036334255\n",
      "cliff: 0.0//0.006703590650451624\n",
      "fourth: 0.000584956238096482//0.001301364865064365\n",
      "disappoint: 0.0007271417086824132//0.0044025709959410915\n",
      "alto: 0.0034092476700313813//0.00021774492859610116\n",
      "outfitter: 0.0056448633812449175//0.0\n",
      "survey: 0.0008219141958857272//0.0010120264455280858\n",
      "myriad: 0.002494817638321971//0.0011069516415714022\n",
      "southern: 0.002779991649636676//0.0010832656308926143\n",
      "journal: 0.001117699024361075//0.0009232598661826667\n",
      "suffer: 0.000769034913392074//0.0020263357127000316\n",
      "brown: 0.0//0.0019773400087791907\n",
      "kindred: 0.0032425903052966856//0.0\n",
      "warn: 0.0006795247935004737//0.003085776357806502\n",
      "tenet: 0.0019798035493824883//0.0\n",
      "within: 0.0011173762519382775//0.0006751521273348931\n",
      "accident: 0.0010428686679022792//0.0013020609335205149\n",
      "southwestern: 0.002036272949261621//0.0\n",
      "robin: 0.0019665366606750312//0.0\n",
      "inch: 0.000746211886529308//0.0004846775188824116\n",
      "closer: 0.0005375612340974817//0.0009153118704201193\n",
      "lodging: 0.0008073171349199684//0.0026282419299557855\n",
      "crash: 0.0007685827447076811//0.0014033489911006757\n",
      "micron: 0.0063894118362433465//0.0009655563393859201\n",
      "choice: 1.623959163178863e-05//0.0018673528337511118\n",
      "tri: 0.0011053232737539818//0.0010628579330773204\n",
      "century: 0.0013207706861964165//0.0005805342145416685\n",
      "transocean: 0.004718927876627654//0.0\n",
      "ackman: 0.0009934361398020266//0.0006908002685573994\n",
      "broker: 0.001028985548972144//0.0014479337989394766\n",
      "collin: 0.0//0.0028030880803554714\n",
      "soar: 0.0025718717869148157//0.0013141142761085333\n",
      "dot: 1.301177298844867e-05//0.0019926313520766325\n",
      "foot: 0.0005077760989392388//0.0016563889531001207\n",
      "locker: 0.0001268580527984564//0.0011667964359697478\n",
      "flu: 0.0013803299773544162//0.001101441674784443\n",
      "vale: 0.0017729959469748638//0.0\n",
      "signet: 0.0026613116267041924//0.0\n",
      "pioneer: 0.0014545752417788407//0.000880294275980321\n",
      "safety: 0.0011211774888301546//0.0025298190090922387\n",
      "charter: 0.00021509674027844796//0.002068149027993267\n",
      "rig: 0.0013036524651517092//0.00023994968276431618\n",
      "sanction: 0.00047761815795720967//0.0015432926211761248\n",
      "weather: 0.0003604172192011524//0.0016573181180024254\n",
      "kohl: 0.0016227881968112956//0.000578959049713614\n",
      "soup: 0.0//0.0027541664822904655\n",
      "lumber: 0.004620740698489848//0.00231683207351246\n",
      "discover: 0.0//0.0028468882454744217\n",
      "pinnacle: 0.0016832924104583325//0.0001449266247278702\n",
      "soon: 0.0007748052947294546//0.0017408737358025905\n",
      "lam: 0.0016117626129398255//1.6325825938980685e-05\n",
      "dominion: 0.0005624623162301127//0.0018430384953950516\n",
      "boston: 0.0//0.007682513429908318\n",
      "monster: 0.0034790471467772794//0.0003378546635971753\n",
      "train: 0.0005120613427263268//0.0015374973762411457\n",
      "fisher: 0.0013184541484977683//0.0001362364115710118\n",
      "thermo: 0.0012525847545986262//0.0\n",
      "governor: 0.0//0.00424326078843613\n",
      "starboard: 0.0008553429414792733//0.0013565322311085675\n",
      "vera: 0.0020288387506391414//0.0\n",
      "Prev days: -1\n",
      "Prev days: 0\n",
      "Prev days: 1\n",
      "Prev days: 2\n",
      "Prev days: 3\n",
      "Prev days: 4\n",
      "Prev days: 5\n",
      "Prev days: 6\n",
      "Prev days: 7\n",
      "Prev days: 8\n",
      "New folder ./data/out-of-sample/2013-05-01-vw-day+8 created\n",
      "Prev days: 9\n",
      "New folder ./data/out-of-sample/2013-05-01-vw-day+9 created\n",
      "Prev days: 10\n",
      "New folder ./data/out-of-sample/2013-05-01-vw-day+10 created\n"
     ]
    }
   ],
   "source": [
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "prev_days = 7\n",
    "best_config_file = './data/models/stemming/word-lists/2013-5-1.csv'\n",
    "sentiment_words = []\n",
    "O = np.array([0,0])\n",
    "with open(best_config_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "O = O[1:]\n",
    "for i in range(len(sentiment_words)):\n",
    "    print(sentiment_words[i] + \": \" + str(O[i][0]) + \"//\" + str(O[i][1]))\n",
    "for prev_days in range(-1,11):\n",
    "    print('Prev days: ' + str(prev_days))\n",
    "    # Collect out of sample articles\n",
    "    trial_id = '2013-05-01-vw-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    curr_date = possible_dates[min(prev_days,0)]\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('long value'), str('EARNING LONG'), str('short value'), str('EARNING SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    portfolio_value = 10000\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        # article_date_1 = article_date + dt.timedelta(days=1)\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        # total_earnings_long = 0\n",
    "        # total_earnings_short = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_top = sum([prev_top[t] for t in prev_top])\n",
    "                for tick in prev_top:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # investment = portfolio_value*(1/len(prev_top))\n",
    "                        investment = portfolio_value*(prev_top[tick]/total_sum_top)\n",
    "                        earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                        investment_long += investment\n",
    "                        # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                        # csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_bot = sum([prev_bot[t] for t in prev_bot])\n",
    "                for tick in prev_bot:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                        # investment = portfolio_value*(1/len(prev_bot))\n",
    "                        investment = portfolio_value*(prev_bot[tick]/total_sum_bot)\n",
    "                        earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                        investment_short += investment\n",
    "                        # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_d.append(oos_bow)\n",
    "            \n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0.5\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                testing_s = sum(oos_bow.get(w,0) for w in sentiment_words)\n",
    "                if (testing_s > 0):\n",
    "                    est_p = fminbound(equation_to_solve, 0, 1, (O,oos_bow, sentiment_words,testing_s,LAM))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            # ensure the portfolio can't buy the same stock for both long and short (because that's SILLY)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items() if val > 0.5}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items() if val < 0.5}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((curr_date).date()), str(investment_long), str(earning_long), str(investment_short), str(earning_short), str(len(prev_top)), str(len(prev_bot)), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w/ bigrams\n",
    "Construct portfolios using the model trained by bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EW\n",
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "prev_days = 7\n",
    "best_config_file = './data/models/stemming/word-lists/2013-5-1.csv'\n",
    "best_config_bigram_file = './data/models/bigrams-stem/word-lists/2010-1-1.csv'\n",
    "sentiment_words = []\n",
    "O = np.array([0,0])\n",
    "with open(best_config_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "with open(best_config_bigram_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "O = O[1:]\n",
    "\n",
    "print(sentiment_words)\n",
    "# change this value to calculate from day t + x to t + y\n",
    "for prev_days in range(1,2):\n",
    "    print('Prev days: ' + str(prev_days))\n",
    "    # Collect out of sample articles\n",
    "\n",
    "    trial_id = '2013-05-01-111-+bigram-ew-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    curr_date = possible_dates[min(prev_days,0)]\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('long value'), str('EARNING LONG'), str('short value'), str('EARNING SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    # amount invested\n",
    "    portfolio_value = 10000\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_top = sum([prev_top[t] for t in prev_top])\n",
    "                for tick in prev_top:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        investment = portfolio_value*(1/len(prev_top))\n",
    "                        # investment = portfolio_value*(prev_top[tick]/total_sum_top)\n",
    "                        earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                        investment_long += investment\n",
    "                        # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                        # csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_bot = sum([prev_bot[t] for t in prev_bot])\n",
    "                for tick in prev_bot:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                        investment = portfolio_value*(1/len(prev_bot))\n",
    "                        # investment = portfolio_value*(prev_bot[tick]/total_sum_bot)\n",
    "                        earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                        investment_short += investment\n",
    "                        # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            oos_d_bigram = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_bow.update(text_to_bow_bigram(oos_a['headline']))\n",
    "                oos_d.append(oos_bow)\n",
    "                # oos_bow_bigram = text_to_bow_bigram(oos_a['headline'])\n",
    "                # oos_d.append(oos_bow)\n",
    "\n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0.5\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                testing_s = sum(oos_bow.get(w,0) for w in sentiment_words)\n",
    "                if (testing_s > 0):\n",
    "                    est_p = fminbound(equation_to_solve, 0, 1, (O,oos_bow, sentiment_words,testing_s,LAM))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            # ensure the portfolio can't buy the same stock for both long and short (because that's SILLY)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items() if val > 0.5}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items() if val < 0.5}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((curr_date).date()), str(investment_long), str(earning_long), str(investment_short), str(earning_short), str(len(prev_top)), str(len(prev_bot)), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n",
      "['neutral', 'upgrade', 'buy', 'jack', 'loser', 'miss', 'rais', 'replace', 'proceed', 'nuclear', 'lower', 'offer', 'pressure', 'bill', 'volume', 'improving', 'strength', 'fall', 'overweight', 'gainer', 'downgrade', 'valuation', 'strong', 'mover', 'high', 'solid', 'mention', 'mix', 'cut', 'resume', 'higher', 'atlantic', 'date', 'outperform', 'prelim', 'worst', 'public', 'build', 'material', 'downbeat', 'concern', 'fitch', 'attract', 'spike', 'lift', 'underweight', 'low', 'placement', 'peer', 'remove', 'feel', 'uncertainty', 'clearance', 'wont', 'plummet', 'chronic', 'accept', 'shire', 'interim', 'loss', 'common', 'shelf', 'repurchase', 'cite', 'downside', 'fail', 'completion', 'replacement', 'bond', 'navy', 'weight', 'rumor', 'tumble', 'press', 'extension', 'standpoint', 'hunt', 'weak', 'rail', 'la', 'connect', 'commerce', 'threat', 'agency', 'art', 'east', 'period', 'joint', 'large', 'stress', 'shop', 'battle', 'p', 'transfer', 'innovation', 'defend', 'bath', 'probe', 'litigation', 'interact', 'appeal', 'smith', 'nephew', 'staple', 'dynamic', 'ing', 'candid', 'airway', 'love', 'watcher', 'lone', 'pine', 'silicon', 'sink', 'roundup', 'debut', 'modest', 'jazz', 'publish', 'plunge', 'shift', 'storage', 'l', 'convict', 'retreat', 'commodity', 'export', 'farmer', 'drink', 'impact', 'commission', 'field', 'secondary', 'every', 'act', 'salon', 'entertain', 'unusual', 'cliff', 'fourth', 'disappoint', 'alto', 'outfitter', 'survey', 'myriad', 'southern', 'journal', 'suffer', 'brown', 'kindred', 'warn', 'tenet', 'within', 'accident', 'southwestern', 'robin', 'inch', 'closer', 'lodging', 'crash', 'micron', 'choice', 'tri', 'century', 'transocean', 'ackman', 'broker', 'collin', 'soar', 'dot', 'foot', 'locker', 'flu', 'vale', 'signet', 'pioneer', 'safety', 'charter', 'rig', 'sanction', 'weather', 'kohl', 'soup', 'lumber', 'discover', 'pinnacle', 'soon', 'lam', 'dominion', 'boston', 'monster', 'train', 'fisher', 'thermo', 'governor', 'starboard', 'vera', 'notable put', 'put option', 'option activity', 'senior note', 'note due', 'q v', 'report q', 'revenue b', 'financial breakfast', 'breakfast morn', 'morn news', 'news summary', 'earn preview', 'expect rise', 'sign agreement', 'long short', 'basic material', 'look ahead', 'oil gas', 'consumer good', 'sector recap', 'ratio leader', 'previous quarter', 'gainer loser', 'natural resource', 'award million', 'overweight rate', 'gold silver', 'round investor', 'perform industry', 'march th', 'option brief', 'first quarter', 'tale tape', 'remain neutral', 'million contract', 'fourth quarter', 'maintain neutral', 'upbeat q', 'operating margin', 'next week', 'health care', 'week high', 'profit margin', 'standpoint research', 'creat new', 'auto part', 'open near', 'beat estimate', 'news corporate', 'second quarter', 'call sold', 'hit week', 'metal mine', 'move higher', 'volume mover', 'report upbeat', 'watch fresh', 'fresh week', 'high low', 'bank lynch', 'lower po', 'rise previous', 'round like', 'highest operating', 'notable call', 'call option', 'future signal', 'start wall', 'wall street', 'interest rate', 'unconfirm rumor', 'win contract', 'share repurchase', 'repurchase program', 'heavy volume', 'fall previous', 'conference call', 'new york', 'realty trust', 'per share', 'spike higher', 'offer common', 'dividend cent', 'investor idea', 'million order', 'may comparable', 'resume trade', 'fast money', 'money pick', 'afternoon mover', 'global x', 'winner loser', 'highest gross', 'public offer', 'gainer th', 'trade start', 'joint venture', 'award contract', 'retirement plan', 'investor await', 'crude oil', 'invest idea', 'insider trade', 'narrow base', 'base index', 'hedge fund', 'u equity', 'equity market', 'boston scientific', 'higher heavy', 'tender offer', 'adobe system', 'agreement acquire', 'rais po', 'prior quarter', 'c bac', 'late market', 'june th', 'piper reiter', 'hour gainer', 'clinic trial', 'net income', 'bull bear', 'bear day', 'day highlight', 'central bank', 'jobless claim', 'time warner', 'credit card', 'alliance data', 'utility lead', 'first solar', 'retail sale', 'bull day', 'trade alert', 'natural gas', 'gold miner', 'eagle outfitter', 'worth look', 'dick sport', 'urban outfitter', 'material lag', 'material lead', 'micron technology', 'lead lag', 'health system', 'care lead', 'intern paper', 'money recap', 'perform period', 'bookkeeping week', 'chang fund', 'fund posit', 'posit year', 'th bal', 'gainer fa', 'broker forgot', 'forgot mention', 'enter oversold', 'duke energy', 'franklin resource', 'aggress growth', 'daily small', 'general mill', 'near session', 'session high', 'spike lower', 'decker outdoor', 'dollar tree', 'lead basic', 'electron art', 'clean energy', 'discover financial', 'capital one', 'bath beyond', 'bed bath', 'rang resource', 'metal mix', 'big lot', 'ahead next', 'northern trust', 'wast management', 'vertex pharmaceutical', 'general dynamic', 'fifth third', 'delta air', 'air line', 'marathon oil', 'western union', 'session low', 'community health', 'st joe', 'lead consumer', 'good lag', 'dish network', 'marvel technology', 'lead health', 'metal higher', 'new session', 'care lag']\n",
      "Prev days: 1\n",
      "New folder ./data/out-of-sample/2013-05-01-+bigram-vw-day+1 created\n"
     ]
    }
   ],
   "source": [
    "# VW\n",
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "prev_days = 7\n",
    "best_config_file = './data/models/stemming/word-lists/2013-5-1.csv'\n",
    "best_config_bigram_file = './data/models/bigrams-stem/word-lists/2010-1-1.csv'\n",
    "sentiment_words = []\n",
    "O = np.array([0,0])\n",
    "with open(best_config_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "with open(best_config_bigram_file, encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            sentiment_words.append(row[0])\n",
    "            O = np.vstack((O,[row[1], row[2]]))\n",
    "        line_count += 1\n",
    "O = O[1:]\n",
    "\n",
    "print(sentiment_words)\n",
    "# change this value to calculate from day t + x to t + y\n",
    "for prev_days in range(1,2):\n",
    "    print('Prev days: ' + str(prev_days))\n",
    "    # Collect out of sample articles\n",
    "\n",
    "    trial_id = '2013-05-01-+bigram-vw-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    curr_date = possible_dates[min(prev_days,0)]\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('long value'), str('EARNING LONG'), str('short value'), str('EARNING SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    # amount invested\n",
    "    portfolio_value = 10000\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_top = sum([prev_top[t] for t in prev_top])\n",
    "                for tick in prev_top:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # investment = portfolio_value*(1/len(prev_top))\n",
    "                        investment = portfolio_value*(prev_top[tick]/total_sum_top)\n",
    "                        earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                        investment_long += investment\n",
    "                        # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                        # csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                # with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                #     csvwriter = csv.writer(csv_file)\n",
    "                total_sum_bot = sum([prev_bot[t] for t in prev_bot])\n",
    "                for tick in prev_bot:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        # csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                        # investment = portfolio_value*(1/len(prev_bot))\n",
    "                        investment = portfolio_value*(prev_bot[tick]/total_sum_bot)\n",
    "                        earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                        investment_short += investment\n",
    "                        # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            oos_d_bigram = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow(oos_a['headline'])\n",
    "                oos_bow.update(text_to_bow_bigram(oos_a['headline']))\n",
    "                oos_d.append(oos_bow)\n",
    "                # oos_bow_bigram = text_to_bow_bigram(oos_a['headline'])\n",
    "                # oos_d.append(oos_bow)\n",
    "\n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0.5\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                testing_s = sum(oos_bow.get(w,0) for w in sentiment_words)\n",
    "                if (testing_s > 0):\n",
    "                    est_p = fminbound(equation_to_solve, 0, 1, (O,oos_bow, sentiment_words,testing_s,LAM))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            # ensure the portfolio can't buy the same stock for both long and short (because that's SILLY)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items() if val > 0.5}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items() if val < 0.5}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((curr_date).date()), str(investment_long), str(earning_long), str(investment_short), str(earning_short), str(len(prev_top)), str(len(prev_bot)), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LM and H4 lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form each word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc negative and positive word lists for LM\n",
    "positive_words_lm = {}\n",
    "negative_words_lm = {}\n",
    "with open('../external-csvs/LM-Dictionary-1993-2021.csv', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            if row[8] != str(0):\n",
    "                positive_words_lm[str(row[0]).lower()] = 0\n",
    "            if row[7] != str(0):\n",
    "                negative_words_lm[str(row[0]).lower()] = 0\n",
    "            # sentiment_score[row[0]] = {'neg': float(row[8]), 'pos': float(row[9])}\n",
    "        line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc negative and positive word lists for h4\n",
    "positive_words_h4 = {}\n",
    "negative_words_h4 = {}\n",
    "with open('../external-csvs/HIV-4.csv', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    # FORMAT: line#,headline,date,stock\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            if row[2] != \"\":\n",
    "                positive_words_h4[str(row[0]).lower()] = 0\n",
    "            if row[3] != \"\":\n",
    "                negative_words_h4[str(row[0]).lower()] = 0\n",
    "            # sentiment_score[row[0]] = {'neg': float(row[8]), 'pos': float(row[9])}\n",
    "        line_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LM portfolio formation\n",
    "TODO: change positive words to positive_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n"
     ]
    }
   ],
   "source": [
    "# Collect out of sample articles\n",
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "for prev_days in range(4,8):\n",
    "    trial_id = 'LM-EW-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    # sentiment_words = []\n",
    "    # sentiment_score = {}\n",
    "    # with open('../external-csvs/LM-Dictionary-1993-2021.csv', encoding='utf-8') as csv_file:\n",
    "    #     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    #     line_count = 0\n",
    "    #     # FORMAT: line#,headline,date,stock\n",
    "    #     for row in csv_reader:\n",
    "    #         if line_count > 0:\n",
    "    #             sentiment_score[row[0]] = {'neg': float(row[8]), 'pos': float(row[9])}\n",
    "    #         line_count += 1\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('EARNING LONG'), str('INVESTMENT LONG'), str('EARNING SHORT'), str('INVESTMENT SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TURNOVER %'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        # article_date_1 = article_date + dt.timedelta(days=1)\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        # calculate tfidf for each day\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        # total_earnings_long = 0\n",
    "        # total_earnings_short = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        portfolio_value = 10000\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                    csvwriter = csv.writer(csv_file)\n",
    "                    for tick in prev_top:\n",
    "                        testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                        if not testing_value == 'E':\n",
    "                            investment = portfolio_value*(1/len(prev_top))\n",
    "                            earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                            investment_long += investment\n",
    "                            # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                            csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                            if (prev_top[tick] - testing_value <= 0):\n",
    "                                long_correct += 1\n",
    "                with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                    csvwriter = csv.writer(csv_file)\n",
    "                    for tick in prev_bot:\n",
    "                        testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                        if not testing_value == 'E':\n",
    "                            csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                            investment = portfolio_value*(1/len(prev_bot))\n",
    "                            earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                            investment_short += investment\n",
    "                            # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "                            if (prev_bot[tick] - testing_value > 0):\n",
    "                                short_correct += 1\n",
    "                    #else what? Assume not correct? Assume correct? I have chosen to assume incorrect\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow_raw(oos_a['headline'])\n",
    "                oos_d.append(oos_bow)\n",
    "            for w in positive_words_lm:\n",
    "                # tf = \n",
    "                idf = len(oos_d) / (1+(len([a for a in oos_d if w in a])))\n",
    "                if idf > len(oos_d):\n",
    "                    print(str(w) + \" // \" + str(idf))\n",
    "                positive_words_lm[w] = math.log(idf)\n",
    "            # print(oos_d[0])\n",
    "            # print('coverage' in oos_d[0])\n",
    "            # print([a for a in oos_d if 'miss' in a])\n",
    "            for w in negative_words_lm:\n",
    "                # tf = \n",
    "                idf = len(oos_d) / (1+(len([a for a in oos_d if w in a])))\n",
    "                if idf > len(oos_d):\n",
    "                    print(str(w) + \" // \" + str(idf))\n",
    "                negative_words_lm[w] = math.log(idf)\n",
    "            \n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                for w in oos_bow:\n",
    "                    # if w in positive_words or w in negative_words:\n",
    "                    est_p += (positive_words_lm.get(w,0)*math.log(1+oos_bow.get(w,0)) - negative_words_lm.get(w,0)*math.log(1+oos_bow.get(w,0)))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items()}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items()}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "            if (len(prev_top) > 0):\n",
    "                diff_stocks /= (len(prev_bot) + len(prev_top))\n",
    "            else:\n",
    "                diff_stocks = 0\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((prev_date).date()), str(earning_long), str(investment_long), str(earning_short), str(investment_short), str(len(prev_top)), str(len(prev_bot)), str(diff_stocks), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))\n",
    "        curr_date = curr_date + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4 portfolio formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223227\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collect out of sample articles\n",
    "oos_start_year = 2019\n",
    "oos_start_month = 1\n",
    "# oos_start_year = 2020\n",
    "# oos_start_month = 5\n",
    "total_outgoings = 0\n",
    "total_earnings = 0\n",
    "start_date = datetime(oos_start_year, oos_start_month, 1, 0,0,0,0,est)\n",
    "oos_arts   = [a for a in article_list if (a['date'] >= start_date)]\n",
    "print(len(oos_arts))\n",
    "curr_day = 1\n",
    "LAM = 5\n",
    "\n",
    "for prev_days in range(-1,8):\n",
    "    trial_id = 'H4-EW-day+' + str(prev_days)\n",
    "    curr_month = oos_start_month\n",
    "    curr_year = oos_start_year\n",
    "    destination_directory = os.path.join('./data/out-of-sample/', trial_id)\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.mkdir(destination_directory)\n",
    "        # os.mkdir(destination_directory, 'portfolios')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios'))\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/short'))\n",
    "        # os.mkdir(destination_directory + '/portfolios/short')\n",
    "        os.mkdir(os.path.join(destination_directory,'portfolios/long'))\n",
    "        print('New folder ' + str(destination_directory) + ' created')\n",
    "\n",
    "\n",
    "    # sentiment_words = []\n",
    "    # sentiment_score = {}\n",
    "    # with open('../external-csvs/LM-Dictionary-1993-2021.csv', encoding='utf-8') as csv_file:\n",
    "    #     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    #     line_count = 0\n",
    "    #     # FORMAT: line#,headline,date,stock\n",
    "    #     for row in csv_reader:\n",
    "    #         if line_count > 0:\n",
    "    #             sentiment_score[row[0]] = {'neg': float(row[8]), 'pos': float(row[9])}\n",
    "    #         line_count += 1\n",
    "    list_dates = [a['date'] for a in oos_arts]\n",
    "    end_date = max(list_dates)\n",
    "    curr_date = datetime(curr_year, curr_month, curr_day, 9, 0,0,0,est)\n",
    "    prev_date = curr_date - dt.timedelta(days=1)\n",
    "    prev_top = {}\n",
    "    prev_bot = {}\n",
    "    output_file = os.path.join(destination_directory, 'estimations.csv')\n",
    "\n",
    "    #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "    #empty file\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow([str('DATE'), str('EARNING LONG'), str('INVESTMENT LONG'), str('EARNING SHORT'), str('INVESTMENT SHORT'), str('NUMBER LONG'), str('NUMBER SHORT'), str('TURNOVER %'), str('TOTAL FIRMS WITH ARTS'), str('HEADLINES WITH SENTIMENT WORDS')])\n",
    "\n",
    "    for curr_t in range(max(prev_days,0), len(possible_dates) + min(prev_days,0)-1):\n",
    "        curr_date = possible_dates[curr_t]\n",
    "        # calculate new date and pull new articles\n",
    "        article_date = possible_dates[curr_t - prev_days]\n",
    "        article_date_1 = possible_dates[(curr_t - prev_days)+1]\n",
    "        # article_date_1 = article_date + dt.timedelta(days=1)\n",
    "        #pull articles from previous day to work out what stocks to buy today\n",
    "        daily_arts = [a for a in oos_arts if (article_date < a['date']) and (article_date_1 > a['date'])]\n",
    "        # calculate tfidf for each day\n",
    "        long_correct = 0\n",
    "        short_correct = 0\n",
    "        # total_earnings_long = 0\n",
    "        # total_earnings_short = 0\n",
    "        investment_long = 0\n",
    "        investment_short = 0\n",
    "        earning_long = 0\n",
    "        earning_short = 0\n",
    "        portfolio_value = 10000\n",
    "        if (len(daily_arts) > 0 and not stock_data['A']['Open'].get(str(curr_date.date()),'E') == 'E'):\n",
    "            #calculate how many guesses were right from yesterday\n",
    "            if len(prev_bot) > 0:\n",
    "                with open(os.path.join(destination_directory, 'portfolios/long/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                    csvwriter = csv.writer(csv_file)\n",
    "                    for tick in prev_top:\n",
    "                        testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                        if not testing_value == 'E':\n",
    "                            investment = portfolio_value*(1/len(prev_top))\n",
    "                            earning_long += (testing_value)*(investment/prev_top[tick])\n",
    "                            investment_long += investment\n",
    "                            # total_earnings_long += (testing_value - prev_top[tick]) * (1/len(prev_top))\n",
    "                            csvwriter.writerow([str(tick), prev_top[tick], testing_value])\n",
    "                            if (prev_top[tick] - testing_value <= 0):\n",
    "                                long_correct += 1\n",
    "                with open(os.path.join(destination_directory, 'portfolios/short/' + str(prev_date.date()) + '.csv'), 'w', newline='') as csv_file:\n",
    "                    csvwriter = csv.writer(csv_file)\n",
    "                    for tick in prev_bot:\n",
    "                        testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                        if not testing_value == 'E':\n",
    "                            csvwriter.writerow([str(tick), prev_bot[tick], testing_value])\n",
    "                            investment = portfolio_value*(1/len(prev_bot))\n",
    "                            earning_short += testing_value*(investment/prev_bot[tick])\n",
    "                            investment_short += investment\n",
    "                            # total_earnings_short +=  (prev_bot[tick] - testing_value)*(1/len(prev_bot))\n",
    "                            if (prev_bot[tick] - testing_value > 0):\n",
    "                                short_correct += 1\n",
    "                    #else what? Assume not correct? Assume correct? I have chosen to assume incorrect\n",
    "\n",
    "            #preprocess arts\n",
    "            oos_d = []\n",
    "            for oos_a in daily_arts:\n",
    "                oos_bow = text_to_bow_raw(oos_a['headline'])\n",
    "                oos_d.append(oos_bow)\n",
    "            for w in positive_words_h4:\n",
    "                # tf = \n",
    "                idf = len(oos_d) / (1+(len([a for a in oos_d if w in a])))\n",
    "                if idf > len(oos_d):\n",
    "                    print(str(w) + \" // \" + str(idf))\n",
    "                positive_words_h4[w] = math.log(idf)\n",
    "            # print(oos_d[0])\n",
    "            # print('coverage' in oos_d[0])\n",
    "            # print([a for a in oos_d if 'miss' in a])\n",
    "            for w in negative_words_h4:\n",
    "                # tf = \n",
    "                idf = len(oos_d) / (1+(len([a for a in oos_d if w in a])))\n",
    "                if idf > len(oos_d):\n",
    "                    print(str(w) + \" // \" + str(idf))\n",
    "                negative_words_h4[w] = math.log(idf)\n",
    "            \n",
    "            #generate list of estimates for arts\n",
    "            article_estimates = []\n",
    "            for oos_index in range(len(oos_d)):\n",
    "                est_p = 0\n",
    "                oos_bow = oos_d[oos_index]\n",
    "                for w in oos_bow:\n",
    "                    # if w in positive_words or w in negative_words:\n",
    "                    est_p += (positive_words_h4.get(w,0)*math.log(1+oos_bow.get(w,0)) - negative_words_h4.get(w,0)*math.log(1+oos_bow.get(w,0)))\n",
    "                article_estimates.append(est_p)\n",
    "\n",
    "            #allocate estimates for each stock\n",
    "            tickers = list(set([a['ticker'] for a in oos_arts]))\n",
    "            ticker_sentiment = {}\n",
    "            for t in tickers:\n",
    "                ticker_arts = [index for (index,a) in enumerate(daily_arts) if a['ticker'] == t]\n",
    "                if(len(ticker_arts) > 0):\n",
    "                    ticker_sentiment[t] = sum([article_estimates[ta] for ta in ticker_arts])/len(ticker_arts)\n",
    "                #\n",
    "                # else:\n",
    "                #NOTE!: I chose not to add it so it makes calculating top/bot 50 easier\n",
    "                #     ticker_sentiment[t] = 0.5\n",
    "            \n",
    "            #allocate top 50 of each (if there are 50)\n",
    "            ticker_with_art = len(ticker_sentiment)\n",
    "            ticker_sentiment = {key:val for key, val in ticker_sentiment.items() if val != 0.5}\n",
    "            ticker_with_sent = len(ticker_sentiment)\n",
    "            top_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = True)[:50])\n",
    "            bot_50_tickers = dict(sorted(ticker_sentiment.items(), key = itemgetter(1), reverse = False)[:50])\n",
    "            top_50_tickers = {key:val for key, val in top_50_tickers.items()}\n",
    "            bot_50_tickers = {key:val for key, val in bot_50_tickers.items()}\n",
    "            top_50 = {}\n",
    "            bot_50 = {}\n",
    "            diff_stocks = 0\n",
    "            for tick in top_50_tickers:\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        top_50[tick] = testing_value\n",
    "                        if not tick in prev_top:\n",
    "                            diff_stocks += 1\n",
    "            for tick in bot_50_tickers:\n",
    "                # testing_value = stock_data.get(tick,'E')\n",
    "                if tick in stock_data:\n",
    "                    testing_value = stock_data[tick]['Open'].get(str(curr_date.date()), 'E')\n",
    "                    if not testing_value == 'E':\n",
    "                        bot_50[tick] = testing_value\n",
    "                        if not tick in prev_bot:\n",
    "                            diff_stocks += 1\n",
    "            if (len(prev_top) > 0):\n",
    "                diff_stocks /= (len(prev_bot) + len(prev_top))\n",
    "            else:\n",
    "                diff_stocks = 0\n",
    "\n",
    "            #print results to file\n",
    "            #format: DATE, LONG CORRECT, SHORT CORRECT, NUMBER OF LONG, NUMBER OF SHORT, % TURNOVER (number of stocks changed)\n",
    "            with open(output_file, 'a', newline='') as csv_file:\n",
    "                csvwriter = csv.writer(csv_file)\n",
    "                csvwriter.writerow([str((prev_date).date()), str(earning_long), str(investment_long), str(earning_short), str(investment_short), str(len(prev_top)), str(len(prev_bot)), str(diff_stocks), str(ticker_with_art), str(ticker_with_sent)])\n",
    "\n",
    "            #reset for next iter\n",
    "            prev_date = curr_date\n",
    "            prev_top = top_50\n",
    "            prev_bot = bot_50\n",
    "        # else:\n",
    "        #     print('Not a market day: ' + str(curr_date))\n",
    "        curr_date = curr_date + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate daily turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHORT\n",
      "0.8870249518359968\n",
      "long\n",
      "0.918329076742926\n"
     ]
    }
   ],
   "source": [
    "trial_id = '2013-05-01-ew-day+1'\n",
    "\n",
    "# pseudocode\n",
    "# sum(weight_t+1 - (weight_t(1+value_t+1)/1+sum(weights_t*values_t+1)))/2T\n",
    "\n",
    "portfolio_path_short = './data/out-of-sample/' + trial_id + '/portfolios/short'\n",
    "portfolio_path_long = './data/out-of-sample/' + trial_id + '/portfolios/long'\n",
    "# recreate list of stock information\n",
    "pathlist = Path(portfolio_path_short).rglob('*.csv')\n",
    "i = 0\n",
    "lst_dates = []\n",
    "weights_long = {}\n",
    "vals_long = {}\n",
    "for path in pathlist:\n",
    "    date = datetime.strptime(os.path.basename(str(path))[:-4], '%Y-%m-%d').date()\n",
    "    lst_dates.append(date)\n",
    "    #long\n",
    "    line_count = 0\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        # line_count = sum(1 for row in csv_reader1)\n",
    "        # # FORMAT: line#,headline,date,stock\n",
    "        # line_count = len(list(csv_reader))\n",
    "        # print(line_count)\n",
    "        # print(list(csv_reader))\n",
    "        weights_date = []\n",
    "        for row in csv_reader:\n",
    "            if weights_long.get(row[0], 'E') == 'E':\n",
    "                weights_long[row[0]] = {}\n",
    "                vals_long[row[0]] = {}\n",
    "            vals_long[row[0]][date] = float(row[2])\n",
    "            weights_date.append(float(row[1]))\n",
    "            line_count += 1\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            # weights_long[row[0]][date] = weights_date[line_count]/sum(weights_date)\n",
    "            weights_long[row[0]][date] = 1/len(weights_date)\n",
    "            line_count += 1\n",
    "pathlist = Path(portfolio_path_long).rglob('*.csv')\n",
    "i = 0\n",
    "lst_dates = []\n",
    "weights_short = {}\n",
    "vals_short = {}\n",
    "for path in pathlist:\n",
    "    date = datetime.strptime(os.path.basename(str(path))[:-4], '%Y-%m-%d').date()\n",
    "    lst_dates.append(date)\n",
    "    #long\n",
    "    line_count = 0\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        # line_count = sum(1 for row in csv_reader1)\n",
    "        # # FORMAT: line#,headline,date,stock\n",
    "        # line_count = len(list(csv_reader))\n",
    "        # print(line_count)\n",
    "        # print(list(csv_reader))\n",
    "        weights_date = []\n",
    "        for row in csv_reader:\n",
    "            if weights_short.get(row[0], 'E') == 'E':\n",
    "                weights_short[row[0]] = {}\n",
    "                vals_short[row[0]] = {}\n",
    "            vals_short[row[0]][date] = float(row[2])\n",
    "            weights_date.append(float(row[1]))\n",
    "            line_count += 1\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            # weights_short[row[0]][date] = weights_date[line_count]/sum(weights_date)\n",
    "            weights_short[row[0]][date] = 1/len(weights_date)\n",
    "            line_count+=1\n",
    "\n",
    "# print(vals_long)\n",
    "# print(weights_long)\n",
    "turnover = 0\n",
    "print('SHORT')\n",
    "for t in range(len(lst_dates)-1):\n",
    "    calc_sum = 0\n",
    "    for j in weights_long:\n",
    "        calc_sum += weights_long[j].get(lst_dates[t], 0)*vals_long[j].get(lst_dates[t+1], 0)\n",
    "    for i in weights_long:\n",
    "        turnover += abs(weights_long[i].get(lst_dates[t+1], 0) - (weights_long[i].get(lst_dates[t],0)*(1+vals_long[i].get(lst_dates[t+1], 0)))/(1+calc_sum))\n",
    "turnover /= 2*len(lst_dates)\n",
    "print(turnover)\n",
    "turnover = 0\n",
    "print('long')\n",
    "for t in range(len(lst_dates)-1):\n",
    "    calc_sum = 0\n",
    "    for j in weights_short:\n",
    "        calc_sum += weights_short[j].get(lst_dates[t], 0)*vals_short[j].get(lst_dates[t+1], 0)\n",
    "    for i in weights_short:\n",
    "        turnover += abs(weights_short[i].get(lst_dates[t+1], 0) - (weights_short[i].get(lst_dates[t],0)*(1+vals_short[i].get(lst_dates[t+1], 0)))/(1+calc_sum))\n",
    "turnover /= 2*len(lst_dates)\n",
    "print(turnover)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print table of words with similarities in H4 and LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week high & 0.023589 & 19 & 0 & 0 & downgrade & -0.023946 & 19 & 1 & 0 \\\\\n",
      "volume mover & 0.018901 & 19 & 0 & 0 & loser & -0.016851 & 19 & 0 & 1 \\\\\n",
      "upgrade & 0.016795 & 19 & 0 & 1 & lower & -0.016773 & 19 & 0 & 0 \\\\\n",
      "gainer & 0.013524 & 19 & 0 & 0 & public offer & -0.007176 & 19 & 0 & 0 \\\\\n",
      "high & 0.010034 & 19 & 0 & 0 & fall & -0.004345 & 19 & 0 & 0 \\\\\n",
      "mover & 0.007526 & 19 & 0 & 0 & cut & -0.003035 & 19 & 1 & 0 \\\\\n",
      "rais & 0.011851 & 18 & 0 & 0 & miss & -0.001481 & 19 & 1 & 0 \\\\\n",
      "repurchase & 0.000298 & 17 & 0 & 0 & weak & -0.001191 & 19 & 1 & 0 \\\\\n",
      "spike higher & 0.011535 & 16 & 0 & 0 & underweight & -0.000751 & 19 & 0 & 0 \\\\\n",
      "volume & 0.002742 & 16 & 0 & 0 & low & -0.005291 & 17 & 0 & 0 \\\\\n",
      "rumor & 0.00078 & 16 & 0 & 0 & public & -0.000609 & 17 & 0 & 0 \\\\\n",
      "author & 6.8e-05 & 16 & 0 & 0 & neutral & -0.003327 & 16 & 0 & 0 \\\\\n",
      "higher & 0.006567 & 15 & 0 & 0 & offer & -0.002486 & 16 & 0 & 0 \\\\\n",
      "outperform & 0.002857 & 15 & 1 & 0 & negative & -0.000794 & 16 & 1 & 1 \\\\\n",
      "spike & 0.002189 & 15 & 0 & 0 & offer common & -0.003422 & 15 & 0 & 0 \\\\\n",
      "solid & 0.000432 & 15 & 0 & 0 & disappoint & -0.000759 & 15 & 1 & 0 \\\\\n",
      "buy & 0.008344 & 14 & 0 & 0 & common & -0.000731 & 15 & 0 & 0 \\\\\n",
      "green & 0.000711 & 14 & 0 & 0 & concern & -0.000485 & 15 & 1 & 0 \\\\\n",
      "micron technology & 0.001827 & 13 & 0 & 0 & week low & -0.019854 & 14 & 0 & 0 \\\\\n",
      "overweight & 0.001675 & 13 & 0 & 0 & resume trade & -0.003218 & 14 & 0 & 0 \\\\\n",
      "soar & 0.001171 & 13 & 0 & 0 & secondary offer & -0.002503 & 14 & 0 & 0 \\\\\n",
      "strong & 0.000545 & 13 & 1 & 0 & loss & -0.000699 & 14 & 1 & 1 \\\\\n",
      "repurchase program & 6e-05 & 13 & 0 & 0 & remove & -0.000593 & 14 & 0 & 0 \\\\\n",
      "move higher & 0.003403 & 12 & 0 & 0 & impact & -0.000458 & 14 & 0 & 0 \\\\\n",
      "lift & 0.000659 & 12 & 0 & 0 & sector perform & -0.002324 & 13 & 0 & 0 \\\\\n",
      "tender offer & 0.000494 & 12 & 0 & 0 & bed bath & -0.001656 & 13 & 0 & 0 \\\\\n",
      "top gainer & 0.0146 & 11 & 0 & 0 & bath beyond & -0.001649 & 13 & 0 & 0 \\\\\n",
      "time warner & 0.003665 & 11 & 0 & 0 & general dynamic & -0.001225 & 13 & 0 & 0 \\\\\n",
      "urban outfitter & 0.002973 & 11 & 0 & 0 & tumble & -0.000678 & 13 & 0 & 0 \\\\\n",
      "western union & 0.00197 & 11 & 0 & 0 & resign & -0.000534 & 13 & 1 & 0 \\\\\n",
      "jump & 0.000856 & 11 & 0 & 0 & dip & -0.000478 & 13 & 0 & 0 \\\\\n",
      "standpoint research & 0.000432 & 11 & 0 & 0 & worst perform & -0.01493 & 12 & 0 & 0 \\\\\n",
      "special & 0.000154 & 11 & 0 & 1 & drop & -0.00074 & 12 & 0 & 0 \\\\\n",
      "strength & 0.000147 & 11 & 1 & 0 & resume & -0.000661 & 12 & 0 & 0 \\\\\n",
      "mention & 9.7e-05 & 11 & 0 & 0 & mix security & -0.000536 & 12 & 0 & 0 \\\\\n",
      "alert call & 0.001606 & 10 & 0 & 0 & pressure & -0.000469 & 12 & 0 & 0 \\\\\n",
      "marvel technology & 0.000905 & 10 & 0 & 0 & shelf & -0.00027 & 12 & 0 & 0 \\\\\n",
      "chatter & 0.000873 & 10 & 0 & 0 & security shelf & -0.000228 & 12 & 0 & 0 \\\\\n",
      "stake & 0.000815 & 10 & 0 & 0 & boston scientific & -0.003506 & 11 & 0 & 0 \\\\\n",
      "jobless claim & 0.000489 & 10 & 0 & 0 & worst & -0.002948 & 11 & 1 & 1 \\\\\n",
      "narrow & 0.000471 & 10 & 0 & 0 & sell & -0.000961 & 11 & 0 & 0 \\\\\n",
      "pop & 0.000359 & 10 & 0 & 0 & secondary & -0.000194 & 11 & 0 & 0 \\\\\n",
      "boost & 0.000324 & 10 & 1 & 0 & first solar & -0.009475 & 10 & 0 & 0 \\\\\n",
      "dish network & 0.000207 & 10 & 0 & 0 & hold remove & -0.0027 & 10 & 0 & 0 \\\\\n",
      "dynamic & 5.2e-05 & 10 & 0 & 1 & finish line & -0.001732 & 10 & 0 & 0 \\\\\n",
      "western digit & 0.003229 & 9 & 0 & 0 & cliff natural & -0.001449 & 10 & 0 & 0 \\\\\n",
      "alto network & 0.002602 & 9 & 0 & 0 & adobe & -0.001185 & 10 & 0 & 0 \\\\\n",
      "office depot & 0.000438 & 9 & 0 & 0 & perform & -0.001148 & 10 & 0 & 0 \\\\\n",
      "special dividend & 0.000385 & 9 & 0 & 0 & miss estimate & -0.00089 & 10 & 0 & 0 \\\\\n",
      "expansion & 0.000243 & 9 & 0 & 0 & fitch & -0.000813 & 10 & 0 & 0 \\\\\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_top_words(data, n=2, order=False, reverse=True):\n",
    "    \"\"\"Get top n words by tone. \n",
    "\n",
    "    Returns a dictionary or an `OrderedDict` if `order` is true.\n",
    "    \"\"\" \n",
    "    top = sorted(data.items(), key=lambda x: x[1][''], reverse=reverse)[:n]\n",
    "    if order:\n",
    "        return OrderedDict(top)\n",
    "    return dict(top)\n",
    "\n",
    "pathlist = Path('./data/models/stemming/word-lists/').rglob('*.csv')\n",
    "word_list_tone = {}\n",
    "word_list_count = {}\n",
    "for path in pathlist:\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        line = 0\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if line > 0:\n",
    "                if (row[0] in word_list_tone):\n",
    "                    word_list_tone[row[0]] += (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] += 1\n",
    "                else:\n",
    "                    word_list_tone[row[0]] = (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] = 1\n",
    "            line += 1\n",
    "pathlist = Path('./data/models/bigrams-stem/word-lists/').rglob('*.csv')\n",
    "for path in pathlist:\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        line = 0\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if line > 0:\n",
    "                if (row[0] in word_list_tone):\n",
    "                    word_list_tone[row[0]] += (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] += 1\n",
    "                else:\n",
    "                    word_list_tone[row[0]] = (float(row[1]) - float(row[2]))/2\n",
    "                    word_list_count[row[0]] = 1\n",
    "            line += 1\n",
    "\n",
    "word_list_count = {k: v for k, v in sorted(word_list_count.items(), reverse = True, key=lambda item: item[1])}\n",
    "word_list_tone = {k: v for k, v in sorted(word_list_tone.items(), key=lambda item: item[1])}\n",
    "for w in word_list_tone:\n",
    "    word_list_tone[w] /= 19\n",
    "# print(word_list_count)\n",
    "\n",
    "high_word_counts    = [w for w in word_list_count if word_list_count[w] > 14]\n",
    "# for w in high_word_counts:\n",
    "#     print(w, word_list_tone[w], word_list_count[w])\n",
    "pos_word_list_count = []\n",
    "neg_word_list_count = []\n",
    "count_id = 20\n",
    "while (len(pos_word_list_count) < 50):\n",
    "    new_words = {w : word_list_tone[w] for w in word_list_count if (word_list_count[w] == count_id and word_list_tone[w] > 0)}\n",
    "    new_words = dict(sorted(new_words.items(), key=lambda item: item[1], reverse = True))\n",
    "    for w in new_words:\n",
    "        pos_word_list_count.append(w)\n",
    "    count_id -= 1\n",
    "pos_word_list_count = pos_word_list_count[:50]\n",
    "count_id = 20\n",
    "while (len(neg_word_list_count) < 50):\n",
    "    new_words = {w : word_list_tone[w] for w in word_list_count if (word_list_count[w] == count_id and word_list_tone[w] < 0)}\n",
    "    new_words = dict(sorted(new_words.items(), key=lambda item: item[1], reverse = False))\n",
    "    for w in new_words:\n",
    "        neg_word_list_count.append(w)\n",
    "    count_id -= 1\n",
    "neg_word_list_count = neg_word_list_count[:50]\n",
    "# pos_word_list_count = [w for w in word_list_count if word_list_tone[w] > 0][:50]\n",
    "# neg_word_list_count = [w for w in word_list_count if word_list_tone[w] < 0][:50]\n",
    "for i in range(50):\n",
    "# for w in pos_word_list_count:\n",
    "    pos_w = pos_word_list_count[i]\n",
    "    neg_w = neg_word_list_count[i]\n",
    "    in_lm_pos = 0\n",
    "    in_h4_pos = 0\n",
    "    if pos_w in positive_words_lm:\n",
    "        in_lm_pos = 1\n",
    "    if pos_w in positive_words_h4:\n",
    "        in_h4_pos = 1\n",
    "# for w in neg_word_list_count:\n",
    "    in_lm_neg = 0\n",
    "    in_h4_neg = 0\n",
    "    if neg_w in negative_words_lm:\n",
    "        in_lm_neg = 1\n",
    "    if neg_w in negative_words_h4:\n",
    "        in_h4_neg = 1\n",
    "    print(pos_w + \" & \" + str(round(word_list_tone[pos_w],6)) + \" & \" + str(word_list_count[pos_w]) + \" & \" + str(in_lm_pos) + \" & \" + str(in_h4_pos) + \" & \" + neg_w + \" & \" + str(round(word_list_tone[neg_w],6)) + \" & \" + str(word_list_count[neg_w]) + \" & \" + str(in_lm_neg) + \" & \" + str(in_h4_neg) + \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing profit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input ff data\n",
    "ff_5 = {}\n",
    "with open('../external-csvs/F-F_Research_Data_5_Factors_2x3_daily_CSV/F-F_Research_Data_5_Factors_2x3_daily.CSV', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0:\n",
    "            ff_date = datetime.strptime(row[0],'%Y%m%d').date()\n",
    "            if ff_date >= datetime(2019,1,1).date():\n",
    "                # print(ff_date)\n",
    "                ff_5[ff_date] = [float(row[1]), float(row[2]), float(row[3]), float(row[4]), float(row[5]), float(row[6])]\n",
    "        line_count += 1\n",
    "\n",
    "test_date = datetime(2019,1,4).date()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the lines in the latex table showing sharpe ratio, average returns, and fama french regression results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======UNIGRAMS======\n",
      "formation & sharpe & avg profit (bps) & empty & ff3 a & ff3 r^2 & ff5 a & ff5 r^2\n",
      "EW LS & 0.66 & 6.51 & & 5.52 & 2.24\\% & 4.16 & 4.51\\% \\\\\n",
      "EW L & 0.64 & 5.46 & & 1.95 & 9.67\\% & 0.58 & 12.97\\% \\\\\n",
      "EW S & 0.03 & 1.05 & & 2.76 & 16.16\\% & 2.77 & 16.96\\% \\\\\n",
      "VW LS & 0.03 & 0.98 & & 0.93 & 2.56\\% & 0.75 & 2.66\\% \\\\\n",
      "VW L & -0.37 & -1.58 & & -4.67 & 14.12\\% & -5.13 & 14.62\\% \\\\\n",
      "VW S & 0.25 & 2.56 & & 4.79 & 21.41\\% & 5.07 & 21.56\\% \\\\\n",
      "=======BIGRAMS========\n",
      "formation & sharpe & avg profit (bps) & empty & ff3 a & ff3 r^2 & ff5 a & ff5 r^2\n",
      "EW LS & 1.75 & 20.69 & & 18.19 & 8.87\\% & 17.71 & 8.98\\% \\\\\n",
      "EW L & 1.29 & 17.84 & & 14.55 & 18.81\\% & 14.02 & 19.93\\% \\\\\n",
      "EW S & 0.15 & 2.85 & & 2.95 & 34.48\\% & 3.0 & 35.2\\% \\\\\n",
      "VW LS & 0.9 & 8.09 & & 6.85 & 5.71\\% & 7.29 & 6.31\\% \\\\\n",
      "VW L & 0.68 & 7.43 & & 4.95 & 29.62\\% & 4.97 & 30.64\\% \\\\\n",
      "VW S & -0.0 & 0.66 & & 1.21 & 35.07\\% & 1.63 & 36.71\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def gen_alpha_table(path,prefix):\n",
    "    long_profit = {}\n",
    "    short_profit = {}\n",
    "    ls_profit = {}\n",
    "    long_rf_profit = []\n",
    "    short_rf_profit = []\n",
    "    ls_rf_profit = []\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count > 0:\n",
    "                # DATE,long value,EARNING LONG,short value,EARNING SHORT,NUMBER LONG,NUMBER SHORT,TOTAL FIRMS WITH ARTS,HEADLINES WITH SENTIMENT WORDS\n",
    "                if float(row[1]) > 0:\n",
    "                    long_profit[row[0]] = (float(row[2])/float(row[1])  - 1)\n",
    "                    short_profit[row[0]] = (float(row[3])/float(row[4])  - 1)\n",
    "                    ls_profit[row[0]] = long_profit[row[0]] + short_profit[row[0]]\n",
    "            line_count += 1\n",
    "        \n",
    "    long_avg = 0\n",
    "    short_avg = 0\n",
    "    ls_avg = 0\n",
    "    len_d = 0\n",
    "    mkt_rf = []\n",
    "    smb = []\n",
    "    hml = []\n",
    "    rmw = []\n",
    "    cma = []\n",
    "    rf = []\n",
    "    for d in long_profit:\n",
    "        long_avg += long_profit[d]\n",
    "        short_avg += short_profit[d]\n",
    "        ls_avg += ls_profit[d]\n",
    "\n",
    "        prof_date = datetime.strptime(d, '%Y-%m-%d').date()\n",
    "\n",
    "        if prof_date in ff_5:\n",
    "            #Mkt-RF,SMB,HML,RMW,CMA,RF\n",
    "            mkt_rf.append(ff_5[prof_date][0])\n",
    "            smb.append(ff_5[prof_date][1])\n",
    "            hml.append(ff_5[prof_date][2])\n",
    "            rmw.append(ff_5[prof_date][3])\n",
    "            cma.append(ff_5[prof_date][4])\n",
    "            rf.append(ff_5[prof_date][5])\n",
    "            ls_rf_profit.append((float(ls_profit[d])*100 - float(ff_5[prof_date][5])))\n",
    "            long_rf_profit.append((float(long_profit[d])*100 - float(ff_5[prof_date][5])))\n",
    "            short_rf_profit.append((float(short_profit[d])*100 - float(ff_5[prof_date][5])))\n",
    "            len_d += 1\n",
    "\n",
    "    ff_3_data = [mkt_rf, smb, hml]\n",
    "    ff_5_data = [mkt_rf, smb, hml, rmw, cma]\n",
    "\n",
    "    ff_3_data = np.array(ff_3_data).transpose()\n",
    "    ff_5_data = np.array(ff_5_data).transpose()\n",
    "    # print(ff_3_data)\n",
    "\n",
    "    ff_3_sm = sm.add_constant(ff_3_data)\n",
    "    ff_5_sm = sm.add_constant(ff_5_data)\n",
    "    model_3 = sm.OLS(ls_rf_profit, ff_3_sm)\n",
    "    results_3 = model_3.fit()\n",
    "\n",
    "    model_5 = sm.OLS(ls_rf_profit, ff_5_sm)\n",
    "    results_5 = model_5.fit()\n",
    "\n",
    "    ls_avg /= len_d\n",
    "    sharpe_ls = ((np.average(ls_rf_profit)/np.std(ls_rf_profit))) * math.sqrt(252)\n",
    "    print(prefix + \" LS & \" + str(round(sharpe_ls,2)) + \" & \" + str(round(ls_avg*10000,2)) + ' & & ' + str(round(results_3.params[0]*100,2)) + ' & ' + str(round(results_3.rsquared*100,2)) + '\\\\% & ' + str(round(results_5.params[0]*100,2)) + ' & ' + str(round(results_5.rsquared*100,2)) + '\\\\% \\\\\\\\')\n",
    "    # print(results.model())\n",
    "\n",
    "    long_avg /= len_d\n",
    "\n",
    "    model_3 = sm.OLS(long_rf_profit, ff_3_sm)\n",
    "    results_3 = model_3.fit()\n",
    "\n",
    "    model_5 = sm.OLS(long_rf_profit, ff_5_sm)\n",
    "    results_5 = model_5.fit()\n",
    "\n",
    "    sharpe_long = ((np.average(long_rf_profit)/np.std(long_rf_profit))) * math.sqrt(252)\n",
    "    print(prefix + \" L & \" + str(round(sharpe_long,2)) + \" & \" + str(round(long_avg*10000,2)) + ' & & ' + str(round(results_3.params[0]*100,2)) + ' & ' + str(round(results_3.rsquared*100,2)) + '\\\\% & ' + str(round(results_5.params[0]*100,2)) + ' & ' + str(round(results_5.rsquared*100,2)) + '\\\\% \\\\\\\\')\n",
    "\n",
    "    short_avg /= len_d\n",
    "    model_3 = sm.OLS(short_rf_profit, ff_3_sm)\n",
    "    results_3 = model_3.fit()\n",
    "\n",
    "    model_5 = sm.OLS(short_rf_profit, ff_5_sm)\n",
    "    results_5 = model_5.fit()\n",
    "\n",
    "    sharpe_short = ((np.average(short_rf_profit)/np.std(short_rf_profit))) * math.sqrt(252)\n",
    "    print(prefix + \" S & \" + str(round(sharpe_short,2)) + \" & \" + str(round(short_avg*10000,2)) + ' & & ' + str(round(results_3.params[0]*100,2)) + ' & ' + str(round(results_3.rsquared*100,2)) + '\\\\% & ' + str(round(results_5.params[0]*100,2)) + ' & ' + str(round(results_5.rsquared*100,2)) + '\\\\% \\\\\\\\')\n",
    "\n",
    "# output average returns of LS, L, S of equal and value weighted\n",
    "# equal weighted\n",
    "\n",
    "\n",
    "print('======UNIGRAMS======')\n",
    "print(\"formation & sharpe & avg profit (bps) & empty & ff3 a & ff3 r^2 & ff5 a & ff5 r^2\")\n",
    "trial_date  = '2013-05-01'\n",
    "suffix      = '-ew-day+1-no-covid' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "path = target_directory + '/estimations.csv'\n",
    "gen_alpha_table(path, 'EW')\n",
    "trial_date  = '2013-05-01'\n",
    "suffix      = '-vw-day+1-no-covid' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "path = target_directory + '/estimations.csv'\n",
    "gen_alpha_table(path, 'VW')\n",
    "\n",
    "\n",
    "print('=======BIGRAMS========')\n",
    "print(\"formation & sharpe & avg profit (bps) & empty & ff3 a & ff3 r^2 & ff5 a & ff5 r^2\")\n",
    "trial_date  = '2013-05-01'\n",
    "suffix      = '-+bigram-ew-day+1' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "path = target_directory + '/estimations.csv'\n",
    "gen_alpha_table(path, 'EW')\n",
    "trial_date  = '2013-05-01'\n",
    "suffix      = '-+bigram-vw-day+1' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "path = target_directory + '/estimations.csv'\n",
    "gen_alpha_table(path, 'VW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-day ahead cum log graph\n",
    "\n",
    "This section generates the graph showing cumulative log returns of unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [[\"null\",0,0,0,0,0,0]]\n",
    "trial_date  = '2013-05-01'\n",
    "suffix      = '-ew-day+1' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "ew_path = target_directory + '/estimations.csv'\n",
    "\n",
    "suffix      = '-vw-day+1' \n",
    "target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "vw_path = target_directory + '/estimations.csv'\n",
    "\n",
    "with open(str(ew_path), encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    prev_row = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and float(row[1]) > 0:\n",
    "            #do something\n",
    "            long_profit = math.log(float(row[2])/float(row[1]))\n",
    "            short_profit = math.log(float(row[3])/float(row[4]))\n",
    "            days.append([row[0], long_profit + days[prev_row][1], short_profit + days[prev_row][2], long_profit + short_profit + days[prev_row][3]])\n",
    "            prev_row += 1\n",
    "        line_count += 1\n",
    "\n",
    "with open(str(vw_path), encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    prev_row = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and float(row[1]) > 0:\n",
    "            #do something\n",
    "            long_profit = math.log(float(row[2])/float(row[1]))\n",
    "            short_profit = math.log(float(row[3])/float(row[4]))\n",
    "            days[prev_row + 1].extend([long_profit + days[prev_row][4], short_profit + days[prev_row][5], short_profit + long_profit + days[prev_row][6]])\n",
    "            prev_row += 1\n",
    "        line_count += 1\n",
    "\n",
    "with open('../Report/data/one-day-ahead.csv', 'w',newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"date\",\"EW-L\",\"EW-S\",\"EW-LS\",\"VW-L\",\"VW-S\",\"VW-LS\"])\n",
    "    for d in days:\n",
    "        if d[0] != 'null':\n",
    "            csvwriter.writerow([d[0], d[1]*100, d[2]*100, d[3]*100, d[4]*100, d[5]*100, d[6]*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EW LS & 15.48 & 210.11 & & 207.8 & 1.09\\% & 207.19 & 4.93\\% \\\\\n",
      "EW L & 7.83 & 122.42 & & 120.91 & 21.43\\% & 120.44 & 23.06\\% \\\\\n",
      "EW S & 5.87 & 87.7 & & 86.2 & 28.81\\% & 86.05 & 29.01\\% \\\\\n",
      "VW LS & 10.8 & 130.31 & & 129.13 & 0.67\\% & 127.78 & 5.08\\% \\\\\n",
      "VW L & 5.48 & 70.39 & & 68.46 & 24.33\\% & 68.15 & 26.38\\% \\\\\n",
      "VW S & 4.64 & 59.92 & & 59.98 & 28.73\\% & 58.93 & 29.28\\% \\\\\n",
      "EW LS & 15.67 & 234.84 & & 233.93 & 2.46\\% & 233.89 & 3.05\\% \\\\\n",
      "EW L & 7.57 & 134.7 & & 134.21 & 10.26\\% & 134.64 & 11.18\\% \\\\\n",
      "EW S & 6.21 & 100.15 & & 99.03 & 21.85\\% & 98.56 & 22.02\\% \\\\\n",
      "VW LS & 12.51 & 139.79 & & 139.08 & 1.39\\% & 139.21 & 1.62\\% \\\\\n",
      "VW L & 3.77 & 53.24 & & 51.09 & 15.87\\% & 51.56 & 17.01\\% \\\\\n",
      "VW S & 6.87 & 86.55 & & 87.3 & 24.3\\% & 86.96 & 24.89\\% \\\\\n",
      "EW LS & 0.84 & 9.23 & & 5.59 & 4.11\\% & 5.57 & 4.16\\% \\\\\n",
      "EW L & 0.69 & 9.75 & & 6.56 & 21.62\\% & 5.86 & 23.0\\% \\\\\n",
      "EW S & -0.09 & -0.53 & & -1.66 & 25.71\\% & -0.98 & 26.68\\% \\\\\n",
      "VW LS & 0.98 & 8.37 & & 6.63 & 5.74\\% & 7.04 & 7.12\\% \\\\\n",
      "VW L & 0.33 & 3.95 & & 1.38 & 27.93\\% & 1.17 & 28.64\\% \\\\\n",
      "VW S & 0.31 & 4.41 & & 4.55 & 31.78\\% & 5.18 & 33.82\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "## day -1 to day +1 table\n",
    "\n",
    "for day_no in range(-1, 2):\n",
    "    trial_date  = '2013-05-01'\n",
    "    suffix      = '-ew-day+' + str(day_no)\n",
    "    target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "    path = target_directory + '/estimations.csv'\n",
    "    gen_alpha_table(path, 'EW')\n",
    "\n",
    "    trial_date  = '2013-05-01'\n",
    "    suffix      = '-vw-day+'  + str(day_no)\n",
    "    target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "    path = target_directory + '/estimations.csv'\n",
    "    gen_alpha_table(path, 'VW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed assimilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.01380094018022714 0.0115783684089095 0.013461522321822494\n",
      "2 0.014199736919011378 0.020219835707874785 0.020543548154340667\n",
      "3 0.013565863605007115 0.01894264111511986 0.021018467557207186\n",
      "4 0.012397054275419147 0.018718018620618018 0.022064260227885206\n",
      "5 0.015277410191244779 0.018422608726119165 0.022045922016126904\n",
      "6 0.012123400306644511 0.020219908151102303 0.020836337891396894\n",
      "7 0.013257556403260096 0.018976168317106315 0.02207080952686712\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../Report/data/speed-assimilation.csv', 'w',newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"day\",\"avg-LS\",\"avg-L\",\"avg-S\", \"avg-LS-no-covid\", \"avg-L-no-covid\", \"avg-S-no-covid\"])\n",
    "\n",
    "for day_no in range(1, 8):\n",
    "    trial_date  = '2013-05-01'\n",
    "    suffix      = '-ew-day+' + str(day_no) + '-no-covid'\n",
    "    target_directory = './data/out-of-sample/' + trial_date + suffix\n",
    "    path = target_directory + '/estimations.csv'\n",
    "    long_profit = {}\n",
    "    short_profit = {}\n",
    "    ls_profit = {}\n",
    "    long_profit_no_covid = {}\n",
    "    short_profit_no_covid = {}\n",
    "    ls_profit_no_covid = {}\n",
    "    covid_date = datetime(2020,2,1,0,0,0,0)\n",
    "    with open(str(path), encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count > 0:\n",
    "                # DATE,long value,EARNING LONG,short value,EARNING SHORT,NUMBER LONG,NUMBER SHORT,TOTAL FIRMS WITH ARTS,HEADLINES WITH SENTIMENT WORDS\n",
    "                if float(row[1]) > 0:\n",
    "                    long_profit[row[0]] = (float(row[2])/float(row[1])  - 1)\n",
    "                    short_profit[row[0]] = (float(row[3])/float(row[4])  - 1)\n",
    "                    ls_profit[row[0]] = long_profit[row[0]] + short_profit[row[0]]\n",
    "                if float(row[1]) > 0 and datetime.strptime(row[0],'%Y-%m-%d') < covid_date:\n",
    "                    long_profit_no_covid[row[0]] = (float(row[2])/float(row[1])  - 1)\n",
    "                    short_profit_no_covid[row[0]] = (float(row[3])/float(row[4])  - 1)\n",
    "                    ls_profit_no_covid[row[0]] = long_profit[row[0]] + short_profit[row[0]]\n",
    "            line_count += 1\n",
    "        \n",
    "    long_avg = 0\n",
    "    short_avg = 0\n",
    "    ls_avg = 0\n",
    "    for d in long_profit:\n",
    "        long_avg += long_profit[d]\n",
    "        short_avg += short_profit[d]\n",
    "        ls_avg += ls_profit[d]\n",
    "\n",
    "    long_avg_no_covid = 0\n",
    "    short_avg_no_covid = 0\n",
    "    ls_avg_no_covid = 0\n",
    "    for d in long_profit_no_covid:\n",
    "        long_avg_no_covid += long_profit_no_covid[d]\n",
    "        short_avg_no_covid += short_profit_no_covid[d]\n",
    "        ls_avg_no_covid += ls_profit_no_covid[d]\n",
    "\n",
    "    print(day_no, np.std(list(ls_profit.values())), np.std(list(long_profit.values())), np.std(list(short_profit.values())))\n",
    "    with open('../Report/data/speed-assimilation.csv', 'a',newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([str(day_no),str(ls_avg*10000/len(ls_profit)), str(long_avg*10000/len(long_profit)), str(short_avg*10000/len(short_profit)), str(ls_avg_no_covid*10000/len(ls_profit_no_covid)), str(long_avg_no_covid*10000/len(long_profit_no_covid)), str(short_avg_no_covid*10000/len(short_profit_no_covid))])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
