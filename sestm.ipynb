{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SESTM - Sentiment Extraction via Screening and Topic Modelling\n",
    "This is a novel sentiment extraction algorithm that gleans sentiment from realised stock returns. This notebook will walk you through how the nitty gritty of how exactly it works\n",
    "\n",
    "First things first, import this absolute monstrosity of imports; we'll be needing all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "# import scikit-learn\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# from textblob import TextBlob as tb\n",
    "import datetime\n",
    "import math\n",
    "from nbformat import read\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import unittest\n",
    "import sympy as sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/josh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/josh/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/josh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install nltk stuff\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that's out of the way, let's define some sample articles to demonstrate how it works. Normally, I would pull articles from the Refinitiv Eikon API, which returns the webpage with the content inside. I've simulated this somwhat by using `<p>` tags, but the general idea for real articles is the same as these sample ones\n",
    "\n",
    "Here are three sample articles that I made up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_1 = {\n",
    "    'date': '2021-12-23 12:58:45.061000+00:00',\n",
    "    'ticker': 'ABDN',\n",
    "    'mrkt_info': {\n",
    "        'open': 233.7,\n",
    "        'close': 200.3\n",
    "    },\n",
    "    'html': '<p>John likes to watch films and eat pizza.\\nMary likes films too.</p>'\n",
    "}\n",
    "\n",
    "article_2 = {\n",
    "    'date': '2022-01-26 07:11:46.774000+00:00',\n",
    "    'ticker': 'ABDN', \n",
    "    'mrkt_info': {\n",
    "        'open': 229.2,\n",
    "        'close': 241.0\n",
    "    },\n",
    "    'html': '<p>Mary also likes to watch football games.</p>'\n",
    "}\n",
    "\n",
    "article_3 = {\n",
    "    'date': '2021-10-25 13:22:07.985000+00:00',\n",
    "    'ticker': 'ABDN',\n",
    "    'mrkt_info': {\n",
    "        'open': 250.3,\n",
    "        'close': 258.5\n",
    "    },\n",
    "    'html': '<p>Carl likes to play football. He finds films boring.</p>'\n",
    "}\n",
    "\n",
    "art_list = [article_1, article_2, article_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing\n",
    "We now need to define some things about these articles, namely:\n",
    "1. The bag of words representation\n",
    "2. The sign of the article (`sgn`)\n",
    "3. The articles realised returns for article $i$ ($y_i$)\n",
    "\n",
    "Steps 2 and 3 are the easiest, so let's do those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns for article 1: -33.39999999999998\n",
      "Sign for article 1: -1\n",
      "Returns for article 2: 11.800000000000011\n",
      "Sign for article 2: 1\n",
      "Returns for article 3: 8.199999999999989\n",
      "Sign for article 3: 1\n"
     ]
    }
   ],
   "source": [
    "sgn = []    # list of article signs\n",
    "y = []      # list of realised returns\n",
    "i = 1\n",
    "for a in art_list:\n",
    "    returns = a['mrkt_info']['close'] - a['mrkt_info']['open']\n",
    "    y.append(returns)\n",
    "    sgn_a = -1\n",
    "    if (returns > 0): # add -1 if returns are 0 or less, 1 otherwise\n",
    "        sgn_a = 1\n",
    "    sgn.append(sgn_a)\n",
    "    print(\"Returns for article \" + str(i) + \": \" + str(returns))\n",
    "    print(\"Sign for article \" + str(i) + \": \" + str(sgn_a))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare the bag of words (BOW) representation for each of the articles. Before we can do that we need to *extract* the actual text content from the HTML, and then *normalise* the text content. Normlising consists of:\n",
    "- Converting to lower case\n",
    "- Removing non alphabetic chars\n",
    "- Removing stop words (pronouns, connectives, etc.)\n",
    "- Removing non-english words\n",
    "- Lemmatising (converting likes to likes etc.)\n",
    "- Tokenising (converting to list of words)\n",
    "\n",
    "A bit of notation here:\n",
    "- We have a collection of $n$ news articles and a dictionary of $m$ words\n",
    "- We record the word counts in the $i^{th}$ article in vector $d_i$\n",
    "    - $d_{i,j}$ is the number of times word $j$ appears in article $i$\n",
    "    - $D = [d_{1}, ..., d_{n}]$ is an $m \\times n$ doc-term matrix.\n",
    "    - We occasionally work with a subset of columns where only indices are those with sentiment, we define this as $d_{[S],i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text for article 1: 'john likes to watch films and eat pizza.\n",
      "mary likes films too.'\n",
      "BOW for article 1: {'like': 2, 'watch': 1, 'film': 2, 'eat': 1, 'pizza': 1, 'mary': 1}\n",
      "Text for article 2: 'mary also likes to watch football games.'\n",
      "BOW for article 2: {'mary': 1, 'also': 1, 'like': 1, 'watch': 1, 'football': 1, 'game': 1}\n",
      "Text for article 3: 'carl likes to play football. he finds films boring.'\n",
      "BOW for article 3: {'carl': 1, 'like': 1, 'play': 1, 'football': 1, 'find': 1, 'film': 1, 'bore': 1}\n",
      "[{'like': 2, 'watch': 1, 'film': 2, 'eat': 1, 'pizza': 1, 'mary': 1}, {'mary': 1, 'also': 1, 'like': 1, 'watch': 1, 'football': 1, 'game': 1}, {'carl': 1, 'like': 1, 'play': 1, 'football': 1, 'find': 1, 'film': 1, 'bore': 1}]\n",
      "\n",
      "\n",
      "Global BOW: {'like': 4, 'watch': 2, 'film': 3, 'eat': 1, 'pizza': 1, 'mary': 2, 'also': 1, 'football': 2, 'game': 1, 'carl': 1, 'play': 1, 'find': 1, 'bore': 1}\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "global_bow = {} # global bag of words\n",
    "d = []          # word count vector d, where di is the word count vector for article i\n",
    "\n",
    "i = 1\n",
    "for a in art_list:\n",
    "    raw_html = a['html']\n",
    "    if (raw_html): # when extracting articles from eikon, some articles are just pdfs, so this might not exist\n",
    "        readable_text = bs(raw_html, 'lxml').get_text().lower()\n",
    "        print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "        # substitute non alphabet chars (new lines become spaces)\n",
    "        readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "        readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "        # sub multiple spaces with one space\n",
    "        readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "        # tokenise text\n",
    "        words = nltk.wordpunct_tokenize(readable_text)\n",
    "        if len(words) > 0:\n",
    "            # lemmatise, remove non-english, and remove stopwords\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatised_words = []\n",
    "            en_words = set(nltk.corpus.words.words())\n",
    "            for w in words:\n",
    "                rootword = lemmatizer.lemmatize(w, pos=\"v\")\n",
    "                if rootword not in STOP_WORDS and (rootword in en_words or not rootword.isalpha()):\n",
    "                    lemmatised_words.append(rootword)\n",
    "            # convert to bag of words\n",
    "            bow_art = {}\n",
    "            # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "            # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "            for l in lemmatised_words:\n",
    "                if l in global_bow:\n",
    "                    global_bow[l] += 1\n",
    "                else:\n",
    "                    global_bow[l] = 1\n",
    "                if l in bow_art:\n",
    "                    bow_art[l] += 1\n",
    "                else:\n",
    "                    bow_art[l] = 1\n",
    "            print(\"BOW for article \" + str(i) + \": \" + str(bow_art))\n",
    "            d.append(bow_art)\n",
    "    i += 1\n",
    "\n",
    "print(d)\n",
    "print('\\n')\n",
    "print(\"Global BOW: \" + str(global_bow))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screening for sentiment-charged words\n",
    "With our BOWs now defined, we can move onto the maths.\n",
    "\n",
    "Our first step calculates the frequency word $j$ co-occurs with a positive return:\n",
    "\n",
    "$$f_j = \\frac{\\text{count of word } j \\text{ in article with } sgn(y) = + 1}{\\text{ count of word } j \\text{ in all articles}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_j = \n",
      "like: 2/4\n",
      "watch: 1/2\n",
      "film: 1/3\n",
      "eat: 0/1\n",
      "pizza: 0/1\n",
      "mary: 1/2\n",
      "also: 1/1\n",
      "football: 2/2\n",
      "game: 1/1\n",
      "carl: 1/1\n",
      "play: 1/1\n",
      "find: 1/1\n",
      "bore: 1/1\n"
     ]
    }
   ],
   "source": [
    "pos_j = {}  #j occuring in positive article\n",
    "total_j = {}#j occuring in any article\n",
    "for i in range(len(d)):\n",
    "    for w in d[i]:\n",
    "        pos_sent = 0\n",
    "        if (sgn[i] == 1):\n",
    "            pos_sent = 1\n",
    "        if w in total_j:\n",
    "            total_j[w] += d[i][w]\n",
    "            pos_j[w] += d[i][w]*pos_sent\n",
    "        else:\n",
    "            total_j[w] = d[i][w]\n",
    "            pos_j[w] = d[i][w]*pos_sent\n",
    "\n",
    "f = {}\n",
    "print(\"f_j = \")\n",
    "for w in total_j:\n",
    "    f[w] = pos_j[w]/total_j[w]\n",
    "    print(w + \": \" + str(pos_j[w]) + \"/\" + str(total_j[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate our list of sentiment charged words. Also, be aware that $\\hat \\pi$ is the fraction of articles marked with $sgn(y) = +1$. Normally, we would calculate this, but since there are only three, I'll just say it's $\\hat \\pi = 2/3$. In practice, $\\hat \\pi \\approx 1/2$\n",
    "\n",
    "At this point, we introduce some **hyper-parameters** that we can tune for an optimal model:\n",
    "- $\\alpha_{+}, \\alpha_{-}$ are the upper and lower thresholds respectively that determine a word is sentimentally charged (or not). For this set of data, so that any word $j$ with $0.47 \\le f_j \\le 0.67$ is sentiment neutral, we set:\n",
    "    - $\\alpha_- = 0.2$ \n",
    "    - $\\alpha_+ = 0$ \n",
    "- $\\kappa$ is the minimum frequency with which a word should appear across all articles to avoid low frequency words messing up data.\n",
    "    - Because our example dataset is very small, we will set $\\kappa = 1$, so we remove this constraint for now.\n",
    "\n",
    "This gives us the set of sentiment charged words $\\hat S$:\n",
    "$$\\hat S = \\{j: f_j \\ge \\hat \\pi + \\alpha_+ \\text{ or } f_j \\le \\hat \\pi - \\alpha_-\\} \\cap \\{j:k_j \\ge \\kappa\\}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S = ['film', 'eat', 'pizza', 'also', 'football', 'game', 'carl', 'play', 'find', 'bore']\n"
     ]
    }
   ],
   "source": [
    "ALPHA_PLUS  = 0\n",
    "ALPHA_MINUS = 0.2\n",
    "KAPPA       = 1\n",
    "pi = 2/3\n",
    "sentiment_words = [] # S\n",
    "neutral_words = []   # N\n",
    "for i in total_j:\n",
    "    if ((pos_j[i]/total_j[i] >= pi + ALPHA_PLUS or pos_j[i]/total_j[i] <= pi - ALPHA_MINUS) and total_j[i] >= KAPPA):\n",
    "        sentiment_words.append(i)\n",
    "    else:\n",
    "        neutral_words.append(i)\n",
    "\n",
    "print(\"S = \" + str(sentiment_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Sentiment Topics\n",
    "Ok, so we have the wordlise $S$, now we just need to fit a two-topic model to these word counts. We gather these in a matrix $O = [O_+,O_-]$ which determines the expected counts of sentiment charged words in each article.\n",
    "\n",
    "To measure how much article $i$ tilts in favour of a certain sentiment, we say the estimated sentiment of article $i$ is $\\hat p_i$ such that:\n",
    "$$\\hat p_i = \\frac{\\text{rank of } y_i \\text{ in } \\{y_l\\}_{l=1}^n}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_1: 1.0\n",
      "p_2: 0.33333333333333337\n",
      "p_3: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Calculates p_i\n",
    "p = [0] * len(y)\n",
    "for i, x in enumerate(sorted(range(len(y)), key=lambda y_lam: y[y_lam])):\n",
    "    p[x] = float(1 - i/len(y))\n",
    "# p = [((rank+1)/len(y)) for (rank,x) in enumerate(sorted(range(len(y)), key=lambda y_temp: y[y_temp]))]\n",
    "for i in range(len(p)):\n",
    "    print(\"p_\" + str(i+1) + \": \" + str(p[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also require the estimated $|S| \\times 1$ vector of word frequencies for article $i$, denoted $\\hat h_i$:\n",
    "$$\\hat h_i = \\frac{d_{[S],i}}{\\hat s_i} \\quad \\text{where } \\hat s_i = \\sum_{j \\in \\hat S} d_{j,i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_1:4\n",
      "d_(s,1):[2, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "s_2:3\n",
      "d_(s,2):[0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "s_3:6\n",
      "d_(s,3):[1, 0, 0, 0, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "s = []                                          # ith element corresponds to total count of sentiment charged words for document i\n",
    "d_s = []                                        # ith element corresponds to list of word counts for each of the sentiment charged words for document i\n",
    "h = np.zeros((len(d), len(sentiment_words)))    # ith element corresponds to |S|x1 vector of word frequencies divided by total sentiment words in doc i\n",
    "\n",
    "# Calculates s_i\n",
    "for doc in d:\n",
    "    s.append(sum(doc.get(val,0) for val in sentiment_words))\n",
    "    d_s.append([doc.get(val,0) for val in sentiment_words])\n",
    "\n",
    "for i in range(len(s)):\n",
    "    print(\"s_\" + str(i+1) + \":\" + str(s[i]))\n",
    "    print(\"d_(s,\" + str(i+1) + \"):\" + str(d_s[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_1:[0.5  0.25 0.25 0.   0.   0.   0.   0.   0.   0.  ]\n",
      "h_2:[0.         0.         0.         0.33333333 0.33333333 0.33333333\n",
      " 0.         0.         0.         0.        ]\n",
      "h_3:[0.16666667 0.         0.         0.         0.16666667 0.\n",
      " 0.16666667 0.16666667 0.16666667 0.16666667]\n"
     ]
    }
   ],
   "source": [
    "# Calculates h_i\n",
    "for i in range(len(d)):\n",
    "    # subvector of sentiment words in d_i\n",
    "    if (s[i] == 0) :\n",
    "        h[i] = np.zeros(len(sentiment_words)).transpose()\n",
    "    else:\n",
    "        h[i] = np.array([(j/s[i]) for j in d_s[i]]).transpose()\n",
    "\n",
    "for i in range(len(h)):\n",
    "    print(\"h_\" + str(i+1) + \":\" + str(h[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now have all the tools we need to estimate $O$ ($\\hat O$):\n",
    "$$\\hat O = [\\hat h_1, \\hat h_2, ..., \\hat h_n] \\widehat W' (\\widehat W \\widehat W')^{-1} \\quad \\text{where } \\widehat W =\n",
    "\\begin{bmatrix}\n",
    "\\hat p_1 & \\hat p_2 & \\cdots & \\hat p_n \\\\\n",
    "1- \\hat p_1 & 1-\\hat p_2 & \\cdots & 1-\\hat p_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Finally, we just set all negative entries for $\\hat O$ to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O =\n",
      "[[ 4.72222222e-01 -2.77777778e-01]\n",
      " [ 2.08333333e-01 -1.66666667e-01]\n",
      " [ 2.08333333e-01 -1.66666667e-01]\n",
      " [-5.55555556e-02  4.44444444e-01]\n",
      " [ 2.77555756e-17  5.00000000e-01]\n",
      " [-5.55555556e-02  4.44444444e-01]\n",
      " [ 5.55555556e-02  5.55555556e-02]\n",
      " [ 5.55555556e-02  5.55555556e-02]\n",
      " [ 5.55555556e-02  5.55555556e-02]\n",
      " [ 5.55555556e-02  5.55555556e-02]]\n",
      "O =\n",
      "[[ 3.86363636e-01 -1.25000000e-01]\n",
      " [ 1.70454545e-01 -7.50000000e-02]\n",
      " [ 1.70454545e-01 -7.50000000e-02]\n",
      " [-4.54545455e-02  2.00000000e-01]\n",
      " [ 2.27091073e-17  2.25000000e-01]\n",
      " [-4.54545455e-02  2.00000000e-01]\n",
      " [ 4.54545455e-02  2.50000000e-02]\n",
      " [ 4.54545455e-02  2.50000000e-02]\n",
      " [ 4.54545455e-02  2.50000000e-02]\n",
      " [ 4.54545455e-02  2.50000000e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Calculates O\n",
    "p_inv = [(1-val) for val in p]\n",
    "W = np.column_stack((p, p_inv))\n",
    "W = W.transpose()\n",
    "ww = np.matmul(W,W.transpose())\n",
    "w2 = np.matmul(W.transpose(), inv(ww))\n",
    "O = np.matmul(h.transpose(),w2)\n",
    "# Normalise O columns to have l1 norm\n",
    "print(\"O =\\n\" + str(O))\n",
    "O = O.transpose()\n",
    "O = sklearn.preprocessing.normalize(O,norm='l1')\n",
    "O = O.transpose()\n",
    "\n",
    "print(\"O =\\n\" + str(O))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice here that the last few entries in $\\hat O$ for this example are equal. This is because these entries correspond to sentiment-charged words that only appear in article 3, which would be ranked 2/3, or in other words, directly in the middle. This makes the word neutral overall in this context, evidenced by the two values being equal.\n",
    "\n",
    "## Scoring new articles\n",
    "With our shiny new estimates for sentiment-charged words and our $\\hat O$, we can now estimate sentiment of articles not in our dataset. So, let's define a new article we want to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_4 = {\n",
    "    'date': '2022-02-15 16:23:17.923000+00:00',\n",
    "    'ticker': 'ABDN',\n",
    "    # 'mrkt_info': {\n",
    "    #     'open': 250.3,\n",
    "    #     'close': 258.5\n",
    "    # },\n",
    "    'html': '<p>Josh likes to play football. He finds films tiring.</p>'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's basically the same as article 3, so we would expect the predicted outcome to be neutral.\n",
    "\n",
    "Let's turn it into a bag of words like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text for article 1: 'josh likes to play football. he finds films tiring.'\n",
      "BOW for article 1: {'josh': 1, 'like': 1, 'play': 1, 'football': 1, 'find': 1, 'film': 1, 'tire': 1}\n"
     ]
    }
   ],
   "source": [
    "new_art_list = [article_4]\n",
    "new_bow = {}\n",
    "new_d = []\n",
    "i = 1\n",
    "for a in new_art_list:\n",
    "    raw_html = a['html']\n",
    "    if (raw_html): # when extracting articles from eikon, some articles are just pdfs, so this might not exist\n",
    "        readable_text = bs(raw_html, 'lxml').get_text().lower()\n",
    "        print(\"Text for article \" + str(i) + \": '\" + readable_text + \"'\")\n",
    "        # substitute non alphabet chars (new lines become spaces)\n",
    "        readable_text = re.sub(r'\\n', ' ', readable_text)\n",
    "        readable_text = re.sub(r'[^a-z ]', '', readable_text)\n",
    "        # sub multiple spaces with one space\n",
    "        readable_text = re.sub(r'\\s+', ' ', readable_text)\n",
    "        # tokenise text\n",
    "        words = nltk.wordpunct_tokenize(readable_text)\n",
    "        if len(words) > 0:\n",
    "            # lemmatise, remove non-english, and remove stopwords\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatised_words = []\n",
    "            en_words = set(nltk.corpus.words.words())\n",
    "            for w in words:\n",
    "                rootword = lemmatizer.lemmatize(w, pos=\"v\")\n",
    "                if rootword not in STOP_WORDS and (rootword in en_words or not rootword.isalpha()):\n",
    "                    lemmatised_words.append(rootword)\n",
    "            # convert to bag of words\n",
    "            bow_art = {}\n",
    "            # global_bow = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "            # bow_art = {l: val+1 for l in lemmatised_words for val in global_bow.get(l, 0)}\n",
    "            for l in lemmatised_words:\n",
    "                if l in bow_art:\n",
    "                    bow_art[l] += 1\n",
    "                else:\n",
    "                    bow_art[l] = 1\n",
    "            new_bow = bow_art\n",
    "            print(\"BOW for article \" + str(i) + \": \" + str(bow_art))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the estimating, we need to calculate $\\hat s$ for the new article (total count of words from $\\hat S$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new s: 4\n"
     ]
    }
   ],
   "source": [
    "new_s = 0\n",
    "\n",
    "for w in sentiment_words:\n",
    "    new_s += new_bow.get(w, 0)\n",
    "\n",
    "print(\"new s: \" + str(new_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just estimate $p_4$ using **maximum likelihood estimation** (MLE) because it is statistically efficient. We also add a penalty term $\\lambda \\log(p(1-p))$ where $\\lambda > 0$ is a tuning parameter. This is to help with limited numbers of observations and low signal-to-noise ratios, to give us:\n",
    "\n",
    "$$\\hat p = \\arg \\max_{p \\in [0,1]} \\left\\{ \\hat s^{-1} \\sum_{j\\in \\hat S} d_j \\log \\left( p \\hat O_{+,j} + (1-p) \\hat O_{-,j} \\right) + \\lambda \\log(p(1-p))\\right\\}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: film // 0.3333333333333333\n",
      "d_j:1\n",
      "O+:0.3863636363636363\n",
      "O-:-0.12500000000000008\n",
      "Word: football // 1.0\n",
      "d_j:1\n",
      "O+:0.3863636363636363\n",
      "O-:-0.12500000000000008\n",
      "Word: play // 1.0\n",
      "d_j:1\n",
      "O+:0.3863636363636363\n",
      "O-:-0.12500000000000008\n",
      "Word: find // 1.0\n",
      "d_j:1\n",
      "O+:0.3863636363636363\n",
      "O-:-0.12500000000000008\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "entries = []\n",
    "\n",
    "new_p = sym.symbols('p')\n",
    "lam = 1\n",
    "terms = np.zeros(len(sentiment_words))\n",
    "i = 0\n",
    "for j in sentiment_words:\n",
    "    # a = (new_bow.get(j,0) * math.log(new_p*O[i][0] + (1-new_p)*O[i][1]))\n",
    "    d_j = new_bow.get(j,0)\n",
    "    if d_j > 0:\n",
    "        print(\"Word: \" + j + \" // \" + str(f[j]))\n",
    "        print(\"d_j:\" + str(new_bow.get(j,0)))\n",
    "        print(\"O+:\" + str(O[i][0]))\n",
    "        print(\"O-:\" + str(O[i][1]))\n",
    "    # i += 1/new_s + lam * (new_p*(1-new_p))\n",
    "\n",
    "# /new_s + lam * (new_p*(1-new_p))\n",
    "\n",
    "\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurrah! The $p$ value is approximately 0.68, which is really close to that of article 3, which it shares virtually all of the sentimentally charged words with, so with our very sparse dataset, it's done well."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
