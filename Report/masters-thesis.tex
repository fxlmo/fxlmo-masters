% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).

\documentclass[ oneside,% the name of the author
                    author={Joshua Felmeden},
                % the degree programme: BSc, MEng, MSci or MSc.
                    degree={MEng},
                % the dissertation    title (which cannot be blank)
                     title={Sentiment Analysis of Financial Headlines Based on Stock Returns},
                % the dissertation subtitle (which can    be blank)
                  subtitle={Research}]{dissertation}

\usepackage[%
  backend=biber,
  style=numeric,
  natbib=true
]{biblatex}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[english]{babel}
\usepackage{url}
\addbibresource{bibliography.bib}

\definecolor{commentsColor}{rgb}{0.497495, 0.497587, 0.497464}
\definecolor{keywordsColor}{rgb}{0.000000, 0.000000, 0.635294}
\definecolor{stringColor}{rgb}{0.558215, 0.000000, 0.135316}
\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{commentsColor}\textit,    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=tb,	                   	   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{keywordsColor}\bfseries,       % keyword style
  language=Python,                 % the language of the code (can be overrided per snippet)
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{commentsColor}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{stringColor}, % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  columns=fixed                    % Using fixed column width (for e.g. nice alignment)
}

\edef\restoreparindent{\parindent=\the\parindent\relax}
\usepackage{parskip}
\restoreparindent
% \setlength{\parskip}{\baselineskip}%
\AtBeginDocument{\addtocontents{toc}{\protect\setlength{\parskip}{0pt}}}
\makeatletter
\patchcmd{\@chapter}
  {\addtocontents{lof}}
  {\addtocontents{loa}{\protect\addvspace{10pt}}\addtocontents{lof}}
  {}{}
\makeatother
\usepackage{titlesec}

% \titlespacing*\chapter{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
% \titlespacing*\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
% \titlespacing*\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
% \titlespacing*\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\pgfplotsset{compat=1.12}
\usetikzlibrary{
    pgfplots.dateplot,
}
\usepackage{subcaption}
\newcommand\setrow[1]{\gdef\rowmac{#1}#1\ignorespaces}
\newcommand\clearrow{\global\let\rowmac\relax}
\clearrow

\begin{document}

% =============================================================================

% This section simply introduces the structural guidelines.  It can clearly
% be deleted (or commented out) if you use the file as a template for your
% own dissertation: everything following it is in the correct order to use 
% as is.

% \section*{Prelude}
% \thispagestyle{empty}

% A typical dissertation will be structured according to (somewhat) standard 
% sections, described in what follows.  However, it is hard and perhaps even 
% counter-productive to generalise: the goal is {\em not} to be prescriptive, 
% but simply to act as a guideline.  In particular, each page count given is
% important but {\em not} absolute: their aim is simply to highlight that a 
% clear, concise description is better than a rambling alternative that makes
% it hard to separate important content and facts from trivia.

% % You can use this document as a \LaTeX-based~\cite{latexbook1,latexbook2} 
% template for your own dissertation by simply deleting extraneous sections
% and content; keep in mind that the associated {\tt Makefile} could be of
% use. %, in particular because it automatically executes \mbox{\BibTeX} to 
% deal with the associated bibliography. 
% Alternatively, upload this template, dissertation.bib, dissertation.cls, 
% dtklogos.sty and the "logo" folder to Overleaf (an online \LaTeX editor and compiler) and work on your thesis there.

% \textbf{Do not include this section in your final dissertation --- just delete it from the source.}

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter


%\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}


In this project, I implement Zheng Ke, Bryan Kelly, and Dacheng Xiu's novel text sentiment extraction algorithm and evaluate its performance when considering a dataset of headlines. I extract information from headlines and use a supervised learning framework to construct a score, which is later used for stock movement prediction. In the study of the success of this algorithm, I use data from Yahoo Finance and compare predictive results of prediction from this algorithm against baseline lexicons. My research hypothesis is that this algorithm will be able to more accurately predict the movement of stocks based on the headlines on the previous day. I have also included data obtained from the experiments conducted, allowing for verification of the methods used.

\section*{List of Achievements}
\begin{itemize}
      \item I wrote more than 3000 lines of source code in Python for pre-processing data, constructing the algorithm, and testing by creating portfolios
      \item I adapted the methods explored by Ke, Kelly, and Dacheng to accommodate for headline input, rather than article bodies.
      \item I designed experiments to test the predictive success the algorithm.
      \item I implemented the algorithm and spent considerable time optimising the code to run as efficiently as possible
      \item I trained a number of different models to see the effect of differing setups
\end{itemize}

% {\bf A compulsory section, of at most 300 words} 
% \vspace{1cm} 

% \noindent
% This section should pr\'{e}cis the project context, aims and objectives,
% and main contributions (e.g., deliverables) and achievements; the same 
% section may be called an abstract elsewhere.  The goal is to ensure the 
% reader is clear about what the topic is, what you have done within this 
% topic, {\em and} what your view of the outcome is.

% The former aspects should be guided by your specification: essentially 
% this section is a (very) short version of what is typically the first 
% chapter. If your project is experimental in nature, this should include 
% a clear research hypothesis.  This will obviously differ significantly
% for each project, but an example might be as follows:

% \begin{quote}
% My research hypothesis is that a suitable genetic algorithm will yield
% more accurate results (when applied to the standard ACME data set) than 
% the algorithm proposed by Jones and Smith, while also executing in less
% time.
% \end{quote}

% \noindent
% The latter aspects should (ideally) be presented as a concise, factual 
% bullet point list.  Again the points will differ for each project, but 
% an might be as follows:

% \begin{quote}
% \noindent
% \begin{itemize}
% \item I spent $120$ hours collecting material on and learning about the 
%       Java garbage-collection sub-system. 
% \item I wrote a total of $5000$ lines of source code, comprising a Linux 
%       device driver for a robot (in C) and a GUI (in Java) that is 
%       used to control it.
% \item I designed a new algorithm for computing the non-linear mapping 
%       from A-space to B-space using a genetic algorithm, see page $17$.
% \item I implemented a version of the algorithm proposed by Jones and 
%       Smith in [6], see page $12$, corrected a mistake in it, and 
%       compared the results with several alternatives.
% \end{itemize}
% \end{quote}

% -----------------------------------------------------------------------------


\chapter*{Dedication and Acknowledgements}

% {\bf A compulsory section}
% \vspace{1cm} 

% \noindent
% It is common practice (although totally optional) to acknowledge any
% third-party advice, contribution or influence you have found useful
% during your work.   include support from friends or family, 
% the input of your Supervisor and/or Advisor, external organisations 
% or persons who  have supplied resources of some kind (e.g., funding, 
% advice or time), and so on.

I would first like thank my family for your continued encouragement and support for my entire university life. I would also like to thank my friends for supporting me, but also making me laugh when I needed it most.

I would also like to mention Mr. Richard Green, my secondary school computer science teacher, without who I would not be pursuing the field today.

Finally, I would like to thank Rami Chehab, who assisted in my research greatly and inspired the vast quantity of this project. I could not have done this without all of you.



% -----------------------------------------------------------------------------


% \chapter*{COVID-19 Statement}

% {\bf An optional section, of at most 800 words} 
% \vspace{1cm} 

% \noindent
% A summary of any planned research activities disrupted by Covid-19 restrictions and the extent to which it was possible to adapt the work in those changed circumstances. If the project was able to go forward as planned, you can safely remove this section without losing any marks. The following may be included:

% \begin{itemize}
% \item Details of any planned research activities curtailed by the pandemic because of, for example, lack of access to facilities, libraries, archives, research participants, fieldwork, etc. Information on any curtailed training should be included only insofar as it relates to the impact on research activities and on the dissertation.

% \item An acknowledgement of the anticipated contribution and value to the dissertation if those research activities had not been curtailed and what was possible to include in the dissertation in the circumstances, including where alternative choices were made to adapt the work and whether there are any weaknesses that could not be overcome.

% \item Any other relevant factors on the impact of Covid-19 on research activities and on the contents of the dissertation.

% \item Details of any research activities required by the examiners as part of a resubmission that were curtailed by the pandemic may be included in a new or revised Covid-19 statement in the resubmitted dissertation.
% \end{itemize}

% -----------------------------------------------------------------------------

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl



% -----------------------------------------------------------------------------

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures and tables.  These are all compulsory parts of the dissertation.

\tableofcontents
\listoffigures
\listoftables

% -----------------------------------------------------------------------------



\chapter*{Ethics Statement}

This project did not require ethical review, as determined by my supervisor, Dr. Rami Chehab.


% -----------------------------------------------------------------------------

% \chapter*{Summary of Changes}

% {\bf A conditional section} 
% \vspace{1cm} 

% If and only if the dissertation represents a resubmission (e.g., as the result of
% a resit), this section is compulsory: the content should summarise all
% non-trivial changes made to the initial submission.  Otherwise you can
% omit it, since a summary of this type is clearly nonsensical.

% When included, the section will ideally be used to highlight additional
% work completed, and address criticism raised in any associated feedback.
% Clearly it is difficult to give generic advice about how to do so, but
% an example might be as follows:

% \begin{quote}
% \noindent
% \begin{itemize}
% \item Feedback from the initial submission criticised the design and 
%       implementation of my genetic algorithm, stating ``there seems 
%       to have been no attention to computational complexity during the
%       design, and obvious methods of optimisation are missing within
%       the resulting implementation''.  Chapter $3$ now includes a
%       comprehensive analysis of the algorithm, in terms of both time
%       and space.  While I have not altered the algorithm itself, I
%       have included a cache mechanism (also detailed in Chapter $3$)
%       that provides a significant improvement in average run-time.
% \item I added a feature in my implementation to allow automatic rather
%       than manual selection of various parameters; the experimental
%       results in Chapter $4$ have been updated to reflect this.
% \item Questions after the presentation highlighted a range of related
%       work that I had not considered: I have make a number of updates 
%       to Chapter $2$, resolving this issue.
% \end{itemize}
% \end{quote}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}

% {\bf An optional section}
% \vspace{1cm} 

% \noindent
% This section should present a detailed summary, in bullet point form, 
% of any third-party resources (e.g., hardware and software components) 
% used during the project.  Use of such resources is always perfectly 
% acceptable: the goal of this section is simply to be clear about how
% and where they are used, so that a clear assessment of your work can
% result.  The content can focus on the project topic itself (rather,
% for example, than including ``I used \mbox{\LaTeX} to prepare my 
% dissertation''); an example is as follows:

% \begin{quote}
% \noindent
% \begin{itemize}
% \item I used the Java {\tt BigInteger} class to support my implementation 
%       of RSA.
% \item I used a parts of the OpenCV computer vision library to capture 
%       images from a camera, and for various standard operations (e.g., 
%       threshold, edge detection).
% \item I used an FPGA device supplied by the Department, and altered it 
%       to support an open-source UART core obtained from 
%       \url{http://opencores.org/}.
% \item The web-interface component of my system was implemented by 
%       extending the open-source WordPress software available from
%       \url{http://wordpress.org/}.
% \end{itemize}
% \end{quote}

\begin{itemize}
      \item I used a sample of financial headlines from Kaggle as training and validation data (\url{https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests})
      \item I used the Natural Language Toolkit to assist the preprocessing of data, using their English words, stop words data, and stemmers and lemmatizers (\url{https://www.nltk.org/})
      \item I used the yfinance library on Python to pull stock data (\url{https://pypi.org/project/yfinance/})
      \item I used Refinitiv Eikon to pull outstanding stock data (\url{https://www.refinitiv.com/en/products/eikon-trading-software#overview})
      \item I used Jupyter Notebook as an interactive development environment (\url{https://docs.jupyter.org/en/latest})
\end{itemize}


% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

\begin{quote}
\noindent
\begin{tabular}{lcl}
NLP               &:    &     Natural Language Processing \\
SESTM             &:    &     Semantic Extraction via Screening and Topic Modelling \\
$|x|$             &:    &     Size of $x$ \\
TF-IDF            &:    &     Term frequency inverse document frequency\\
EST               &:    &     Eastern standard time\\
UTC               &:    &     Coordinated universal time\\
\\
\textbf{Fama French Factors} \\
CAPM              &:    &     Capital asset pricing model\\
$R_i$             &:    &     Return of investment\\
$R_f$             &:    &     Risk free rate\\
SMB               &:    &     Small minus big\\
HML               &:    &     High minus low\\    
RMW               &:    &     Robus minus weak\\
CMA               &:    &     Conservative minus aggressive\\
\\
\textbf{SESTM Specific Notation} \\
$m$               &:    &     Number of words in sample \\
$n$               &:    &     Number of articles in sample \\
$d_{i,j}$         &:    &     Number of times word $j$ appears in text $i$ \\
$d_{[S],i}$       &:    &     Subset of columns where the only indices are those with sentiment \\
$D = [d_1, \dots, d_n]$ &:    & $m \times n$ Document term matrix \\
$sgn(y)$          &:    &     Sign of returns of article y \\
$\hat x$          &:    &    Expected value of variable $x$ \\
\end{tabular}
\end{quote}


% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter


\input{introduction.tex}

\input{background.tex}

\input{execution.tex}

\input{evaluation.tex}

\input{conclusion.tex}

\backmatter

% \bibliography{dissertation}
\printbibliography

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

% \chapter{Appendix}
% \label{appx}

% Content which is not central to, but may enhance the dissertation can be 
% included in one or more appendices; examples include, but are not limited
% to

% \begin{itemize}
% \item lengthy mathematical proofs, numerical or graphical results which 
%       are summarised in the main body,
% \item sample or example calculations, 
%       and
% \item results of user studies or questionnaires.
% \end{itemize}

% \noindent
% Note that in line with most research conferences, the marking panel is not
% obliged to read such appendices. The point of including them is to serve as
% an additional reference if and only if the marker needs it in order to check
% something in the main text. For example, the marker might check a program listing 
% in an appendix if they think the description in the main dissertation is ambiguous.


% =============================================================================
\chapter{List of Optimisations}
\label{appx:optimisations}
As part of the project I intended to optimise my implementation of the algorithm. This section of the appendix details the optimisations I made, along with justifications and overall speedup.The training and validation window is a simple three deep nested loop with $O(n^3)$, so keeping the inside of the loop as small as possible is paramount. Small increments in runtime here are compounded significantly. The original algorithm is shown in Listing \ref{lst:train-val-orig}. In this listing, the first optimisation can be seen. Python allows the use of \textit{list comprehension}, which is a shorthand method of creating list while using a loop. The basic idea behind this is shown in Listing \ref{lst:list-comprehension}. As shown in the Python wiki \cite{list-comprehension}, there is a slight speedup when using list comprehension, because in this example, it does not have to look up the list and the append method every iteration. When using many data points and inside a triple nested loop, this slight time gain is significant.

\begin{lstlisting}[float={!htb},caption={List comprehension example},label={lst:list-comprehension},language=Python]
#round each value in oldlist to 2 dp and place into newlist
oldlist = [0.121, 0.232, 0.465, 0.987]

#standard for loop
newlist = []
for num in oldlist:
      newlist.append(round(num,2))

#list comprehension
newlist = [round(num, 2) for num in oldlist]
\end{lstlisting}

\begin{lstlisting}[float={!htb},caption={Original training and validation window},label={lst:train-val-orig},language=Python]

for alpha in alpha_configs:
      for KAPPA in kappa_configs:
            for lam in lambda_configs:
                  #TRAINING
                  kappa_percentile = np.percentile(np.array(list(total_j.values())),KAPPA) # return the nth percentile of all appearances for KAPPA

                  #calculate alpha vals
                  ALPHA_PLUS  = train_pi/2
                  ALPHA_MINUS = train_pi/2
                  num_pos_words = len([w for w in total_j if f[w] >= train_pi + ALPHA_PLUS and total_j[w] >= kappa_percentile])
                  num_neg_words = len([w for w in total_j if f[w] <= train_pi - ALPHA_MINUS and total_j[w] >= kappa_percentile])
                  while(num_pos_words > alpha):
                        ALPHA_PLUS += 0.0001
                        num_pos_words = len([w for w in total_j if f[w] >= train_pi + ALPHA_PLUS and total_j[w] >= kappa_percentile])
                  while(num_neg_words > alpha):
                        ALPHA_MINUS += 0.0001
                        num_neg_words = len([w for w in total_j if f[w] <= train_pi - ALPHA_MINUS and total_j[w] >= kappa_percentile])
                  sentiment_words = [w for w in total_j if f[w] >= train_pi + ALPHA_PLUS and total_j[w] >= kappa_percentile])

                  sentiment_words = [w for w in total_j if ((f[w] >= train_pi + ALPHA_PLUS or f[w] <= train_pi - ALPHA_MINUS) and total_j[w] >= kappa_percentile)]

                  (s, d_s)    = calc_s(sentiment_words, train_d)
                  h           = calc_h(sentiment_words, train_d, s, d_s)
                  O           = calc_o(p,h)

                  # VALIDATION
                  error_arr = np.array(0)
                  for val_index in range(len(val_d)):
                        est_p = 0.5
                        val_bow = val_d[val_index]

                        testing_s = sum(val_bow.get(w,0) for w in sentiment_words)
                        if (testing_s > 0):
                              est_p = fminbound(equation_to_solve, 0, 1, (O,val_bow, sentiment_words,testing_s,LAM))
                        error_arr = np.append(error_arr, est_p - val_p[val_index])
                  normalised_error = np.linalg.norm(error_arr, 1)
                  lam_trial = {
                        'alpha': alpha,
                        'alpha_plus': ALPHA_PLUS,
                        'alpha_minus': ALPHA_MINUS,
                        'kappa': KAPPA,
                        'lam': LAM,
                        'o': O,
                        'sentiment_words': sentiment_words,
                        'norm_err': normalised_error
                  }
                  trials.append(lam_trial)
                  best_config = min(trials, key=lambda x:x['norm_err'])
\end{lstlisting}

It is also possible to take some of the computation up a loop level. For example, calculating the \texttt{kappa\_percentile} does not rely on either of the other loop values, and therefore can be taken to the first level of the loop. Similarly, the values for \texttt{alpha} only require information from \texttt{kappa}, and this can therefore be moved to the second loop. The training section only takes around 5 seconds in total to complete a single iteration, but this only needs to be computed around 15 times (once for each configuration of $\kappa$ and $\alpha$) rather than 45 (including $\alpha$), leading to a potential speedup of 150 seconds. Over all 19 windows, this equates to around an hour total removed from the runtime. These changes can be seen in a shortened version of the complete code in Listing \ref{lst:loop-level}.

\begin{lstlisting}[float={!htb},caption={Moving computation up loop levels},label={lst:loop-level},language=Python]
for KAPPA in kappa_configs:
      kappa_percentile = np.percentile(np.array(list(total_j.values())),KAPPA) # return the nth percentile of all appearances for KAPPA
      for alpha in alpha_configs:
            #calculate alpha vals
            ...
            for lam in lambda_configs:
                  ...
\end{lstlisting}

Furthermore, the efficiency of locating the values of $\alpha$ can be increased. Instead of incrementally searching values, a binary search can be employed. For each estimated value of $\alpha_\pm$, if the number of sentiment words calculated using this parameter is too large, then $\alpha_\pm$ is too small, otherwise it is too big. To shorten computation, by halving the change in value each time, the possibilities are also halved, reducing time from $O(n)$ to $O(\log(n))$. The updated $\alpha$ calculation is shown in Listing \ref{lst:binary}
\begin{lstlisting}[float={!htb},caption={Binary search for alpha values},label={lst:binary},language=Python]
ALPHA_PLUS  = train_pi/2
ALPHA_MINUS = train_pi/2
delta_plus  = train_pi/4
delta_minus  = train_pi/4
delta_limit = 0.0001 # set a limit on number of searches
while(delta_plus > delta_limit):
      no_pos_words = len([w for w in total_j if f[w] >= train_pi + ALPHA_PLUS and total_j[w] >= kappa_percentile])
      if no_pos_words == alpha:
            #alpha found
            delta_plus = 0
      elif (no_pos_words > alpha):
            ALPHA_PLUS += delta_plus
            delta_plus /= 2
      else:
            ALPHA_PLUS -= delta_plus
            delta_plus /= 2
while(delta_minus > delta_limit):
      no_neg_words = len([w for w in total_j if f[w] <= train_pi - ALPHA_MINUS and total_j[w] >= kappa_percentile])
      if no_neg_words == alpha:
            #alpha found
            delta_minus = 0
      elif (no_neg_words > alpha):
            ALPHA_MINUS += delta_minus
            delta_minus /= 2
      else:
            ALPHA_MINUS -= delta_minus
            delta_minus /= 2
sentiment_words = [w for w in total_j if ((f[w] >= train_pi + ALPHA_PLUS or f[w] <= train_pi - ALPHA_MINUS) and total_j[w] >= kappa_percentile)]
\end{lstlisting}

Finally, I opted to use multithreading to calculate each of the $\lambda$ values at the highest loop level. This algorithm lends itself to multithreading because the order in which the calculations are returned do not matter. For this reason, I chose to run each configuration of $\lambda$ on its own thread. The updated code for calculating $\lambda$ can be seen in Listing \ref{lst:multithread}.

\begin{lstlisting}[float={!htb},caption={Multithreadling Lambda},label={lst:multithread},language=Python]
def validate_window(index, val_d, val_p, sentiment_words, LAM, trials):
    error_arr = np.array(0)
    # print(str(index) + " computing lambda of " + str(LAM))
    for val_index in range(len(val_d)):
        est_p = 0.5
        val_bow = val_d[val_index]

        testing_s = sum(val_bow.get(w,0) for w in sentiment_words)
        if (testing_s > 0):
            est_p = fminbound(equation_to_solve, 0, 1, (O,val_bow, sentiment_words,testing_s,LAM))
        error_arr = np.append(error_arr, est_p - val_p[val_index])
    normalised_error = np.linalg.norm(error_arr, 1)
    lam_trial = {
        'alpha': alpha,
        'alpha_plus': ALPHA_PLUS,
        'alpha_minus': ALPHA_MINUS,
        'kappa': KAPPA,
        'lam': LAM,
        'o': O,
        'sentiment_words': sentiment_words,
        'norm_err': normalised_error
    }
    trials.append(lam_trial)

for kappa in kappa_configs:
      for alpha in alpha_configs:
            ...
            threads = []
            for index in range(3):
                x = threading.Thread(target=validate_window, args=(index,val_d, val_p, sentiment_words,lambda_configs[index],trials))
                threads.append(x)
                x.start()

            for index, thread in enumerate(threads):
                thread.join()
\end{lstlisting}

To quantify the speedup of the various steps, I ran a series of experiments, and timed the computation. I tested a single configuration for each configuration of $\alpha$, as this has the most impact on computation time (as intuitively, the more words in the word list, more computation required in validation). The results in Table \ref{tab:speedup} are calculated using an Intel Core i5-4460 CPU. Each cell in the table refers to the average runtime for all $\lambda$ configurations for $\kappa = 86$, and $\alpha$ is as defined in the table over 5 repetitions. The total speedup refers to the speedup from the original to the most optimised (multithreading).

\begin{table}[!htb]
\centering
\begin{tabular}{lllll}
      \toprule
      Alpha value & Original & List Comprehension & Multithreading & Total speedup \\
      \midrule
      25 & 146s & 93s   & 43s & 3.93X \\
      50 & 313s & 208s  & 75s & 4.17X\\
      100& 390s & 353s  & 150s& 2.6X\\
      \bottomrule
\end{tabular}
\caption{Speedup for each optimisation}
\label{tab:speedup}
\end{table}

\chapter{Word and Phrase lists}
\begin{table}[!ht]
\centering
\begin{tabular}{lcccclcccc}
\multicolumn{5}{c}{\textbf{Positive}} & \multicolumn{5}{c}{\textbf{Negative}} \\
\cmidrule(lr){1-5}
\cmidrule(lr){6-10}
Word & Sentiment & Count & LM & H4 & Word & Sentiment & Count & LM & H4 \\
\cmidrule(lr){1-5}
\cmidrule(lr){6-10}
upgrade & 0.016795 & 19 & 0 & 1 & downgrade & -0.023946 & 19 & 1 & 0 \\
gainer & 0.013524 & 19 & 0 & 0 & loser & -0.016851 & 19 & 0 & 1 \\
high & 0.010034 & 19 & 0 & 0 & lower & -0.016773 & 19 & 0 & 0 \\
mover & 0.007526 & 19 & 0 & 0 & fall & -0.004345 & 19 & 0 & 0 \\
rais & 0.011851 & 18 & 0 & 0 & cut & -0.003035 & 19 & 1 & 0 \\
repurchase & 0.000298 & 17 & 0 & 0 & miss & -0.001481 & 19 & 1 & 0 \\
volume & 0.002742 & 16 & 0 & 0 & weak & -0.001191 & 19 & 1 & 0 \\
rumor & 0.00078 & 16 & 0 & 0 & underweight & -0.000751 & 19 & 0 & 0 \\
author & 6.8e-05 & 16 & 0 & 0 & low & -0.005291 & 17 & 0 & 0 \\
higher & 0.006567 & 15 & 0 & 0 & public & -0.000609 & 17 & 0 & 0 \\
outperform & 0.002857 & 15 & 1 & 0 & neutral & -0.003327 & 16 & 0 & 0 \\
spike & 0.002189 & 15 & 0 & 0 & offer & -0.002486 & 16 & 0 & 0 \\
solid & 0.000432 & 15 & 0 & 0 & negative & -0.000794 & 16 & 1 & 1 \\
buy & 0.008344 & 14 & 0 & 0 & disappoint & -0.000759 & 15 & 1 & 0 \\
green & 0.000711 & 14 & 0 & 0 & common & -0.000731 & 15 & 0 & 0 \\
overweight & 0.001675 & 13 & 0 & 0 & concern & -0.000485 & 15 & 1 & 0 \\
soar & 0.001171 & 13 & 0 & 0 & loss & -0.000699 & 14 & 1 & 1 \\
strong & 0.000545 & 13 & 1 & 0 & remove & -0.000593 & 14 & 0 & 0 \\
lift & 0.000659 & 12 & 0 & 0 & impact & -0.000458 & 14 & 0 & 0 \\
jump & 0.000856 & 11 & 0 & 0 & tumble & -0.000678 & 13 & 0 & 0 \\
special & 0.000154 & 11 & 0 & 1 & resign & -0.000534 & 13 & 1 & 0 \\
strength & 0.000147 & 11 & 1 & 0 & dip & -0.000478 & 13 & 0 & 0 \\
mention & 9.7e-05 & 11 & 0 & 0 & drop & -0.00074 & 12 & 0 & 0 \\
chatter & 0.000873 & 10 & 0 & 0 & resume & -0.000661 & 12 & 0 & 0 \\
stake & 0.000815 & 10 & 0 & 0 & pressure & -0.000469 & 12 & 0 & 0 \\
narrow & 0.000471 & 10 & 0 & 0 & shelf & -0.00027 & 12 & 0 & 0 \\
pop & 0.000359 & 10 & 0 & 0 & worst & -0.002948 & 11 & 1 & 1 \\
boost & 0.000324 & 10 & 1 & 0 & sell & -0.000961 & 11 & 0 & 0 \\
dynamic & 5.2e-05 & 10 & 0 & 1 & secondary & -0.000194 & 11 & 0 & 0 \\
expansion & 0.000243 & 9 & 0 & 0 & adobe & -0.001185 & 10 & 0 & 0 \\
steel & 0.001328 & 8 & 0 & 0 & perform & -0.001148 & 10 & 0 & 0 \\
dividend & 0.000862 & 8 & 0 & 0 & fitch & -0.000813 & 10 & 0 & 0 \\
micron & 0.000665 & 8 & 0 & 0 & plunge & -0.000421 & 10 & 0 & 0 \\
rally & 0.00044 & 8 & 0 & 1 & valuation & -0.000143 & 10 & 0 & 0 \\
base & 0.000407 & 8 & 0 & 0 & lose & -7.9e-05 & 10 & 1 & 0 \\
beat & 0.000404 & 8 & 0 & 0 & laboratory & -0.000361 & 9 & 0 & 0 \\
f & 0.000321 & 8 & 0 & 0 & warn & -0.000352 & 9 & 1 & 0 \\
southern & 0.000314 & 8 & 0 & 0 & downbeat & -0.00029 & 9 & 0 & 0 \\
upside & 0.000271 & 8 & 0 & 1 & beyond & -0.00023 & 9 & 0 & 0 \\
final & 0.000216 & 8 & 0 & 0 & halt & -0.000225 & 9 & 1 & 0 \\
add & 0.000191 & 8 & 0 & 0 & prelim & -0.00012 & 9 & 0 & 0 \\
outfitter & 0.000167 & 8 & 0 & 0 & four & -3.1e-05 & 9 & 0 & 0 \\
unconfirm & 0.00014 & 8 & 0 & 0 & gap & -0.000914 & 8 & 0 & 0 \\
test & 2.1e-05 & 8 & 0 & 0 & price & -0.000484 & 8 & 0 & 0 \\
proceed & 0.0 & 8 & 0 & 0 & mix & -0.000447 & 8 & 0 & 0 \\
yelp & 0.001107 & 7 & 0 & 0 & bath & -0.000343 & 8 & 0 & 0 \\
call & 0.001105 & 7 & 0 & 0 & paper & -0.000232 & 8 & 0 & 0 \\
warner & 0.000664 & 7 & 0 & 0 & downside & -0.000129 & 8 & 0 & 0 \\
transocean & 0.000651 & 7 & 0 & 0 & delay & -8.2e-05 & 8 & 1 & 0 \\
surge & 0.000343 & 7 & 0 & 1 & loan & -5.1e-05 & 8 & 0 & 0 \\
\bottomrule
\end{tabular}
\caption[Sentiment word list for unigrams]{Top sentiment words for each polarity, along with appearance in either Loughran McDonald dictionary (LM) or Harvard IV psychological dictionary (H4). Note sentiment in this case refers to average \textit{tone} over all 20 training windows. Words are first sorted via count of training windows appeared in, and then by sentiment}
\end{table}


\begin{table}[!ht]
\centering
\begin{tabular}{lcccclcccc}
\multicolumn{5}{c}{\textbf{Positive}} & \multicolumn{5}{c}{\textbf{Negative}} \\
\cmidrule(lr){1-5}
\cmidrule(lr){6-10}
Word & Sentiment & Count & LM & H4 & Word & Sentiment & Count & LM & H4 \\
\cmidrule(lr){1-5}
\cmidrule(lr){6-10}
week high & 0.023589 & 19 & 0 & 0 & downgrade & -0.023946 & 19 & 1 & 0 \\
volume mover & 0.018901 & 19 & 0 & 0 & loser & -0.016851 & 19 & 0 & 1 \\
upgrade & 0.016795 & 19 & 0 & 1 & lower & -0.016773 & 19 & 0 & 0 \\
gainer & 0.013524 & 19 & 0 & 0 & public offer & -0.007176 & 19 & 0 & 0 \\
high & 0.010034 & 19 & 0 & 0 & fall & -0.004345 & 19 & 0 & 0 \\
mover & 0.007526 & 19 & 0 & 0 & cut & -0.003035 & 19 & 1 & 0 \\
rais & 0.011851 & 18 & 0 & 0 & miss & -0.001481 & 19 & 1 & 0 \\
repurchase & 0.000298 & 17 & 0 & 0 & weak & -0.001191 & 19 & 1 & 0 \\
spike higher & 0.011535 & 16 & 0 & 0 & underweight & -0.000751 & 19 & 0 & 0 \\
volume & 0.002742 & 16 & 0 & 0 & low & -0.005291 & 17 & 0 & 0 \\
rumor & 0.00078 & 16 & 0 & 0 & public & -0.000609 & 17 & 0 & 0 \\
author & 6.8e-05 & 16 & 0 & 0 & neutral & -0.003327 & 16 & 0 & 0 \\
higher & 0.006567 & 15 & 0 & 0 & offer & -0.002486 & 16 & 0 & 0 \\
outperform & 0.002857 & 15 & 1 & 0 & negative & -0.000794 & 16 & 1 & 1 \\
spike & 0.002189 & 15 & 0 & 0 & offer common & -0.003422 & 15 & 0 & 0 \\
solid & 0.000432 & 15 & 0 & 0 & disappoint & -0.000759 & 15 & 1 & 0 \\
buy & 0.008344 & 14 & 0 & 0 & common & -0.000731 & 15 & 0 & 0 \\
green & 0.000711 & 14 & 0 & 0 & concern & -0.000485 & 15 & 1 & 0 \\
micron technology & 0.001827 & 13 & 0 & 0 & week low & -0.019854 & 14 & 0 & 0 \\
overweight & 0.001675 & 13 & 0 & 0 & resume trade & -0.003218 & 14 & 0 & 0 \\
soar & 0.001171 & 13 & 0 & 0 & secondary offer & -0.002503 & 14 & 0 & 0 \\
strong & 0.000545 & 13 & 1 & 0 & loss & -0.000699 & 14 & 1 & 1 \\
repurchase program & 6e-05 & 13 & 0 & 0 & remove & -0.000593 & 14 & 0 & 0 \\
move higher & 0.003403 & 12 & 0 & 0 & impact & -0.000458 & 14 & 0 & 0 \\
lift & 0.000659 & 12 & 0 & 0 & sector perform & -0.002324 & 13 & 0 & 0 \\
tender offer & 0.000494 & 12 & 0 & 0 & bed bath & -0.001656 & 13 & 0 & 0 \\
top gainer & 0.0146 & 11 & 0 & 0 & bath beyond & -0.001649 & 13 & 0 & 0 \\
time warner & 0.003665 & 11 & 0 & 0 & general dynamic & -0.001225 & 13 & 0 & 0 \\
urban outfitter & 0.002973 & 11 & 0 & 0 & tumble & -0.000678 & 13 & 0 & 0 \\
western union & 0.00197 & 11 & 0 & 0 & resign & -0.000534 & 13 & 1 & 0 \\
jump & 0.000856 & 11 & 0 & 0 & dip & -0.000478 & 13 & 0 & 0 \\
standpoint research & 0.000432 & 11 & 0 & 0 & worst perform & -0.01493 & 12 & 0 & 0 \\
special & 0.000154 & 11 & 0 & 1 & drop & -0.00074 & 12 & 0 & 0 \\
strength & 0.000147 & 11 & 1 & 0 & resume & -0.000661 & 12 & 0 & 0 \\
mention & 9.7e-05 & 11 & 0 & 0 & mix security & -0.000536 & 12 & 0 & 0 \\
alert call & 0.001606 & 10 & 0 & 0 & pressure & -0.000469 & 12 & 0 & 0 \\
marvel technology & 0.000905 & 10 & 0 & 0 & shelf & -0.00027 & 12 & 0 & 0 \\
chatter & 0.000873 & 10 & 0 & 0 & security shelf & -0.000228 & 12 & 0 & 0 \\
stake & 0.000815 & 10 & 0 & 0 & boston scientific & -0.003506 & 11 & 0 & 0 \\
jobless claim & 0.000489 & 10 & 0 & 0 & worst & -0.002948 & 11 & 1 & 1 \\
narrow & 0.000471 & 10 & 0 & 0 & sell & -0.000961 & 11 & 0 & 0 \\
pop & 0.000359 & 10 & 0 & 0 & secondary & -0.000194 & 11 & 0 & 0 \\
boost & 0.000324 & 10 & 1 & 0 & first solar & -0.009475 & 10 & 0 & 0 \\
dish network & 0.000207 & 10 & 0 & 0 & hold remove & -0.0027 & 10 & 0 & 0 \\
dynamic & 5.2e-05 & 10 & 0 & 1 & finish line & -0.001732 & 10 & 0 & 0 \\
western digit & 0.003229 & 9 & 0 & 0 & cliff natural & -0.001449 & 10 & 0 & 0 \\
alto network & 0.002602 & 9 & 0 & 0 & adobe & -0.001185 & 10 & 0 & 0 \\
office depot & 0.000438 & 9 & 0 & 0 & perform & -0.001148 & 10 & 0 & 0 \\
special dividend & 0.000385 & 9 & 0 & 0 & miss estimate & -0.00089 & 10 & 0 & 0 \\
expansion & 0.000243 & 9 & 0 & 0 & fitch & -0.000813 & 10 & 0 & 0 \\
\bottomrule
\end{tabular}
\caption[Sentiment word list for bigrams]{Top sentiment words including bigrams for each polarity, along with appearance in either Loughran McDonald dictionary (LM) or Harvard IV psychological dictionary (H4). Note sentiment in this case refers to average \textit{tone} over all 20 training windows. Words are first sorted via count of training windows appeared in, and then by sentiment}
\end{table}
\end{document}